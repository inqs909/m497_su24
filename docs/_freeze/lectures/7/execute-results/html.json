{
  "hash": "9a756d858837d2b384305d7bc4801aa6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Monte Carlo Methods\"\nsubtitle: \"Bootstrap Methods\"\nformat:\n  revealjs:\n    include-in-header: \"math_commands.html\"\n    scrollable: true\n    navigation-mode: vertical\n    controls-layout: bottom-right\n    controls-tutorial: true\n    incremental: false \n    chalkboard:\n      src: chalkboard.json\n      storage: chalkboard_pres\n      theme: whiteboard\n      chalk-width: 4\nengine: knitr\nknitr:\n  opts_chunk: \n    code-fold: true\n    echo: true\n    eval: true\n    comment: \"#>\" \nfilters: \n  - reveal-header\n  - reveal-auto-agenda\n  - code-fullscreen\n  - webr\nwebr: \n  show-startup-message: true\n  packages: ['ggplot2', 'dplyr', 'stringr']\neditor_options: \n  chunk_output_type: console\n---\n\n## R Packages\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\ntheme_set(theme_bw())\ntheme_update(axis.title = element_text(size = 24))\n\n\ntuesdata <- tidytuesdayR::tt_load(2020, week = 28)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> \tDownloading file 1 of 1: `coffee_ratings.csv`\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\ncoffee_ratings <- tuesdata$coffee_ratings\ncoffee_aroma <- coffee_ratings |> filter(aroma > 5.5)\n\nshuffle <- function(x){\n  n <- length(x)\n  return(sample(x, n))\n}\n\nresample <- function(df){\n  if (!is.data.frame(df)){\n    stop(\"The df object must be a data frame.\")\n  }\n  dplyr::slice_sample(df, n = nrow(df), replace = T )\n}\n\n\npenguins <- penguins |> drop_na() \n```\n:::\n\n\n\n# Bootstrap\n\n## Empirical Distribution Function\n\nThe empirical distribution function is designed to estimate a random variable's distribution function. For an observed sample $\\{x_i\\}^n_{i=1}$, the empirical distribution function is\n\n$$\nF_n(x) \\left\\{\\begin{array}{cc}\n0, & x < x_{(1)} \\\\\n\\frac{i}{n},& x_{(i)} \\leq x <x_{(i+1)},\\ i = 1,\\ldots,n-1\\\\\n1,& x_{(n)}\\leq x\n\\end{array}\n\\right.\n$$\n\nwhere $x_{(1)}, \\ldots, x_{(n)}$ are the ordered sample.\n\n## Sampling an unknown $F$\n\nThe idea behind bootstrapping is that the data comes from a distribution $F$ with unknown parameters.\n\nUsing the sample, we can get parameters that explain a parameteric distribution or the emperical distribution for a nonparameteric approach. \n\n## The Bootstrap Method\n\nThe Bootstrap Method utilizes the sample to describe the target distribution function to construct a sampling mechanism of the target distribution.\n\nThis method will allow us to construct a new sample that targets the distribution.\n\nWe can then construct the sampling distribution of a statistic based on the data.\n\n## Standard Error\n\nThe bootstrap-based standard error of a test statistic is shown to provide an unbiased estimate of the true standard error. \n\n## Limitation to Boostrap Methods\n\nThe assumption is that the data provides a good estimate of the distribution function.\n\nIf the data set is small, it may not contain enough information to accurately describe the distribution.\n\n## Limitation Example\n\n# Parameteric Bootstrap\n\n## Parameteric Bootstrap\n\nParametric bootstrap methods are statistical techniques used to estimate the sampling distribution of an estimator or test statistic by resampling with a model-based approach. This method assumes that the data follow a known probability distribution, and utilizes the estimated statistics as the parameters for the distribution function to construct the sampling distribution.\n\n## Parameteric Bootstrap Algorithm\n\n1. Estimate the Parameters: Fit a parametric model to the observed data and estimate the parameters of the model. \n\n2. Generate Bootstrap Samples: Using the estimated parameters, generate a large number of new data sets (bootstrap samples) from the fitted model. These samples are simulated data sets that mimic the original data but are generated from the parametric model.\n\n3. Compute the Statistic of Interest: For each bootstrap sample, calculate the statistic of interest (e.g., the mean, variance, regression coefficients, etc.).\n\n4. Construct the Sampling Distribution: Use the calculated statistics from all the bootstrap samples to construct an empirical sampling distribution. \n\n5. Estimate Confidence Intervals: Use the empirical sampling distribution to estimate confidence intervals.\n\n## Example\n\nUse a parameteric bootstrap model to determine the standard errors of the mean body mass of each penguin species.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npenguins |> group_by(species) |> \n  summarise(mean = mean(body_mass_g),\n            se = sd(body_mass_g) / sqrt(n()))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3 Ã— 3\n#>   species    mean    se\n#>   <fct>     <dbl> <dbl>\n#> 1 Adelie    3706.  38.0\n#> 2 Chinstrap 3733.  46.6\n#> 3 Gentoo    5092.  46.0\n```\n\n\n:::\n:::\n\n\nAnswer:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nmeans <- penguins$body_mass_g |> tapply(penguins$species, mean)\nnns <- penguins$body_mass_g |> tapply(penguins$species, length)\nsds <- penguins$body_mass_g |> tapply(penguins$species, sd)\nAmeans <- numeric(10000)\nCmeans <- numeric(10000)\nGmeans <- numeric(10000)\nfor (i in 1:10000){\n  Ameans[i] <- rnorm(nns[1], mean = means[1], sd = sds[1]) |> mean()\n  Cmeans[i] <- rnorm(nns[2], mean = means[2], sd = sds[2]) |> mean()\n  Gmeans[i] <- rnorm(nns[3], mean = means[3], sd = sds[3]) |> mean()\n}\n```\n:::\n\n\n\n# Nonparameteric Bootstrap\n\n## Nonparameteric Bootsrap\n\nThe nonparameteric approach assumes that distribution function of the data does not follow a common distribution function. Therefore, the data itself will be contain all the information needed to construct the sampling distribution.\n\nThis requires sampling with replacement.\n\n## Nonparameteric Bootstrap Algorithm\n\n1.  Draw a sample $X*$ of size $n$ with replacement from the original data $X$.\n    1.  $n$ is the size of the data\n2.  Compute the bootstrap replicate statistic $T* = g(X*)$, where $g(\\cdot)$ is the function that computes the statistic of interest.\n3.  Repeat steps 1-2 $B$ times to obtain $B$ bootstrap replicates ${T*_1, T*_2, ..., T*_B}$.\n4.  The computed statistics from $B$ samples are the empirical bootstrap distribution of the statistic, $g(X)$.\n5.  Calculate the bootstrap standard error of the statistic, $se_b(g(X))$, as the standard deviation of the bootstrap replicates.\n6.  Calculate the bootstrap confidence interval for the statistic, $CI(g(X))$, with the $\\alpha$ and $(1-\\alpha)%$ percentiles of the bootstrap replicates, where $\\alpha$ is the desired level of significance.\n\n## Example\n\nFitting the following model:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(palmerpenguins)\nlibrary(tidyverse)\npenguins <- penguins |> drop_na()\npenguins |> lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n               data = _)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Call:\n#> lm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n#>     bill_depth_mm, data = penguins)\n#> \n#> Coefficients:\n#>       (Intercept)  flipper_length_mm     bill_length_mm      bill_depth_mm  \n#>         -6445.476             50.762              3.293             17.836\n```\n\n\n:::\n:::\n\n\nObtain the Bootstrap-based Standard Errors for the regression coefficients. Use $B=1000$ bootstrap samples.\n\n# Markov Chain Monte Carlo Methods\n\n## Markov Chain {.smaller}\n\n::: incremental\n-   A Markov chain is a collection states of a certain phenomenom\n\n    -   $X^{(0)},X^{(1)},X^{(2)},X^{(3)},X^{(4)},X^{(5)},X^{(6)},X^{(7)}, \\cdots, X^{(k)}$\n\n-   The changing of the state is only dependent on the current state, not the previous states\n\n    -   $P\\left\\{X^{(k+1)}\\boldsymbol{\\Big|}X^{(k)},X^{(k-1)},X^{(k-2)},\\ldots,X^{(1)},X^{(0)}\\right\\}=P\\left\\{X^{(k+1)}\\boldsymbol{\\Big |}X^{(k)}\\right\\}$\n:::\n\n## Cat Markov Chains\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Markov Kernel\n\n::: columns\n::: {.column width=\"50%\"}\n::: incremental\n-   A Markov kernel provides the probability of going to another state, given the current state\n\n-   Also known a transition matrix\n:::\n:::\n\n::: {.column width=\"50%\"}\n::: fragement\n\n::: {.cell}\n::: {.cell-output-display}\n![](7_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n:::\n:::\n:::\n\n## Stationary (limiting) distribution\n\n::: columns\n::: {.column width=\"50%\"}\n### Conditions\n\n::: incremental\n-   *Irreducibility:* The kernel allows for free movement of all the state space\n\n-   *Recurrent:* The chain will return to any nonnegligible set an infinite number of times\n\n-   *Aperiodic:* The chain can return to any state immediately\n:::\n:::\n\n::: {.column width=\"50%\"}\n### Resulting\n\n::: incremental\n-   $X^{(t)}\\rightarrow X$\n\n    -   Regardless of $X^{(0)}$\n\n-   $X \\sim f$\n\n    -   $f$: is a distribution function\n\n-   $\\frac{1}{T}\\sum_{t=1}^{T} h\\{X^{(t)}\\} \\rightarrow E_f\\{h(X)\\}$\n\n    -   $h$: any integrable function\n\n    -   by Law of Large Numbers\n:::\n:::\n:::\n\n## Markov Chains Monte Carlo\n\n::: incremental\n-   MCMC Methods are used to a distribution function that is not easily obtained.\n\n-   A Markov chain is contructed by simulating Monte Carlo Samples and accepted based on a certain criteria\n\n-   Based on the MCMC Central Limit Theorem, the Markov chain will construct a limiting distribution that is desired.\n:::\n\n## Hamiltonian Monte Carlo\n\n-   Hamiltonian Monte Carlo is a relatively new MCMC technique used to construct the target distribution\n\n-   It utilizes Hamiltonian dynamics to simulate the next random variable\n\n-   The random variable is the accepted based the MH probability\n\n-   Using Hamiltonian dyanmics improves the mixing properties of the chain and draws are more targeted to the desired distribution\n\n# Bayesian Analysis\n\n## Bayesian Analysis\n\n## Bayesian Analysis in R\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(brms)\ny <- rnorm(10000, mean = 3, sd = 1)\nbrm(y~1, data = tibble(y = y),\n    family = gaussian())\n```\n:::\n\n\n\n## Bayesian Analysis in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(brms)\ny <- rpois(10000, 3)\nbrm(y~1, data = tibble(y = y),\n    family = poisson())\n```\n:::\n",
    "supporting": [
      "7_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}