[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Summer 2024\nInstructor: Isaac Quintanilla Salinas\nContact: isaac.qs@csuci.edu\nOffice Location: BTE 2840\nOffice Hours:\nBy By Appointment (schedule with me) or Zoom appointment: calendly.com/isaac-qs/office-hours\nLecture: Wednesday 5:00-7:00 PM in BTE 2810 and individual appointments for registered students to work on projects.\nCourse Website: m497.inqs.info AND Canvas\n\n\n\nStudents will learn an introductory level of Monte Carlo methods related to random experiments, random number generation, random variable generations, and integration. Topics include inverse-transformation method, accept-reject algorithm, importance sampling, Markov Chains Monte Carlo. This class is useful for students who want to learn the primary engines of Monte Carlo hypothesis testing, Monte Carlo integration and optimization, and Bayesian Statistics. All analysis will be conducted in R.\n\n\n\n\n\n\n\nStatistical Computing (SC)\n\nIsaac Quintanilla Salinas\nwww.inqs.info/stat_comp\nhypothes.is/groups/xMmDdj2A/m408\n\n\n\n\n\nFor this course, we will use R, Quarto, and RStudio. Please download and install on your computer.\n\nR is a free statistical software program that is available for download at: www.r-project.org.\nR Markdown is a scientific documentation known as an RMD file that can be used to provide reproducible code and documents.\nRStudio provides free and open source tools for your data analysis in R: posit.co/downloads\ncsucistats is a developmental R package that will contain RMD templates to submit assignments for class: csucistats\n\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n50%\n\n\nFinal Project\n50%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\nHomework will be assigned on a regular basis and posted here and Canvas. The homework is to help you practice the concepts learned in lecture and to help you study. You must turn in your own individual homework and show your understanding of the material.\n\n\n\nImplement a Monte Carlo Method to answer a scientific question, study a statistical model, implement a numerical algorithm, or reporting of Monte Carlo Method not mentioned in class.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\nWk\nTopic\nReading\nAssignment\n\n\n\n\n1\nIntro to R and Control Flow\nSC: Ch 2-3\n\n\n\n2\nControl Flow/Functional Programming\nSC: Ch 3-4\nHW # 1\n\n\n3\nRandom Number/Variable Generation\nSC:\n\n\n\n4\nIntegration & Optimization\nSC:\nHW # 2\n\n\n5\nHypothesis Testing\nSC:\nHW # 3\n\n\n6\nPermutation Methods\nSC:\nHW # 4\n\n\n7\nBootstrapping Techniques\nSC:\nHW #5\n\n\n8\nSimulation Study: Linear & Generalized Linear Models\nSC:\n\n\n\n9\nSimulation Study: Zero-Inflated Models\nSC:\n\n\n\n10\nSimulation Study: Mixed-Effects Models\nSC:\n\n\n\n11\nFinal Presentation\n\n\n\n\n\n\n\n\n\nAcademic Honesty:\nPlease conduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\nDisabilities:\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\nEmergency Procedure Notice to Students:\nCSUCI is following guidelines and public orders from the California Department of Public Health and Ventura County Public Health for the COVID-19 pandemic as it pertains to CSUCI students, employees and visitors on the campus. Students are expected to adhere to all health and safety requirements as noted on the University’s Spring 2023 Semester website or they may be subject to removal from the classroom."
  },
  {
    "objectID": "syllabus.html#math-497-intro-to-monte-carlo-methods",
    "href": "syllabus.html#math-497-intro-to-monte-carlo-methods",
    "title": "Syllabus",
    "section": "",
    "text": "Term: Summer 2024\nInstructor: Isaac Quintanilla Salinas\nContact: isaac.qs@csuci.edu\nOffice Location: BTE 2840\nOffice Hours:\nBy By Appointment (schedule with me) or Zoom appointment: calendly.com/isaac-qs/office-hours\nLecture: Wednesday 5:00-7:00 PM in BTE 2810 and individual appointments for registered students to work on projects.\nCourse Website: m497.inqs.info AND Canvas\n\n\n\nStudents will learn an introductory level of Monte Carlo methods related to random experiments, random number generation, random variable generations, and integration. Topics include inverse-transformation method, accept-reject algorithm, importance sampling, Markov Chains Monte Carlo. This class is useful for students who want to learn the primary engines of Monte Carlo hypothesis testing, Monte Carlo integration and optimization, and Bayesian Statistics. All analysis will be conducted in R.\n\n\n\n\n\n\n\nStatistical Computing (SC)\n\nIsaac Quintanilla Salinas\nwww.inqs.info/stat_comp\nhypothes.is/groups/xMmDdj2A/m408\n\n\n\n\n\nFor this course, we will use R, Quarto, and RStudio. Please download and install on your computer.\n\nR is a free statistical software program that is available for download at: www.r-project.org.\nR Markdown is a scientific documentation known as an RMD file that can be used to provide reproducible code and documents.\nRStudio provides free and open source tools for your data analysis in R: posit.co/downloads\ncsucistats is a developmental R package that will contain RMD templates to submit assignments for class: csucistats\n\n\n\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n50%\n\n\nFinal Project\n50%\n\n\n\nAt the end of the quarter, course grades will be assigned according to the following scale:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA+\n98 – 100\nB+\n87 – &lt;90\nC+\n77 – &lt;80\nD+\n67 – &lt;70\n\n\n\n\nA\n93 – &lt;98\nB\n83 – &lt;87\nC\n73 – &lt;77\nD\n63 – &lt;67\nF\n&lt; 60\n\n\nA–\n90 - &lt;93\nB-\n80 – &lt;83\nC–\n70 – &lt;73\nD–\n60 – &lt;63\n\n\n\n\n\n\n\nHomework will be assigned on a regular basis and posted here and Canvas. The homework is to help you practice the concepts learned in lecture and to help you study. You must turn in your own individual homework and show your understanding of the material.\n\n\n\nImplement a Monte Carlo Method to answer a scientific question, study a statistical model, implement a numerical algorithm, or reporting of Monte Carlo Method not mentioned in class.\n\n\n\n\nThe following outline may be subject to change. Any changes will be announced in class.\n\n\n\n\n\n\n\n\n\nWk\nTopic\nReading\nAssignment\n\n\n\n\n1\nIntro to R and Control Flow\nSC: Ch 2-3\n\n\n\n2\nControl Flow/Functional Programming\nSC: Ch 3-4\nHW # 1\n\n\n3\nRandom Number/Variable Generation\nSC:\n\n\n\n4\nIntegration & Optimization\nSC:\nHW # 2\n\n\n5\nHypothesis Testing\nSC:\nHW # 3\n\n\n6\nPermutation Methods\nSC:\nHW # 4\n\n\n7\nBootstrapping Techniques\nSC:\nHW #5\n\n\n8\nSimulation Study: Linear & Generalized Linear Models\nSC:\n\n\n\n9\nSimulation Study: Zero-Inflated Models\nSC:\n\n\n\n10\nSimulation Study: Mixed-Effects Models\nSC:\n\n\n\n11\nFinal Presentation\n\n\n\n\n\n\n\n\n\nAcademic Honesty:\nPlease conduct yourself with honesty and integrity. Do not submit others’ work as your own. For assignments and quizzes that allow you to work with a group, only put your name on what the group submits if you genuinely contributed to the work. Work completely independently on exams, using only the materials that are indicated as allowed. Failure to observe academic honesty results in substantial penalties that can include failing the course.\nDisabilities:\nIf you are a student with a disability requesting reasonable accommodations in this course, you need to contact Disability Accommodations and Support Services (DASS) located on the second floor of Arroyo Hall, via email accommodations@csuci.edu or call 805-437-3331. All requests for reasonable accommodations require registration with DASS in advance of need: https://www.csuci.edu/dass/students/apply-for-services.htm. Faculty, students and DASS will work together regarding classroom accommodations. You are encouraged to discuss approved.\nEmergency Procedure Notice to Students:\nCSUCI is following guidelines and public orders from the California Department of Public Health and Ventura County Public Health for the COVID-19 pandemic as it pertains to CSUCI students, employees and visitors on the campus. Students are expected to adhere to all health and safety requirements as noted on the University’s Spring 2023 Semester website or they may be subject to removal from the classroom."
  },
  {
    "objectID": "lectures/5.html#r-packages",
    "href": "lectures/5.html#r-packages",
    "title": "Monte Carlo Methods",
    "section": "R Packages",
    "text": "R Packages\n\nlibrary(tidyverse)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "lectures/5.html#hypothesis-tests",
    "href": "lectures/5.html#hypothesis-tests",
    "title": "Monte Carlo Methods",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nHypothesis tests are used to test whether claims are valid or not. This is conducted by collecting data, setting the Null and Alternative Hypothesis."
  },
  {
    "objectID": "lectures/5.html#null-hypothesis-h_0",
    "href": "lectures/5.html#null-hypothesis-h_0",
    "title": "Monte Carlo Methods",
    "section": "Null Hypothesis \\(H_0\\)",
    "text": "Null Hypothesis \\(H_0\\)\nThe null hypothesis is the claim that is initially believed to be true. For the most part, it is always equal to the hypothesized value."
  },
  {
    "objectID": "lectures/5.html#alternative-hypothesis-h_a",
    "href": "lectures/5.html#alternative-hypothesis-h_a",
    "title": "Monte Carlo Methods",
    "section": "Alternative Hypothesis \\(H_a\\)",
    "text": "Alternative Hypothesis \\(H_a\\)\nThe alternative hypothesis contradicts the null hypothesis."
  },
  {
    "objectID": "lectures/5.html#example-of-null-and-alternative-hypothesis",
    "href": "lectures/5.html#example-of-null-and-alternative-hypothesis",
    "title": "Monte Carlo Methods",
    "section": "Example of Null and Alternative Hypothesis",
    "text": "Example of Null and Alternative Hypothesis\nWe want to see if \\(\\mu\\) is different from \\(\\mu_0\\)\n\n\n\nNull Hypothesis\nAlternative Hypothesis\n\n\n\n\n\\(H_0: \\mu=\\mu_0\\)\n\\(H_a: \\mu\\ne\\mu_0\\)\n\n\n\\(H_0: \\mu\\le\\mu_0\\)\n\\(H_a: \\mu&gt;\\mu_0\\)\n\n\n\\(H_0: \\mu\\ge\\mu_0\\)\n\\(H_0: \\mu&lt;\\mu_0\\)"
  },
  {
    "objectID": "lectures/5.html#one-side-vs-two-side-hypothesis-tests",
    "href": "lectures/5.html#one-side-vs-two-side-hypothesis-tests",
    "title": "Monte Carlo Methods",
    "section": "One-Side vs Two-Side Hypothesis Tests",
    "text": "One-Side vs Two-Side Hypothesis Tests\nNotice how there are 3 types of null and alternative hypothesis, The first type of hypothesis (\\(H_a:\\mu\\ne\\mu_0\\)) is considered a 2-sided hypothesis because the rejection region is located in 2 regions. The remaining two hypotheses are considered 1-sided because the rejection region is located on one side of the distribution.\n\n\n\nNull Hypothesis\nAlternative Hypothesis\nSide\n\n\n\n\n\\(H_0: \\mu=\\mu_0\\)\n\\(H_a: \\mu\\ne\\mu_0\\)\n2-sided\n\n\n\\(H_0: \\mu\\le\\mu_0\\)\n\\(H_a: \\mu&gt;\\mu_0\\)\n1-sided\n\n\n\\(H_0: \\mu\\ge\\mu_0\\)\n\\(H_0: \\mu&lt;\\mu_0\\)\n1-sided"
  },
  {
    "objectID": "lectures/5.html#t-tests-1",
    "href": "lectures/5.html#t-tests-1",
    "title": "Monte Carlo Methods",
    "section": "t-tests",
    "text": "t-tests\nt-tests are commonly used to determine whether a small sample is different from a hypothesized value.\n\n\\[\nH_0:\\ \\mu = \\mu_0\n\\]"
  },
  {
    "objectID": "lectures/5.html#assumptions",
    "href": "lectures/5.html#assumptions",
    "title": "Monte Carlo Methods",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nData Comes from a normal distribution (\\(N(\\mu, \\sigma^2)\\))\nThe population variance \\(\\sigma^2\\) is unknown\nSample is small (\\(n&lt;30\\))\nWhen \\(n\\geq30\\), use a z-test thanks to CLT"
  },
  {
    "objectID": "lectures/5.html#t-statistic",
    "href": "lectures/5.html#t-statistic",
    "title": "Monte Carlo Methods",
    "section": "T-statistic",
    "text": "T-statistic\n\\[\nT = \\frac{\\bar x -\\mu_0}{s/\\sqrt{n}} \\sim t(n-1)\n\\]\n\n\n\\(\\bar x\\): sample mean\n\\(s\\): sample standard deviation\n\\(n\\): sample size"
  },
  {
    "objectID": "lectures/5.html#why-t-distribution",
    "href": "lectures/5.html#why-t-distribution",
    "title": "Monte Carlo Methods",
    "section": "Why t distribution?",
    "text": "Why t distribution?\n\\[\nt = \\frac{N(0,1)}{\\sqrt{\\chi^2(n-1)/(n-1)}} \\sim t(n-1)\n\\] Knowing the distribution will allow us to compute the p-value!"
  },
  {
    "objectID": "lectures/5.html#monte-carlo-simulations",
    "href": "lectures/5.html#monte-carlo-simulations",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Simulations",
    "text": "Monte Carlo Simulations\nUse Monte Carlo Simulations to show that transforming a data set to the T-Statistic will yield a t-distribution. Use \\(H_0: \\mu = 3\\) and simulate 24 RV’s from \\(N(3, 9)\\). Testing how the distribution will look like assuming that the null hypothesis is true.\n\n\nCode\nx &lt;- rnorm(24, 3, 3)\n\n\nAnswer:\n\n\nCode\nt_stat &lt;- function(i){\n  x &lt;- rnorm(24, 3, 3)\n  tt &lt;- (mean(x) - 3)/(sd(x)/sqrt(24))\n  return(tt)\n}\nt_dist &lt;- sapply(1:100000, t_stat)\nxe &lt;- seq(-5, 5, length.out = 100)\nt_dist |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_line(data = tibble(x = xe, y = dt(xe, 23)),\n            mapping = aes(x,y),\n            col =  \"red\", lwd = 2)"
  },
  {
    "objectID": "lectures/5.html#sample",
    "href": "lectures/5.html#sample",
    "title": "Monte Carlo Methods",
    "section": "Sample",
    "text": "Sample\nA sample is a collection of random variables (data) that is believed to be independent and identically distributed (come from the same distribution).\n\\[\n\\bX = (X_1, X_2, \\cdots, X_n)^{\\mrT}\n\\]"
  },
  {
    "objectID": "lectures/5.html#statistic",
    "href": "lectures/5.html#statistic",
    "title": "Monte Carlo Methods",
    "section": "Statistic",
    "text": "Statistic\nA statistic is a transformation of data by a function.\n\\[\nT(\\bX)\n\\]"
  },
  {
    "objectID": "lectures/5.html#common-statistics",
    "href": "lectures/5.html#common-statistics",
    "title": "Monte Carlo Methods",
    "section": "Common Statistics",
    "text": "Common Statistics\n\n\n\nStatistic\nFunction\n\n\n\n\nMin\n\\(\\mathrm{min}(\\bX)\\)\n\n\nMax\n\\(\\mathrm{max}(\\bX)\\)\n\n\nMean\n\\(\\frac{1}{n}X_i\\)\n\n\nMedian\n\\(P(X_0 &lt; \\bX) = 0.5\\)\n\n\nSD\n\\(\\frac{1}{n-1}\\sum^n_{i=1}(X_i-\\bar X)^2\\)"
  },
  {
    "objectID": "lectures/5.html#sampling-distributions",
    "href": "lectures/5.html#sampling-distributions",
    "title": "Monte Carlo Methods",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\nA sampling distribution is the distribution of a statistic.\n\n\\[\nX_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\n\\]\n\n\n\nStatistic\nDistribution\n\n\n\n\n\\(\\bar X\\)\n\\(N(\\mu, \\sigma^2)\\)\n\n\n\\((n-1)s^2/\\sigma^2\\)\n\\(\\chi^2(n-1)\\)"
  },
  {
    "objectID": "lectures/5.html#central-limit-theorem",
    "href": "lectures/5.html#central-limit-theorem",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nLet \\(X_1, X_2, \\ldots, X_n\\) be identical and independent distributed random variables with \\(E(X_i)=\\mu\\) and \\(Var(X_i) = \\sigma²\\). We define\n\\[\nY_n = \\sqrt n \\left(\\frac{\\bar X-\\mu}{\\sigma}\\right) \\mathrm{ where }\\ \\bar X = \\frac{1}{n}\\sum^n_{i=1}X_i.\n\\]\nThen, the distribution of the function \\(Y_n\\) converges to a standard normal distribution function as \\(n\\rightarrow \\infty\\)."
  },
  {
    "objectID": "lectures/5.html#obtaining-sampling-distributions",
    "href": "lectures/5.html#obtaining-sampling-distributions",
    "title": "Monte Carlo Methods",
    "section": "Obtaining Sampling Distributions",
    "text": "Obtaining Sampling Distributions\nSeveral statistics have distributions that we can conduct inference on such as \\(t\\), \\(\\chi^2\\), and \\(F\\).\n\nHow do we obtain sampling distributions for other types of statistics?\n\n\n\nDistribution Functions\nDensity Functions\nMoment-Generating Functions"
  },
  {
    "objectID": "lectures/5.html#using-monte-carlo-methods",
    "href": "lectures/5.html#using-monte-carlo-methods",
    "title": "Monte Carlo Methods",
    "section": "Using Monte Carlo Methods",
    "text": "Using Monte Carlo Methods\n\nThe sample generated by simulating random variables is said to follow a distribution function.\nThe sample itself are the empirical cumulative density function (ECDF) of the true CDF.\nTransforming the simulated random variables of the sample are said to construct the ECDF of the statistic’s sampling distribution."
  },
  {
    "objectID": "lectures/5.html#sampling-distribution-bar-x",
    "href": "lectures/5.html#sampling-distribution-bar-x",
    "title": "Monte Carlo Methods",
    "section": "Sampling Distribution: \\(\\bar X\\)",
    "text": "Sampling Distribution: \\(\\bar X\\)\n\\[\nX_i \\stackrel{iid}{\\sim} N(5, 2)\n\\]\n\\[\n\\bar X \\sim N(5, 2/n)\n\\]"
  },
  {
    "objectID": "lectures/5.html#mc-sampling-distribution-bar-x",
    "href": "lectures/5.html#mc-sampling-distribution-bar-x",
    "title": "Monte Carlo Methods",
    "section": "MC Sampling Distribution: \\(\\bar X\\)",
    "text": "MC Sampling Distribution: \\(\\bar X\\)\n\n\nCode\nsim1 &lt;- function(i){\n  x &lt;- rnorm(1000, mean = 5, sd = sqrt(2))\n  return(mean(x))\n}\nresults &lt;- replicate(10000, sim1(1))\n\nxe &lt;- seq(4.8, 5.2, length.out = 100)\ntibble(x = results) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_line(data = tibble(x = xe, y = dnorm(xe, 5, sqrt(2/1000))),\n            mapping = aes(x,y),\n            col =  \"red\", lwd = 2)"
  },
  {
    "objectID": "lectures/5.html#mc-sampling-distribution-hat-s2",
    "href": "lectures/5.html#mc-sampling-distribution-hat-s2",
    "title": "Monte Carlo Methods",
    "section": "MC Sampling Distribution: \\(\\hat s^2\\)",
    "text": "MC Sampling Distribution: \\(\\hat s^2\\)\n\n\nCode\nsim2 &lt;- function(i){\n  x &lt;- rnorm(1000, mean = 5, sd = sqrt(2))\n  return(var(x))\n}\nresults &lt;- replicate(10000, sim2(1))\n\nxe &lt;- seq(850, 1150, length.out = 100)\ntibble(x = 999 * results / 2) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_line(data = tibble(x = xe, y = dchisq(xe, 999)),\n            mapping = aes(x,y),\n            col =  \"red\", lwd = 2)"
  },
  {
    "objectID": "lectures/5.html#maximum",
    "href": "lectures/5.html#maximum",
    "title": "Monte Carlo Methods",
    "section": "Maximum",
    "text": "Maximum\n\\[\nf_{max} (x) = n f(x) F(x)^{n-1}\n\\]\n\n\nCode\nsim3 &lt;- function(i){\n  x &lt;- rnorm(1000, mean = 5, sd = sqrt(2))\n  return(max(x))\n}\nresults &lt;- replicate(10000, sim3(1))\n\nxe &lt;- seq(8, 12, length.out = 100)\ntibble(x = results) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_line(data = tibble(x = xe, \n                          y = 1000 * dnorm(xe, 5, sqrt(2)) * pnorm(xe, 5, sqrt(2))^999),\n            mapping = aes(x,y),\n            col =  \"red\", lwd = 2)"
  },
  {
    "objectID": "lectures/5.html#minimum",
    "href": "lectures/5.html#minimum",
    "title": "Monte Carlo Methods",
    "section": "Minimum",
    "text": "Minimum\n\\[\nf_{min}(x) = n\\{1-F(x)\\}^{n-1}f(x)\n\\]\n\n\nCode\nsim4 &lt;- function(i){\n  x &lt;- rnorm(1000, mean = 5, sd = sqrt(2))\n  return(min(x))\n}\nresults &lt;- replicate(10000, sim4(1))\n\nxe &lt;- seq(-2.5, 2, length.out = 100)\ntibble(x = results) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_line(data = tibble(x = xe, \n                          y = 1000 * (1 - pnorm(xe, 5, sqrt(2)))^999 * dnorm(xe, 5, sqrt(2))),\n            mapping = aes(x,y),\n            col =  \"red\", lwd = 2)"
  },
  {
    "objectID": "lectures/5.html#monte-carlo-hypothesis-testing-1",
    "href": "lectures/5.html#monte-carlo-hypothesis-testing-1",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Hypothesis Testing",
    "text": "Monte Carlo Hypothesis Testing\nMonte Carlo hypothesis testing is the process of constructing a sampling distribution of the test statistic given a null distribution.\nUsing the test statistic constructed from the sample, identify its the location compared to the emperical cdf.\nConduct a hypothesis test based on the location with a Monte Carlo p-value."
  },
  {
    "objectID": "lectures/5.html#monte-carlo-algorithm",
    "href": "lectures/5.html#monte-carlo-algorithm",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Algorithm",
    "text": "Monte Carlo Algorithm\n\nSimulate data from the null distribution (\\(H_0\\))\nConstruct the test statistic from the simulated data.\nRepeat steps 1 and 2 to construct an empirical distribution of the null hypothesis.\nConstruct the test statistic from the sample data.\nCount the number of simulated test statistics that are more extreme than the sample test statistic.\nCompute:\n\n\\[\np = \\frac{m +1}{n + 1}\n\\]\n\n\\(m\\): number of extreme values\n\\(n\\): number of simulated test statistics"
  },
  {
    "objectID": "lectures/5.html#extreme-values",
    "href": "lectures/5.html#extreme-values",
    "title": "Monte Carlo Methods",
    "section": "Extreme values",
    "text": "Extreme values\nExtreme values are values that satisfy these condition:\n\n\n\nHypothesis\nCondition\n\n\n\n\n\\(H_a: \\mu\\ne\\mu_0\\)\n\\(|t_{data}| \\leq |t_{sim}|\\)\n\n\n\\(H_a: \\mu&gt;\\mu_0\\)\n\\(t_{data} \\leq t_{sim}\\)\n\n\n\\(H_0: \\mu&lt;\\mu_0\\)\n\\(t_{data} \\geq t_{sim}\\)"
  },
  {
    "objectID": "lectures/5.html#specifying-the-null-distribution",
    "href": "lectures/5.html#specifying-the-null-distribution",
    "title": "Monte Carlo Methods",
    "section": "Specifying the Null Distribution",
    "text": "Specifying the Null Distribution\nThe null distribution follows the theorized data distribution with parameters related to the null hypothesis and sample size!\n\n\\[\nH_0: \\mu =\\mu_0\n\\] \\[\nX\\sim N(\\mu, \\sigma^2)\n\\]"
  },
  {
    "objectID": "lectures/5.html#t-test-example",
    "href": "lectures/5.html#t-test-example",
    "title": "Monte Carlo Methods",
    "section": "t-test Example",
    "text": "t-test Example\nUse a Monte Carlo Hypothesis Test for \\(H_0: \\mu = 3\\) and \\(H_a: \\mu \\neq 3\\). Simulate fake data from 24 RV’s from \\(N(3, 9)\\). Testing how the distribution will look like assuming that the null hypothesis is true.\n\n\nCode\nx &lt;- rnorm(24, 3, 3)\n\n\nAnswer:\n\n\nCode\ntstat &lt;- (mean(x) - 3)/(sd(x)/sqrt(24))\n\n\nt_stat_sim &lt;- function(i){\n  x &lt;- rnorm(24, 3, 3)\n  tt &lt;- (mean(x) - 3)/(sd(x)/sqrt(24))\n  return(tt)\n}\nt_dist &lt;- sapply(1:100000, t_stat_sim)\n\nt_dist |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = tstat), col = \"red\")\n\n\n\nCode\nsum(abs(tstat) &lt; abs(t_dist))\n\n\n#&gt; [1] 69032\n\n\nCode\n(sum(abs(tstat) &lt; abs(t_dist)) + 1) / (length(t_dist) + 1)\n\n\n#&gt; [1] 0.6903231"
  },
  {
    "objectID": "lectures/5.html#t-test-example-1",
    "href": "lectures/5.html#t-test-example-1",
    "title": "Monte Carlo Methods",
    "section": "t-test Example",
    "text": "t-test Example\nUse a Monte Carlo Hypothesis Test for \\(H_0: \\mu = 6\\) and \\(H_a: \\mu \\neq 6\\). Simulate fake data from 24 RV’s from \\(N(3, 9)\\). Testing how the distribution will look like assuming that the null hypothesis is true.\n\n\nCode\nx &lt;- rnorm(24, 3, 3)\n\n\nAnswer:\n\n\nCode\ntstat &lt;- (mean(x) - 6)/(sd(x)/sqrt(24))\n\n\nt_stat_sim &lt;- function(i){\n  x &lt;- rnorm(24, 6, 3)\n  tt &lt;- (mean(x) - 6)/(sd(x)/sqrt(24))\n  return(tt)\n}\nt_dist &lt;- sapply(1:100000, t_stat_sim)\n\nt_dist |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = tstat), col = \"red\")\n\nsum(abs(tstat) &lt; abs(t_dist))\n(sum(abs(tstat) &lt; abs(t_dist)) + 1) / (length(t_dist) + 1)"
  },
  {
    "objectID": "lectures/5.html#uniform-example",
    "href": "lectures/5.html#uniform-example",
    "title": "Monte Carlo Methods",
    "section": "Uniform Example",
    "text": "Uniform Example\nUsing the sample data, test whether \\(H_0: \\theta = 10\\) vs \\(H_a: \\theta &lt; 10\\) for a sample of 35. You believe that the data came from the a \\(U(0, \\thetat)\\).\n\ny &lt;- runif(35, 0, 8)\n\nAnswer:\n\n\nCode\ntstat &lt;- max(y)\n\nt_stat_sim &lt;- function(i){\n  x &lt;- runif(35, 0, 10)\n  tt &lt;- max(x)\n  return(tt)\n}\nt_dist &lt;- sapply(1:100000, t_stat_sim)\n\nt_dist |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = tstat), col = \"red\")\n\nsum(tstat &gt; t_dist)\n(sum(tstat &gt; t_dist) + 1) / (length(t_dist) + 1)"
  },
  {
    "objectID": "lectures/5.html#example",
    "href": "lectures/5.html#example",
    "title": "Monte Carlo Methods",
    "section": "Example",
    "text": "Example\nThe fdeaths data set in R contains the monthly death count from specific lung diseases in the UK from 1974 to 1979. Test whether the monthly average (\\(theta\\)) death count is fdeaths is greater than 550?\nAnswer:\n\n\nCode\ntstat &lt;- mean(fdeaths)\n\nt_stat_sim &lt;- function(i){\n  x &lt;- rpois(72, 550)\n  tt &lt;- mean(x)\n  return(tt)\n}\nt_dist &lt;- sapply(1:100000, t_stat_sim)\n\nt_dist |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = tstat), col = \"red\")\n\nsum(tstat &gt; t_dist)\n(sum(tstat &gt; t_dist) + 1) / (length(t_dist) + 1)"
  },
  {
    "objectID": "lectures/5.html#type-1-and-2-error",
    "href": "lectures/5.html#type-1-and-2-error",
    "title": "Monte Carlo Methods",
    "section": "Type 1 and 2 Error",
    "text": "Type 1 and 2 Error\n\n\nCode\na1 &lt;- data.frame(\n   x = c(2.5,3.5),\n   y = c(41, 41),\n   label = c(\"Yes\", \"No\")\n)\n\na2 &lt;- data.frame(\n   x = c(1.9),\n   y = c(25, 35),\n   label = c(\"False\", \"True\")\n)\na3 &lt;- tibble(x = c(1.7,3), \n             y = c(30, 43), \n             label = c(\"H0\", \"Reject H0\"))\n\nyay &lt;- tibble(x = c(2.5, 3.5), \n              y = c(25, 35), \n              label = c(\"Yay!\", \"Yay!\"))\n\ntype1 &lt;- tibble(x = c(2.5, 3.5),\n                y = c(36.5, 26.5),\n                label = c(\"Type I Error\", \"Type II Error\"))\n\ntype2 &lt;- tibble(x = c(3.5, 2.5),\n                y = c(24.5, 34.5),\n                label = c(\"beta\", \"alpha\"))\n\n\n# basic graph\np &lt;- ggplot() + theme_void()\n\n# Add rectangles\np + annotate(\"rect\", \n             xmin=c(2,3), xmax=c(3,4), \n             ymin=c(20,20), ymax=c(30,30), \n             alpha=0.2, color=\"green\", fill=\"green\") + \n    annotate(\"rect\", \n             xmin=c(2,3), xmax=c(3,4), \n             ymin=c(30,30), ymax=c(40,40), \n             alpha=0.2, color=\"red\", fill=\"red\") +\n    annotate(\"rect\",\n             xmin=c(3,2), xmax=c(4,3),\n             ymin=c(30, 20), ymax=c(40, 30),\n             alpha=0.8, color=\"royalblue\", fill=\"royalblue\") +\n    geom_text(data=a1, aes(x=x, y=y, label=label),\n             size=10 , fontface=\"bold\" ) +\n    geom_text(data=a3, aes(x=x, y=y, label=label),\n             size=10 , fontface=\"bold.italic\" ) +\n    geom_text(data=a2, aes(x=x, y=y, label=label),\n             size=10, angle = 90, fontface=\"bold\" ) +\n    geom_text(data=yay, aes(x=x, y=y, label=label),\n             size=10, fontface=\"bold.italic\" ) +\n    geom_text(data=type1, aes(x=x, y=y, label=label),\n             size=10, fontface=\"bold\") +\n    geom_text(data=type2, aes(x=x, y=y, label=label),\n             size=10, fontface=\"bold\", parse = T )"
  },
  {
    "objectID": "lectures/5.html#type-1-and-2-error-1",
    "href": "lectures/5.html#type-1-and-2-error-1",
    "title": "Monte Carlo Methods",
    "section": "Type 1 and 2 Error",
    "text": "Type 1 and 2 Error\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 1000)\nxx &lt;- seq(-1, 7, length.out = 1000)\ndt_one&lt;-function(x){\n            y &lt;- dnorm(x)\n            y[x &lt; qnorm(.94)] &lt;-NA\n            return(y)\n}\ndt_two&lt;-function(x){\n            y &lt;- dnorm(x, mean = 3)\n            y[x &gt; 1.645] &lt;-NA\n            return(y)\n}\ndf1 &lt;- tibble(x = x, y = dnorm(x))\ndf2 &lt;- tibble(x = xx, y = dnorm(xx, mean = 3))\n\na1 &lt;- tibble(x = c(0,3), y = c(0.43, 0.43), label = c(\"H0\", \"H1\"))\na2 &lt;- tibble(x = c(2.05,1.25), y = c(0.02, 0.02), label = c(paste(\"alpha\"), \"beta\"))\n\ndf1 |&gt; \n  ggplot(aes(x, y)) +\n    stat_function(fun = dt_one, geom = \"area\", fill = \"red\") +\n    geom_line() +\n    geom_line(aes(x, y), df2) +\n    stat_function(fun = dt_two, geom = \"area\", fill = \"green\") +\n    geom_vline(aes(xintercept = 1.645), lwd = 1.5) +\n    geom_text(data=a1, aes(x=x,y=y, label=label),\n              size=14 , fontface=\"bold.italic\" ) +\n    geom_text(data=a2, aes(x=x,y=y, label=label),\n              size=10 , fontface=\"bold.italic\", parse = T) +\n    theme_bw()"
  },
  {
    "objectID": "lectures/5.html#power",
    "href": "lectures/5.html#power",
    "title": "Monte Carlo Methods",
    "section": "Power",
    "text": "Power\nPower is the probability of rejecting \\(H_0\\) given that \\(H_1\\).\n\\[\nPower  = 1-\\beta\n\\]"
  },
  {
    "objectID": "lectures/5.html#power-1",
    "href": "lectures/5.html#power-1",
    "title": "Monte Carlo Methods",
    "section": "Power",
    "text": "Power\nWe want to ensure we have a high power and low type I error rate (\\(\\alpha\\)).\n\nAs practitioners, we control \\(\\alpha\\). Set it before we conduct a study. Usually set at 0.05.\n\n\nWe cannot control power because it is dependent by effect size, \\(\\alpha\\), and sample size."
  },
  {
    "objectID": "lectures/5.html#power-relationships",
    "href": "lectures/5.html#power-relationships",
    "title": "Monte Carlo Methods",
    "section": "Power Relationships",
    "text": "Power Relationships\n\n\\(\\alpha \\uparrow\\) \\(\\rightarrow\\) \\(Power\\ \\uparrow\\)\n\\(n \\uparrow\\) \\(\\rightarrow\\) \\(Power\\ \\uparrow\\)"
  },
  {
    "objectID": "lectures/5.html#power-analysis-in-r",
    "href": "lectures/5.html#power-analysis-in-r",
    "title": "Monte Carlo Methods",
    "section": "Power Analysis in R",
    "text": "Power Analysis in R\nThe pwr package in R will conduct power and sample size for several common statistical tests. For more information, you can check this vignette and their github."
  },
  {
    "objectID": "lectures/5.html#conceptial",
    "href": "lectures/5.html#conceptial",
    "title": "Monte Carlo Methods",
    "section": "Conceptial",
    "text": "Conceptial\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 1000)\nxx &lt;- seq(-1, 7, length.out = 1000)\ndt_one&lt;-function(x){\n            y &lt;- dnorm(x)\n            y[x &lt; qnorm(.94)] &lt;-NA\n            return(y)\n}\ndt_two&lt;-function(x){\n            y &lt;- dnorm(x, mean = 3)\n            y[x &gt; 1.645] &lt;-NA\n            return(y)\n}\ndf1 &lt;- tibble(x = x, y = dnorm(x))\ndf2 &lt;- tibble(x = xx, y = dnorm(xx, mean = 3))\n\na1 &lt;- tibble(x = c(0,3), y = c(0.43, 0.43), label = c(\"H0\", \"H1\"))\na2 &lt;- tibble(x = c(2.05,1.25), y = c(0.02, 0.02), label = c(paste(\"alpha\"), \"beta\"))\n\ndf1 |&gt; \n  ggplot(aes(x, y)) +\n    stat_function(fun = dt_one, geom = \"area\", fill = \"red\") +\n    geom_line() +\n    geom_line(aes(x, y), df2) +\n    stat_function(fun = dt_two, geom = \"area\", fill = \"green\") +\n    geom_vline(aes(xintercept = 1.645), lwd = 1.5) +\n    geom_text(data=a1, aes(x=x,y=y, label=label),\n              size=14 , fontface=\"bold.italic\" ) +\n    geom_text(data=a2, aes(x=x,y=y, label=label),\n              size=10 , fontface=\"bold.italic\", parse = T) +\n    theme_bw()"
  },
  {
    "objectID": "lectures/6.html#r-packages",
    "href": "lectures/6.html#r-packages",
    "title": "Monte Carlo Methods",
    "section": "R Packages",
    "text": "R Packages\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\ntheme_set(theme_bw())\ntheme_update(axis.title = element_text(size = 24))\n\n\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n\n#&gt; \n#&gt;  Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee_ratings &lt;- tuesdata$coffee_ratings\ncoffee_aroma &lt;- coffee_ratings |&gt; filter(aroma &gt; 5.5)\n\nshuffle &lt;- function(x){\n  n &lt;- length(x)\n  return(sample(x, n))\n}\n\npenguins &lt;- penguins |&gt; drop_na()"
  },
  {
    "objectID": "lectures/6.html#type-1-and-2-error",
    "href": "lectures/6.html#type-1-and-2-error",
    "title": "Monte Carlo Methods",
    "section": "Type 1 and 2 Error",
    "text": "Type 1 and 2 Error\n\n\nCode\na1 &lt;- data.frame(\n   x = c(2.5,3.5),\n   y = c(41, 41),\n   label = c(\"Yes\", \"No\")\n)\n\na2 &lt;- data.frame(\n   x = c(1.9),\n   y = c(25, 35),\n   label = c(\"False\", \"True\")\n)\na3 &lt;- tibble(x = c(1.7,3), \n             y = c(30, 43), \n             label = c(\"H0\", \"Reject H0\"))\n\nyay &lt;- tibble(x = c(2.5, 3.5), \n              y = c(25, 35), \n              label = c(\"Yay!\", \"Yay!\"))\n\ntype1 &lt;- tibble(x = c(2.5, 3.5),\n                y = c(36.5, 26.5),\n                label = c(\"Type I Error\", \"Type II Error\"))\n\ntype2 &lt;- tibble(x = c(3.5, 2.5),\n                y = c(24.5, 34.5),\n                label = c(\"beta\", \"alpha\"))\n\n\n# basic graph\np &lt;- ggplot() + theme_void()\n\n# Add rectangles\np + annotate(\"rect\", \n             xmin=c(2,3), xmax=c(3,4), \n             ymin=c(20,20), ymax=c(30,30), \n             alpha=0.2, color=\"green\", fill=\"green\") + \n    annotate(\"rect\", \n             xmin=c(2,3), xmax=c(3,4), \n             ymin=c(30,30), ymax=c(40,40), \n             alpha=0.2, color=\"red\", fill=\"red\") +\n    annotate(\"rect\",\n             xmin=c(3,2), xmax=c(4,3),\n             ymin=c(30, 20), ymax=c(40, 30),\n             alpha=0.8, color=\"royalblue\", fill=\"royalblue\") +\n    geom_text(data=a1, aes(x=x, y=y, label=label),\n             size=10 , fontface=\"bold\" ) +\n    geom_text(data=a3, aes(x=x, y=y, label=label),\n             size=10 , fontface=\"bold.italic\" ) +\n    geom_text(data=a2, aes(x=x, y=y, label=label),\n             size=10, angle = 90, fontface=\"bold\" ) +\n    geom_text(data=yay, aes(x=x, y=y, label=label),\n             size=10, fontface=\"bold.italic\" ) +\n    geom_text(data=type1, aes(x=x, y=y, label=label),\n             size=10, fontface=\"bold\") +\n    geom_text(data=type2, aes(x=x, y=y, label=label),\n             size=10, fontface=\"bold\", parse = T )"
  },
  {
    "objectID": "lectures/6.html#type-1-and-2-error-1",
    "href": "lectures/6.html#type-1-and-2-error-1",
    "title": "Monte Carlo Methods",
    "section": "Type 1 and 2 Error",
    "text": "Type 1 and 2 Error\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 1000)\nxx &lt;- seq(-1, 7, length.out = 1000)\ndt_one&lt;-function(x){\n            y &lt;- dnorm(x)\n            y[x &lt; qnorm(.94)] &lt;-NA\n            return(y)\n}\ndt_two&lt;-function(x){\n            y &lt;- dnorm(x, mean = 3)\n            y[x &gt; 1.645] &lt;-NA\n            return(y)\n}\ndf1 &lt;- tibble(x = x, y = dnorm(x))\ndf2 &lt;- tibble(x = xx, y = dnorm(xx, mean = 3))\n\na1 &lt;- tibble(x = c(0,3), y = c(0.43, 0.43), label = c(\"H0\", \"H1\"))\na2 &lt;- tibble(x = c(2.05,1.25), y = c(0.02, 0.02), label = c(paste(\"alpha\"), \"beta\"))\n\ndf1 |&gt; \n  ggplot(aes(x, y)) +\n    stat_function(fun = dt_one, geom = \"area\", fill = \"red\") +\n    geom_line() +\n    geom_line(aes(x, y), df2) +\n    stat_function(fun = dt_two, geom = \"area\", fill = \"green\") +\n    geom_vline(aes(xintercept = 1.645), lwd = 1.5) +\n    geom_text(data=a1, aes(x=x,y=y, label=label),\n              size=14 , fontface=\"bold.italic\" ) +\n    geom_text(data=a2, aes(x=x,y=y, label=label),\n              size=10 , fontface=\"bold.italic\", parse = T) +\n    theme_bw()"
  },
  {
    "objectID": "lectures/6.html#power",
    "href": "lectures/6.html#power",
    "title": "Monte Carlo Methods",
    "section": "Power",
    "text": "Power\nPower is the probability of rejecting \\(H_0\\) given that \\(H_1\\).\n\\[\nPower  = 1-\\beta\n\\]"
  },
  {
    "objectID": "lectures/6.html#power-1",
    "href": "lectures/6.html#power-1",
    "title": "Monte Carlo Methods",
    "section": "Power",
    "text": "Power\nWe want to ensure we have a high power and low type I error rate (\\(\\alpha\\)).\n\nAs practitioners, we control \\(\\alpha\\). Set it before we conduct a study. Usually set at 0.05.\n\n\nWe cannot control power because it is dependent by effect size, \\(\\alpha\\), and sample size."
  },
  {
    "objectID": "lectures/6.html#power-relationships",
    "href": "lectures/6.html#power-relationships",
    "title": "Monte Carlo Methods",
    "section": "Power Relationships",
    "text": "Power Relationships\n\n\\(\\alpha \\uparrow\\) \\(\\rightarrow\\) \\(Power\\ \\uparrow\\)\n\\(n \\uparrow\\) \\(\\rightarrow\\) \\(Power\\ \\uparrow\\)"
  },
  {
    "objectID": "lectures/6.html#power-analysis-in-r",
    "href": "lectures/6.html#power-analysis-in-r",
    "title": "Monte Carlo Methods",
    "section": "Power Analysis in R",
    "text": "Power Analysis in R\nThe pwr package in R will conduct power and sample size for several common statistical tests. For more information, you can check this vignette and their github."
  },
  {
    "objectID": "lectures/6.html#conceptial",
    "href": "lectures/6.html#conceptial",
    "title": "Monte Carlo Methods",
    "section": "Conceptial",
    "text": "Conceptial\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 1000)\nxx &lt;- seq(-1, 7, length.out = 1000)\ndt_one&lt;-function(x){\n            y &lt;- dnorm(x)\n            y[x &lt; qnorm(.94)] &lt;-NA\n            return(y)\n}\ndt_two&lt;-function(x){\n            y &lt;- dnorm(x, mean = 3)\n            y[x &gt; 1.645] &lt;-NA\n            return(y)\n}\ndf1 &lt;- tibble(x = x, y = dnorm(x))\ndf2 &lt;- tibble(x = xx, y = dnorm(xx, mean = 3))\n\na1 &lt;- tibble(x = c(0,3), y = c(0.43, 0.43), label = c(\"H0\", \"H1\"))\na2 &lt;- tibble(x = c(2.05,1.25), y = c(0.02, 0.02), label = c(paste(\"alpha\"), \"beta\"))\n\ndf1 |&gt; \n  ggplot(aes(x, y)) +\n    stat_function(fun = dt_one, geom = \"area\", fill = \"red\") +\n    geom_line() +\n    geom_line(aes(x, y), df2) +\n    stat_function(fun = dt_two, geom = \"area\", fill = \"green\") +\n    geom_vline(aes(xintercept = 1.645), lwd = 1.5) +\n    geom_text(data=a1, aes(x=x,y=y, label=label),\n              size=14 , fontface=\"bold.italic\" ) +\n    geom_text(data=a2, aes(x=x,y=y, label=label),\n              size=10 , fontface=\"bold.italic\", parse = T) +\n    theme_bw()"
  },
  {
    "objectID": "lectures/6.html#monte-carlo-power-analysis",
    "href": "lectures/6.html#monte-carlo-power-analysis",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Power Analysis",
    "text": "Monte Carlo Power Analysis\nGiven a Null and Alternative hypothesis, one can determine how often a you will reject the null hypothesis from a high number of simulated data.\nThis is done by simulating from a hypothesized alternative distribution, conducting a hypothesis test given that the null hypothesis is true, and determine how often do you reject the null hypothesis.\n\\[\nPower = \\frac{\\#\\ Rejected}{\\#\\ Simulated\\ Data}\n\\]"
  },
  {
    "objectID": "lectures/6.html#monte-carlo-power-analysis-1",
    "href": "lectures/6.html#monte-carlo-power-analysis-1",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Power Analysis",
    "text": "Monte Carlo Power Analysis\n\nConstruct an alternative hypothesized distribution (\\(\\mu = \\mu_a\\))\nSimulate from the alternative hypothesized distribution\nCompute the test statistic based on null hypothesis distribution\nDetermine and record if the test statistic is rejected or not\nRepeat steps 2-4 \\(N\\) times\nFind the proportion rejected from the simulation study"
  },
  {
    "objectID": "lectures/6.html#t-test-example",
    "href": "lectures/6.html#t-test-example",
    "title": "Monte Carlo Methods",
    "section": "t-test Example",
    "text": "t-test Example\nUse a Monte Carlo Hypothesis Test for \\(H_0: \\mu = 6\\) and \\(H_a: \\mu \\neq 6\\). Simulate fake data from 24 RV’s from \\(N(3, 9)\\). Testing how the distribution will look like assuming that the null hypothesis is true.\n\n\nCode\nx &lt;- rnorm(24, 3, 3)\n\n\nAnswer:\n\n\nCode\ntstat &lt;- (mean(x) - 6)/(sd(x)/sqrt(24))\n\n\nt_stat_sim &lt;- function(i){\n  x &lt;- rnorm(24, 6, 3)\n  tt &lt;- (mean(x) - 6)/(sd(x)/sqrt(24))\n  return(tt)\n}\nt_dist &lt;- sapply(1:100000, t_stat_sim)\n\nt_dist |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = tstat), col = \"red\")\n\nsum(abs(tstat) &lt; abs(t_dist))\n(sum(abs(tstat) &lt; abs(t_dist)) + 1) / (length(t_dist) + 1)"
  },
  {
    "objectID": "lectures/6.html#power-analysis-example",
    "href": "lectures/6.html#power-analysis-example",
    "title": "Monte Carlo Methods",
    "section": "Power Analysis Example",
    "text": "Power Analysis Example\n\n\nCode\nalpha &lt;- 0.05\ncv &lt;- qt(1-alpha/2, 23)\n\nt_stat_sim &lt;- function(i, mu){\n  x &lt;- rnorm(24, mu, 3)\n  tt &lt;- (mean(x) - 6)/(sd(x)/sqrt(24))\n  return(cv &lt; abs(tt))\n}\nmus &lt;- 0:12\npowers &lt;- c()\nfor(i in 1:13){\n  t_dist &lt;- sapply(1:100000, t_stat_sim, mu = mus[i])\n  powers &lt;- c(powers, mean(t_dist))\n}\n\ntibble(x = mus, y = powers) |&gt; \n  ggplot(aes(x, y)) +\n  geom_line() +\n  ylab(\"Power\") +\n  xlab(\"Alternative\")"
  },
  {
    "objectID": "lectures/6.html#resampling-techniques-1",
    "href": "lectures/6.html#resampling-techniques-1",
    "title": "Monte Carlo Methods",
    "section": "Resampling Techniques",
    "text": "Resampling Techniques\nResampling techniques involve methods that require to sample from the data, instead of the parametric model. Common Methods:\n\nPermutation Tests\nBoostrapping\nCross-Validation\nJackknifer"
  },
  {
    "objectID": "lectures/6.html#sampling-with-replacement",
    "href": "lectures/6.html#sampling-with-replacement",
    "title": "Monte Carlo Methods",
    "section": "Sampling with Replacement",
    "text": "Sampling with Replacement\nGiven a data set, we sample an observation to a new data set. The sampled observation can be resampled again to the new data set.\n\n\nCode\nx &lt;- 1:20\nsample(x, 10, replace = T)\n\n\n#&gt;  [1] 11 16  3 18  8  1  9 10 16 14"
  },
  {
    "objectID": "lectures/6.html#sampling-without-replacement",
    "href": "lectures/6.html#sampling-without-replacement",
    "title": "Monte Carlo Methods",
    "section": "Sampling without Replacement",
    "text": "Sampling without Replacement\nGiven a data set, we sample an observation to a new data set. The sampled observation cannot be resampled again to the new data set.\n\n\nCode\nx &lt;- 1:20\nsample(x, 10, replace = F)\n\n\n#&gt;  [1] 12  8  1  4  6  3 11 13 14 15"
  },
  {
    "objectID": "lectures/6.html#permutation-tests-1",
    "href": "lectures/6.html#permutation-tests-1",
    "title": "Monte Carlo Methods",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nPermutation tests conducts a statistical test by constructing the null distribution by rearranging the data points in a sample.\nNull hypothesis states that the rearrangements of the data points are random.\nAlternative hypothesis states that the rearrangement of the data points aren’t random."
  },
  {
    "objectID": "lectures/6.html#permutation-distributions",
    "href": "lectures/6.html#permutation-distributions",
    "title": "Monte Carlo Methods",
    "section": "Permutation Distributions",
    "text": "Permutation Distributions\n\n\nNull\n\\[\nF_x = F_y\n\\]\n\nAlternative\n\\[\nF_x \\neq F_y\n\\]"
  },
  {
    "objectID": "lectures/6.html#permutation-distributions-1",
    "href": "lectures/6.html#permutation-distributions-1",
    "title": "Monte Carlo Methods",
    "section": "Permutation Distributions",
    "text": "Permutation Distributions\nSuppose \\(\\{X_i, Y_i\\}^n_{i=1}\\) is an observed permutation, \\(X = \\{X_1, \\ldots, X_n\\}\\), \\(Y = \\{Y_1, \\ldots, Y_n\\}\\).\nThe Probability of any permuation is \\(1/n!\\).\nTherefore, for a statistic \\(T(X,Y)\\), a sampling distribution can be constructed by all the different permutations.\nA hypothesis test can be conducted by observing the proportion of more extreme values of the sample statistic."
  },
  {
    "objectID": "lectures/6.html#approximate-permutation-distribution",
    "href": "lectures/6.html#approximate-permutation-distribution",
    "title": "Monte Carlo Methods",
    "section": "Approximate Permutation Distribution",
    "text": "Approximate Permutation Distribution\nConstructing the distribution for the permutations can be challenging if the number of permutations is high! If \\(n=100\\), the number of permutations is \\(100!\\):\n\nfactorial(100)\n\n#&gt; [1] 9.332622e+157\n\n\nTherefore, simulation techniques are needed to approximate the p-value.\nBy randomly drawing from the sample, we can approximate the p-value."
  },
  {
    "objectID": "lectures/6.html#algorithm",
    "href": "lectures/6.html#algorithm",
    "title": "Monte Carlo Methods",
    "section": "Algorithm",
    "text": "Algorithm\n\nConstruct a new data set\nFix the predictor (\\(X\\)) variable and randomly assign a data point \\(Y\\) to the fixed \\(X\\)\nCompute a test statistic using the new data set and store the value\nRepeat steps 1 and 2 for \\(N\\) times\nCompute the test statistic from the empirical sample (un-permutated)\nCount how many permutated statistics that are more extreme than the sample test statistic (\\(m\\))\nCompute the Monte Carlo p-value\n\n\\[\np = \\frac{m +1}{N + 1}\n\\]"
  },
  {
    "objectID": "lectures/6.html#example-emperical-data",
    "href": "lectures/6.html#example-emperical-data",
    "title": "Monte Carlo Methods",
    "section": "Example: Emperical Data",
    "text": "Example: Emperical Data\n\n\nCode\npenguins |&gt; ggplot(aes(x=species, y = body_mass_g)) +\n  geom_boxplot() +\n  geom_jitter() +\n  labs(x = \"Species\", y = \"Body Mass\")"
  },
  {
    "objectID": "lectures/6.html#example-random-shuffling",
    "href": "lectures/6.html#example-random-shuffling",
    "title": "Monte Carlo Methods",
    "section": "Example: Random Shuffling",
    "text": "Example: Random Shuffling\n\n\nCode\npenguins |&gt; ggplot() +\n  labs(x = \"Species\", y = \"Body Mass\") + \n  geom_jitter(aes(species, shuffle(body_mass_g)))"
  },
  {
    "objectID": "lectures/6.html#example-randomemperical",
    "href": "lectures/6.html#example-randomemperical",
    "title": "Monte Carlo Methods",
    "section": "Example: Random/Emperical",
    "text": "Example: Random/Emperical\n\n\nCode\npenguins |&gt; ggplot(aes(x = species, y = body_mass_g)) +\n  labs(x = \"Species\", y = \"Body Mass\") + \n  geom_jitter(col = \"red\") +\n  geom_jitter(aes(species, shuffle(body_mass_g)))"
  },
  {
    "objectID": "lectures/6.html#example-randomemperical-1",
    "href": "lectures/6.html#example-randomemperical-1",
    "title": "Monte Carlo Methods",
    "section": "Example: Random/Emperical",
    "text": "Example: Random/Emperical\n\n\nCode\npenguins |&gt; ggplot(aes(x = species, y = body_mass_g)) +\n  labs(x = \"Species\", y = \"Body Mass\") + \n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(aes(species, shuffle(body_mass_g))) +\n  geom_jitter(col = \"red\")"
  },
  {
    "objectID": "lectures/6.html#anova",
    "href": "lectures/6.html#anova",
    "title": "Monte Carlo Methods",
    "section": "ANOVA",
    "text": "ANOVA\nWe want to determine if body mass of penguins are different for different species.\n\n\nCode\npenguins |&gt; aov(body_mass_g ~ species, data = _) |&gt; anova()\n\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: body_mass_g\n#&gt;            Df    Sum Sq  Mean Sq F value    Pr(&gt;F)    \n#&gt; species     2 145190219 72595110  341.89 &lt; 2.2e-16 ***\n#&gt; Residuals 330  70069447   212332                      \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/6.html#permutation-test",
    "href": "lectures/6.html#permutation-test",
    "title": "Monte Carlo Methods",
    "section": "Permutation Test",
    "text": "Permutation Test\n\n\nCode\nf_stat &lt;- penguins |&gt; \n  aov(body_mass_g ~ species, data = _) |&gt; \n  anova() |&gt; \n  _$`F value`[1]\n  \n\nf_sim &lt;- function(i){\n  ff &lt;- penguins |&gt; \n    aov(shuffle(body_mass_g) ~ species, data = _) |&gt; \n    anova() |&gt; \n    _$`F value`[1]\n  return(ff)\n}\n\nf_dist &lt;- replicate(10000, f_sim(1))\n\ntibble(x= f_dist) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_vline(xintercept = f_stat)\n\n\n\nCode\nsum(f_stat &lt; f_dist) + 1 / (length(f_dist) + 1)\n\n\n#&gt; [1] 9.999e-05"
  },
  {
    "objectID": "lectures/6.html#permutation-example",
    "href": "lectures/6.html#permutation-example",
    "title": "Monte Carlo Methods",
    "section": "Permutation Example",
    "text": "Permutation Example\nIs there a linear relationship between flavor and aroma in coffee drinks from the coffee_aroma data set.\n\n\nCode\ncoffee_aroma |&gt; ggplot(aes(x=aroma, y = flavor)) +\n  geom_point() + theme_bw() +\n  geom_smooth(method = \"lm\", se = F)"
  },
  {
    "objectID": "lectures/6.html#permutation-linear-regression",
    "href": "lectures/6.html#permutation-linear-regression",
    "title": "Monte Carlo Methods",
    "section": "Permutation Linear Regression",
    "text": "Permutation Linear Regression\n\nKeep the predictor values fixed (unchanged)\nRandomly assign the sampled outcome values to a fixed predictor\nCompute the regression coefficients for the predictor variable"
  },
  {
    "objectID": "lectures/6.html#simulated-permutation",
    "href": "lectures/6.html#simulated-permutation",
    "title": "Monte Carlo Methods",
    "section": "Simulated Permutation",
    "text": "Simulated Permutation\n\n\nCode\ncoffee_aroma |&gt; ggplot(aes(x=aroma, y = shuffle(flavor))) +\n  geom_point() + theme_bw() +\n  geom_smooth(method = \"lm\", se = F)"
  },
  {
    "objectID": "lectures/6.html#permutations",
    "href": "lectures/6.html#permutations",
    "title": "Monte Carlo Methods",
    "section": "Permutations",
    "text": "Permutations\n\n\nCode\ncoffee_aroma |&gt; ggplot() +\n  geom_smooth(mapping = aes(aroma, flavor), method = \"lm\", se = F, col = \"red\") +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F) +\n  geom_smooth(mapping = aes(aroma, shuffle(flavor)), method = \"lm\", se = F)"
  },
  {
    "objectID": "lectures/6.html#permutation-test-1",
    "href": "lectures/6.html#permutation-test-1",
    "title": "Monte Carlo Methods",
    "section": "Permutation Test",
    "text": "Permutation Test\n\n\nCode\nf_stat &lt;- coffee_aroma |&gt; \n  lm(flavor ~ aroma, data = _) |&gt; \n  _$`coefficients`[2]\n\n\n\nf_sim &lt;- function(i){\n  ff &lt;- coffee_aroma |&gt; \n  lm(shuffle(flavor) ~ aroma, data = _) |&gt; \n  _$`coefficients`[2]\n  return(ff)\n}\n\nf_dist &lt;- replicate(10000, f_sim(1))\n\ntibble(x= f_dist) |&gt; \n  ggplot(aes(x, y = ..density..)) +\n  geom_histogram() +\n  geom_vline(xintercept = f_stat)\n\n\n\n\n\n\n\n\n\nCode\nsum(f_stat &lt; f_dist) + 1 / (length(f_dist) + 1)\n\n\n#&gt; [1] 9.999e-05"
  },
  {
    "objectID": "lectures/1.html#introductions",
    "href": "lectures/1.html#introductions",
    "title": "Welcome to Math 497",
    "section": "Introductions",
    "text": "Introductions\n\nSan Bernardino, CA\nCSU Monterey Bay\n\nBS Biology\n\nSan Diego State University\n\nMaster’s in Public Health\n\nUC Riverside\n\nPhD in Applied Statistics"
  },
  {
    "objectID": "lectures/1.html#introductions-1",
    "href": "lectures/1.html#introductions-1",
    "title": "Welcome to Math 497",
    "section": "Introductions",
    "text": "Introductions\n\nName\nYear\nMajor\nFun Fact\nCareer Goal"
  },
  {
    "objectID": "lectures/1.html#goals-for-the-course",
    "href": "lectures/1.html#goals-for-the-course",
    "title": "Welcome to Math 497",
    "section": "Goals for the Course",
    "text": "Goals for the Course\n\nGain R Programming Skills\nLearn Different Monte Carlo Methods\nConduct Simulation Studies"
  },
  {
    "objectID": "lectures/1.html#monte-carlo-methods",
    "href": "lectures/1.html#monte-carlo-methods",
    "title": "Welcome to Math 497",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods\nMonte Carlo Methods is a way to simulate a complex probability distribution using commonly used random variables."
  },
  {
    "objectID": "lectures/1.html#syllabus-1",
    "href": "lectures/1.html#syllabus-1",
    "title": "Welcome to Math 497",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus"
  },
  {
    "objectID": "lectures/1.html#books",
    "href": "lectures/1.html#books",
    "title": "Welcome to Math 497",
    "section": "Books",
    "text": "Books\n\nStatistical Computing (SC)\n\nIsaac Quintanilla Salinas\nhttps://www.inqs.info/stat_comp/\nhttps://hypothes.is/groups/xMmDdj2A/m408"
  },
  {
    "objectID": "lectures/1.html#r-programming",
    "href": "lectures/1.html#r-programming",
    "title": "Welcome to Math 497",
    "section": "R Programming",
    "text": "R Programming\nR is a statistical programming package that allows you to conduct different types of analysis.\nR"
  },
  {
    "objectID": "lectures/1.html#rstudio",
    "href": "lectures/1.html#rstudio",
    "title": "Welcome to Math 497",
    "section": "RStudio",
    "text": "RStudio\nA piece of software that organizes how you conduct statistical analysis in R.\nRStudio"
  },
  {
    "objectID": "lectures/1.html#posit-cloud",
    "href": "lectures/1.html#posit-cloud",
    "title": "Welcome to Math 497",
    "section": "Posit Cloud",
    "text": "Posit Cloud\nA web version of RStudio.\nPosit Cloud"
  },
  {
    "objectID": "lectures/1.html#r-packages",
    "href": "lectures/1.html#r-packages",
    "title": "Welcome to Math 497",
    "section": "R Packages",
    "text": "R Packages\n\nTidyverse\ncsucistats\n\n\ninstall.packages('csucistats', \n                 repos = c('https://inqs909.r-universe.dev', \n                           'https://cloud.r-project.org'))"
  },
  {
    "objectID": "lectures/1.html#r-as-a-calculator",
    "href": "lectures/1.html#r-as-a-calculator",
    "title": "Welcome to Math 497",
    "section": "R as a calculator",
    "text": "R as a calculator\nR can evaluate different expressions in the console tab.\nTry the following:\n\n\\(4(4+2)/34\\)\n\\(6^3\\)\n\\(3-1\\)\n\\(4+4/3+45(32*34-54)\\)"
  },
  {
    "objectID": "lectures/1.html#r-calculator",
    "href": "lectures/1.html#r-calculator",
    "title": "Welcome to Math 497",
    "section": "R Calculator",
    "text": "R Calculator\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#r-functions",
    "href": "lectures/1.html#r-functions",
    "title": "Welcome to Math 497",
    "section": "R Functions",
    "text": "R Functions\nR functions performs tasks to specific data values.\nEvaluate the following values in R:\n\n\\(\\sqrt{3}\\)\n\\(e^3\\)\n\\(\\ln(53)\\)\n\\(\\log(324)\\)\n\\(\\sin(3)\\)\n\\(\\sin(3\\pi)\\)"
  },
  {
    "objectID": "lectures/1.html#r-functions-1",
    "href": "lectures/1.html#r-functions-1",
    "title": "Welcome to Math 497",
    "section": "R Functions",
    "text": "R Functions\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#types-of-data",
    "href": "lectures/1.html#types-of-data",
    "title": "Welcome to Math 497",
    "section": "Types of Data",
    "text": "Types of Data\n\nNumeric\nCharacter\nLogical\nMissing\n\nEvaluate the following code:\n\nis.numeric(1)\nis.numeric(\"1\")\nis.numeric(T)\nis.numeric(NA)"
  },
  {
    "objectID": "lectures/1.html#types-of-data-1",
    "href": "lectures/1.html#types-of-data-1",
    "title": "Welcome to Math 497",
    "section": "Types of Data",
    "text": "Types of Data\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#types-of-objects",
    "href": "lectures/1.html#types-of-objects",
    "title": "Welcome to Math 497",
    "section": "Types of Objects",
    "text": "Types of Objects\nIn R, an object contains a set of data. The most common types are vectors and matrix.\nRun this code and print out the objects in the console:\n\nx &lt;- 3:34\ny &lt;- matrix(1:20, nrow = 4)"
  },
  {
    "objectID": "lectures/1.html#types-of-objects-1",
    "href": "lectures/1.html#types-of-objects-1",
    "title": "Welcome to Math 497",
    "section": "Types of objects",
    "text": "Types of objects\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#vectors",
    "href": "lectures/1.html#vectors",
    "title": "Welcome to Math 497",
    "section": "Vectors",
    "text": "Vectors\nUse the c() function to create a container of data objects.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#data-frames",
    "href": "lectures/1.html#data-frames",
    "title": "Welcome to Math 497",
    "section": "Data Frames",
    "text": "Data Frames\nData frames can be thought of as R’s version of a data set.\nPlay around with mtcars:\n\nmtcars \n\n#&gt;                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n#&gt; Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n#&gt; Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n#&gt; Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n#&gt; Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n#&gt; Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n#&gt; Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n#&gt; Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n#&gt; Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n#&gt; Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n#&gt; Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n#&gt; Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n#&gt; Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n#&gt; Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n#&gt; Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n#&gt; Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n#&gt; Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n#&gt; Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n#&gt; Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n#&gt; Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n#&gt; Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n#&gt; AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n#&gt; Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n#&gt; Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n#&gt; Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n#&gt; Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n#&gt; Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n#&gt; Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n#&gt; Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n#&gt; Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n#&gt; Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "lectures/1.html#lists",
    "href": "lectures/1.html#lists",
    "title": "Welcome to Math 497",
    "section": "Lists",
    "text": "Lists\nList can be thought as an extended vector, but each element is a different R object.\nTry playing with this R object:\n\nlist_one &lt;- list(mtcars, rep(0, 4),\n                 diag(rep(1, 3)))"
  },
  {
    "objectID": "lectures/1.html#lists-1",
    "href": "lectures/1.html#lists-1",
    "title": "Welcome to Math 497",
    "section": "Lists",
    "text": "Lists\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#control-flow",
    "href": "lectures/1.html#control-flow",
    "title": "Welcome to Math 497",
    "section": "Control Flow",
    "text": "Control Flow\nThe order a computer will complete tasks.\nUsually incorporates statements and loops."
  },
  {
    "objectID": "lectures/1.html#indexing-1",
    "href": "lectures/1.html#indexing-1",
    "title": "Welcome to Math 497",
    "section": "Indexing",
    "text": "Indexing\nWithin an R object, you can access an element by indexing it.\nIndexing tells R which values to output."
  },
  {
    "objectID": "lectures/1.html#vectors-1",
    "href": "lectures/1.html#vectors-1",
    "title": "Welcome to Math 497",
    "section": "Vectors",
    "text": "Vectors\nA vector can be indexed by adding [] after the object’s name and specifying the number of each element.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#matrices",
    "href": "lectures/1.html#matrices",
    "title": "Welcome to Math 497",
    "section": "Matrices",
    "text": "Matrices\nA matrix can be indexed by adding [] after the object’s name and specifying the number of each element. Separate the values by commas for specific indexes.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#comparing-numbers-1",
    "href": "lectures/1.html#comparing-numbers-1",
    "title": "Welcome to Math 497",
    "section": "Comparing Numbers",
    "text": "Comparing Numbers\nYou can compare two numbers, or objects, that will result in a logical output."
  },
  {
    "objectID": "lectures/1.html#comparing-numbers-operators",
    "href": "lectures/1.html#comparing-numbers-operators",
    "title": "Welcome to Math 497",
    "section": "Comparing Numbers Operators",
    "text": "Comparing Numbers Operators\n\n\n\nOperator\nDescription\n\n\n\n\n&gt;\nGreater Than\n\n\n&lt;\nLess Than\n\n\n&gt;=\nGreater than or equal\n\n\n&lt;=\nLess than or equal\n\n\n==\nEquals\n\n\n!=\nNot Equals"
  },
  {
    "objectID": "lectures/1.html#comparing-vectors",
    "href": "lectures/1.html#comparing-vectors",
    "title": "Welcome to Math 497",
    "section": "Comparing Vectors",
    "text": "Comparing Vectors\nWhen you compare a number to a vector, it will result as a logical vector."
  },
  {
    "objectID": "lectures/1.html#example",
    "href": "lectures/1.html#example",
    "title": "Welcome to Math 497",
    "section": "Example",
    "text": "Example\nTry the following code and explain what is happening:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#ifelse-statements-1",
    "href": "lectures/1.html#ifelse-statements-1",
    "title": "Welcome to Math 497",
    "section": "if/else Statements",
    "text": "if/else Statements\nif/else statements are used to conduct specific tasks depending on the conditions"
  },
  {
    "objectID": "lectures/1.html#if-statement",
    "href": "lectures/1.html#if-statement",
    "title": "Welcome to Math 497",
    "section": "if Statement",
    "text": "if Statement\nAn if statement is used to if you want R to perform a specific function if a certain condition is met. An if statement will only run a task if a logical is returned. You will need type if, followed by the condition (as a logical) in parentheses, then the task."
  },
  {
    "objectID": "lectures/1.html#example-1",
    "href": "lectures/1.html#example-1",
    "title": "Welcome to Math 497",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#else-statement",
    "href": "lectures/1.html#else-statement",
    "title": "Welcome to Math 497",
    "section": "else statement",
    "text": "else statement\nAn else statement will conduct a different task if the if statement does not conduct the tasks."
  },
  {
    "objectID": "lectures/1.html#example-2",
    "href": "lectures/1.html#example-2",
    "title": "Welcome to Math 497",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#chain-ifelse-statement",
    "href": "lectures/1.html#chain-ifelse-statement",
    "title": "Welcome to Math 497",
    "section": "Chain if/else statement",
    "text": "Chain if/else statement\nIf you have more than two options, you can chain if/else statements by adding an if statement immediately after the word else."
  },
  {
    "objectID": "lectures/1.html#example-3",
    "href": "lectures/1.html#example-3",
    "title": "Welcome to Math 497",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#for-loops-1",
    "href": "lectures/1.html#for-loops-1",
    "title": "Welcome to Math 497",
    "section": "for Loops",
    "text": "for Loops\nfor loops are used to conduct an iterative task with slight changes to the input. The general format goes as follows:\n\nfor (index in vector){\n  Conduct task\n}\n\nYou will repeat the for loop untie all the elements in the vector have been used."
  },
  {
    "objectID": "lectures/1.html#example-4",
    "href": "lectures/1.html#example-4",
    "title": "Welcome to Math 497",
    "section": "Example",
    "text": "Example\nCompute the mean:\n\\[\n\\bar x = \\frac{1}{n}\\sum^n_{i=1}x_i\n\\]\n\nx &lt;- rnorm(100)\nmean(x)\n\n#&gt; [1] 0.1260148"
  },
  {
    "objectID": "lectures/1.html#example-5",
    "href": "lectures/1.html#example-5",
    "title": "Welcome to Math 497",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#example-6",
    "href": "lectures/1.html#example-6",
    "title": "Welcome to Math 497",
    "section": "Example",
    "text": "Example\nCompute the variance:\n\\[\ns^2 = \\frac{1}{n-1}\\sum^n_{i-1}(x_i-\\bar x)^2\n\\]\n\nx &lt;- rnorm(100)\nvar(x)\n\n#&gt; [1] 1.006068"
  },
  {
    "objectID": "lectures/1.html#example-7",
    "href": "lectures/1.html#example-7",
    "title": "Welcome to Math 497",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/1.html#next-statements-1",
    "href": "lectures/1.html#next-statements-1",
    "title": "Welcome to Math 497",
    "section": "next Statements",
    "text": "next Statements\nThe next statement is used to skip an iteration of a loop. This is used along an if statement.\n\nfor (i in vector){\n  perform task\n  if (condition){\n    next\n  } else {\n    perform task\n  }\n}"
  },
  {
    "objectID": "lectures/1.html#break-statements-1",
    "href": "lectures/1.html#break-statements-1",
    "title": "Welcome to Math 497",
    "section": "break Statements",
    "text": "break Statements\nThe break statement is used to stop a loop if the condition is met. This is used along with an if statement.\n\nfor (i in vector){\n  perform task\n  if (condition){\n    break\n  } else {\n    perform task\n  }\n}"
  },
  {
    "objectID": "lectures/2.html#homework",
    "href": "lectures/2.html#homework",
    "title": "Functional Programming",
    "section": "Homework",
    "text": "Homework\nHomework 1 is assigned and can be found here: http://m497.inqs.info/hw.html"
  },
  {
    "objectID": "lectures/2.html#quarto-documentation",
    "href": "lectures/2.html#quarto-documentation",
    "title": "Functional Programming",
    "section": "Quarto Documentation",
    "text": "Quarto Documentation\nquarto use template inqs909/qs_hw"
  },
  {
    "objectID": "lectures/2.html#quarto-resources",
    "href": "lectures/2.html#quarto-resources",
    "title": "Functional Programming",
    "section": "Quarto Resources",
    "text": "Quarto Resources\n\nhttps://www.inqs.info/stat_comp/document.html\nhttps://quarto.org/docs/output-formats/html-basics.html"
  },
  {
    "objectID": "lectures/2.html#nested-for-loops-1",
    "href": "lectures/2.html#nested-for-loops-1",
    "title": "Functional Programming",
    "section": "Nested for Loops",
    "text": "Nested for Loops\nNested for loops are for loops within another for loop. You can stack these loops as much as needed. Just make sure the index is different for each loop. The general format for a loop goes as follow:\n\nfor (i in vector_1){\n  for (ii in vector_2){\n    perform task\n  }\n}"
  },
  {
    "objectID": "lectures/2.html#example",
    "href": "lectures/2.html#example",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nWithout using the sd() function, compute the standard deviation for each column of the matrix:\n\nx &lt;- matrix(rnorm(1000), nrow = 10)\n\n\\[\ns^2 = \\frac{1}{n-1}\\sum^n_{i=1}(x_i-\\bar x)^2\n\\]"
  },
  {
    "objectID": "lectures/2.html#example-1",
    "href": "lectures/2.html#example-1",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/2.html#while-loops-1",
    "href": "lectures/2.html#while-loops-1",
    "title": "Functional Programming",
    "section": "while Loops",
    "text": "while Loops\nA while loop is a combination of a for loop and a break statement. The loop will continue indefinitely until a condition becomes false.\n\n# Initial Condition\ncondition &lt;- starting TRUE condition\n\nwhile (condition){\n  perform task\n  condition &lt;- update condition\n}"
  },
  {
    "objectID": "lectures/2.html#example-2",
    "href": "lectures/2.html#example-2",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nSimulate from a \\(N(0,1)\\) distribution until you have 50 positive numbers.\n\n\nCode\nn &lt;- 0\npos_num &lt;- c()\n\nwhile (n &lt; 51){\n  x &lt;- rnorm(1)\n  if (x &gt; 0) {\n    pos_num &lt;- c(pos_num, x)\n    n &lt;- n + 1\n  }\n}\n\npos_num"
  },
  {
    "objectID": "lectures/2.html#example-3",
    "href": "lectures/2.html#example-3",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nFind the value of \\(x\\) where the function \\(y=1/x\\) relative converges (\\(\\frac{|y_{old}-y_{new}|}{y_{old}}\\)) at a level of \\(10^-6\\) as \\(x\\rightarrow \\infty\\).\n\n\nCode\ndiff &lt;- 10\nx &lt;- 2\ny_old &lt;- 1\nwhile (diff &gt; 1e-6){\n  y_new &lt;- 1 / x\n  diff &lt;- abs(y_old - y_new) / y_old\n  x &lt;- x + 1\n  y_old &lt;- y_new\n}"
  },
  {
    "objectID": "lectures/2.html#example-4",
    "href": "lectures/2.html#example-4",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nSimulate from a \\(Binom(1,.2)\\) distribution until the sum of the random variables generated is 50.\n\n\nCode\nsum_bin &lt;- 0\nx &lt;- c()\nwhile (sum_bin &lt;51) {\n  x &lt;- c(x, rbinom(1, 1, 0.2))\n  sum_bin &lt;- sum(x)\n}\nsum_bin\nlength(x)"
  },
  {
    "objectID": "lectures/2.html#example-5",
    "href": "lectures/2.html#example-5",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/2.html#example-6",
    "href": "lectures/2.html#example-6",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nUsing the code below:\n\nx &lt;- rnorm(5000)\n\nCreate a new vector containing all the positive values of x. The new vector should be less than 5000.\nAnswer:\n\n\nCode\nnn &lt;- length(x)\npos &lt;- c()\nfor (i in 1:nn){\n  if (x[i] &gt; 0) {\n    pos &lt;- c(pos, x[i])\n  }\n}\npos"
  },
  {
    "objectID": "lectures/2.html#example-7",
    "href": "lectures/2.html#example-7",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\n\\[\nf(x,y) = x^2 + y^2 + \\ln(x+y)\n\\]\nFind all the values of \\(f(x,y)\\) for every combination of \\(x \\in \\{1, 8, 13, 25, 42, 67, 95\\}\\) and \\(y \\in \\{6, 12, 18, 52, 61, 79, 83\\}\\)\nStore values in a \\(7\\times 7\\) matrix.\nAnswer:\n\n\nCode\nx &lt;- c(1, 8, 13, 25, 42, 67, 95)\ny &lt;- c(6, 12, 18, 52, 61, 79, 83)\nres &lt;- matrix(nrow = 7, ncol = 7)\ncolnames(res) &lt;- as.character(x)\nrownames(res) &lt;- as.character(y)\n\nfor (i in 1:7){\n  for (ii in 1:7){\n    res[ii,i] &lt;- x[i]^2 + y[ii]^2 + log(x[i] + y[ii])\n  }\n}\nprint(res)"
  },
  {
    "objectID": "lectures/2.html#vectorized-code-1",
    "href": "lectures/2.html#vectorized-code-1",
    "title": "Functional Programming",
    "section": "Vectorized Code",
    "text": "Vectorized Code\nVectorized code is programming where functions or processes are applied to vectors instead of individual values.\n\nIndicating a loop is not necessary to apply a function to each individual element in a vector."
  },
  {
    "objectID": "lectures/2.html#vectorized-code-2",
    "href": "lectures/2.html#vectorized-code-2",
    "title": "Functional Programming",
    "section": "Vectorized Code",
    "text": "Vectorized Code\nMathematical Operations are conducted element-wise to 2 or more vectors\n\nElement 1 in vector x is applied to element 1 in vector y"
  },
  {
    "objectID": "lectures/2.html#example-8",
    "href": "lectures/2.html#example-8",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/2.html#example-9",
    "href": "lectures/2.html#example-9",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/2.html#built-in-functions-1",
    "href": "lectures/2.html#built-in-functions-1",
    "title": "Functional Programming",
    "section": "Built-in Functions",
    "text": "Built-in Functions\nThere are several available functions in R to conduct specific statistical methods or tasks"
  },
  {
    "objectID": "lectures/2.html#help-documentation",
    "href": "lectures/2.html#help-documentation",
    "title": "Functional Programming",
    "section": "Help Documentation",
    "text": "Help Documentation\n\n\n\n\n\n\n\nSection\nDescription\n\n\n\n\nDescription\nProvides a brief introduction of the function\n\n\nUsage\nProvides potential usage of the function\n\n\nArguments\nArguments that the function can take\n\n\nDetails\nAn in depth description of the function\n\n\nValue\nProvides information of the output produced by the function\n\n\nNotes\nAny need to know information about the function\n\n\nAuthors\nDevelopers of the function\n\n\nReferences\nReferences to the model and function\n\n\nSee Also\nProvide information of supporting functions\n\n\nExamples\nExamples of the function"
  },
  {
    "objectID": "lectures/2.html#generic-functions",
    "href": "lectures/2.html#generic-functions",
    "title": "Functional Programming",
    "section": "Generic Functions",
    "text": "Generic Functions\nSeveral R objects have a known class attached to it. A specialized object designed to be read by generic functions, such as summary() and plot().\nFor example, the summary() is a generic for several types of functions: summary.aov(), summary.lm(), summary.glm(), and many more."
  },
  {
    "objectID": "lectures/2.html#commonly-used-function",
    "href": "lectures/2.html#commonly-used-function",
    "title": "Functional Programming",
    "section": "Commonly-used Function",
    "text": "Commonly-used Function\n\n\n\nFunctions\nDescription\n\n\n\n\naov()\nFits an ANOVA Model\n\n\nlm()\nFits a linear model\n\n\nglm()\nFits a general linear model\n\n\nt.test()\nConducts a t-test"
  },
  {
    "objectID": "lectures/2.html#user-built-functions-1",
    "href": "lectures/2.html#user-built-functions-1",
    "title": "Functional Programming",
    "section": "User-built functions",
    "text": "User-built functions\n\nFunctions created by the user for analysis\nNeeds to be ran once to the R environment\nWill be lost when R session is closed"
  },
  {
    "objectID": "lectures/2.html#anatomy",
    "href": "lectures/2.html#anatomy",
    "title": "Functional Programming",
    "section": "Anatomy",
    "text": "Anatomy\n\nname_of_function &lt;- function(data_1, data_2 = NULL, \n                             argument_1, argument_2 = TRUE, argument_3 = NULL,\n                             ...){\n  # Conduct Task\n  # Conduct Task\n  output_object &lt;- Tasks\n  return(output_object)\n}\n\n\n\nfunction: used to construct the function\ndata1: first data argument that needs to supplied\ndata2: second data argument that does not need to be supplied\nargument1: first argument must be supplied to alter function\nargument2: second argument to alter function, set to TRUE\nargument3: third argument that does not need to be supplied\n…: additional arguments supplied to other functions"
  },
  {
    "objectID": "lectures/2.html#example-10",
    "href": "lectures/2.html#example-10",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nCreate a function for\n\\[\ny = \\ln(x^2)\n\\]"
  },
  {
    "objectID": "lectures/2.html#example-11",
    "href": "lectures/2.html#example-11",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nCreate a function for\n\\[\nf(x) = \\left\\{\\begin{array}{cc}\nx^3 & x&lt;0\\\\\nx^2 + 5 & \\mathrm{otherwise}\n\\end{array} \\right.\n\\]"
  },
  {
    "objectID": "lectures/2.html#example-12",
    "href": "lectures/2.html#example-12",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nCreate a function for\n\\[\nf(x,y) = \\left\\{\\begin{array}{cc}\nx^3 e^y &  x&lt;0\\ \\\\\nx^2 + 5 + \\ln(y) & \\mathrm{otherwise}\n\\end{array} \\right.\n\\]"
  },
  {
    "objectID": "lectures/2.html#example-13",
    "href": "lectures/2.html#example-13",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nCreate the function that allows your to compute the z-score of a specific value x using the sampling distribution from a set of data (y vector):\n\\[\nz =  \\frac{x-\\bar y}{\\sqrt{s^2_{y}/n_y}}\n\\]"
  },
  {
    "objectID": "lectures/2.html#example-14",
    "href": "lectures/2.html#example-14",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/2.html#apply-1",
    "href": "lectures/2.html#apply-1",
    "title": "Functional Programming",
    "section": "apply()",
    "text": "apply()\nThe apply function returns a vector, array, or list of values by applying a function to the margins of an array. You will need to specify the following arguments:\n\nX: an array to be indexed and applied\nMARGIN: specifyng which index(es) to subset by\nFUN: function to be applied\n…: further arguments to be applied to FUN, must be labeled\n\n\napply(X, MARGIN, FUN, ...)"
  },
  {
    "objectID": "lectures/2.html#example-15",
    "href": "lectures/2.html#example-15",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nFind the standard deviation of all the columns of the following matrix:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/2.html#example-16",
    "href": "lectures/2.html#example-16",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nFind the \\(25th\\), \\(50th\\), and \\(75th\\) quartiles for each row of the following matrix:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/2.html#lapply-1",
    "href": "lectures/2.html#lapply-1",
    "title": "Functional Programming",
    "section": "lapply()",
    "text": "lapply()\nThe lapply function applies a function to all the elements of a vector or matrix, and it will return a list. You will need to specify the following arguments:\n\nX: object to be iterated\nFUN: a function to be applied\n…: further arguments to be passed along to FUN\n\n\nlapply(X, FUN, ...)"
  },
  {
    "objectID": "lectures/2.html#example-17",
    "href": "lectures/2.html#example-17",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nCreate a function that returns a labeled list for with the following values: mean, standard deviation, median, max, and min."
  },
  {
    "objectID": "lectures/2.html#sapply-1",
    "href": "lectures/2.html#sapply-1",
    "title": "Functional Programming",
    "section": "sapply()",
    "text": "sapply()\nThe sapply() function will apply a function to each element of a list or vector, and it will return a simplified object, vector, matrix, or array. The sapply() function uses 4 main arguments:\n\nX: a vector or list to be iterated\nFUN: a function to be applied\n…: arguments passed along to FUN, must be labeled\nsimplify: indicates how to simplify the function, defaults to n-dimensional array based on output\n\n\nsapply(X, FUN, ..., simplify = TRUE)"
  },
  {
    "objectID": "lectures/2.html#example-18",
    "href": "lectures/2.html#example-18",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nUsing the vector below, compute the length of each string using sapply and str_length() from stringr\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/2.html#example-19",
    "href": "lectures/2.html#example-19",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nUsing the list generated below, compute the mean of each element of the list using sapply.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/2.html#example-20",
    "href": "lectures/2.html#example-20",
    "title": "Functional Programming",
    "section": "Example",
    "text": "Example\nUsing the vector below, use the sapply() to find \\(\\log(x)\\) for each value and return a matrix:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Monte Carlo Methods!",
    "section": "",
    "text": "Brief Introduction\n\n\n\n\n\nWelcome to the course! This is the home page of the course where I will provide a recap on what was covered in the week. Here I will post any documents or videos for your reference. If you have any questions, please email me at isaac.qs@csuci.edu.\n\n\n\n\n\n\n\n\n\nQuarto Template for HW\n\n\n\n\n\nDownload it here: Github Repo\nOR Type this in the RStudio terminal:\nquarto use template inqs909/m408_hw\nType Y for the trust author. Type a name of a new directory in where to save the file. For example, type hw1.\nOR Save this in an empty source quarto document:\n---\ntitle: \"Title\"\nauthor: \"Name Here\"\ndate: \"`r format(Sys.time(),'%m-%d-%Y')`\"\nformat: \n  html:\n    toc: true\n    toc-depth: 2\n    code-fold: true\n    code-tools: true\n    code-line-numbers: true\n    embed-resources: true\nknitr:\n  opts_chunk:\n    echo: true\n    message: false\n    warning: false\n    error: true\n    tidy: styler\n    R.options:\n      digits: 3\n      max.print: 100\n---\n\n## Problem 1\n\n## Problem 2\n\n## Problem 3\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Statistics Book\n\n\n\n\n\nhttps://openintro-ims.netlify.app/\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\n\nMEET Wednesday (7/3)\n\n\n\n\n\nApr 24, 2028\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\n\nMEET Tuesday (7/16)\n\n\n\n\n\nNov 24, 2027\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\n\nMEET Wednesday (7/10)\n\n\n\n\n\nOct 24, 2027\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\n\nMEET Wednesday (6/26)\n\n\n\n\n\nJun 24, 2027\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\n\nMEET Tuesday (6/18)\n\n\n\n\n\nJan 24, 2027\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\nMEET Wednesday (6/12)\n\n\n\n\n\nJun 7, 2024\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\nMEET TUESDAY (6/4) INSTEAD OF WED.\n\n\n\n\n\nJun 1, 2024\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\nThis week we will dive into R and control flow programming.\n\n\n\n\n\nMay 22, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/8.html",
    "href": "posts/8.html",
    "title": "Week 8",
    "section": "",
    "text": "Simulation Study\nLinear Regression\nGeneralized Linear Model\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide"
  },
  {
    "objectID": "posts/8.html#learning-outcomes",
    "href": "posts/8.html#learning-outcomes",
    "title": "Week 8",
    "section": "",
    "text": "Simulation Study\nLinear Regression\nGeneralized Linear Model\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide"
  },
  {
    "objectID": "posts/4.html",
    "href": "posts/4.html",
    "title": "Week 4",
    "section": "",
    "text": "Monte Carlo Integration\nImportance Sampling\nMonte Carlo Optimization\nStochastic Gradient Methods\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo 1 | Video 2 | Video 3"
  },
  {
    "objectID": "posts/4.html#learning-outcomes",
    "href": "posts/4.html#learning-outcomes",
    "title": "Week 4",
    "section": "",
    "text": "Monte Carlo Integration\nImportance Sampling\nMonte Carlo Optimization\nStochastic Gradient Methods\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo 1 | Video 2 | Video 3"
  },
  {
    "objectID": "posts/4.html#further-reading",
    "href": "posts/4.html#further-reading",
    "title": "Week 4",
    "section": "Further Reading",
    "text": "Further Reading\n\nAvailable from Broome Library\nIntroducing Monte Carlo Methods in R: Chapter 3 and 5\nExplorations in Monte Carlo Methods: Chapter 5 and 6\nComputational Statistics: Chapter 4\n\n\nGreat Books (Don’t Buy)\nSimulation and the Monte Carlo Method: Chapter 5\nStatistical Computing in R: Chapter 6\n\n\nWebsites\nDartmouth"
  },
  {
    "objectID": "posts/7.html",
    "href": "posts/7.html",
    "title": "Week 7",
    "section": "",
    "text": "Bootstrapping Methods\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo 1 | Video 2"
  },
  {
    "objectID": "posts/7.html#learning-outcomes",
    "href": "posts/7.html#learning-outcomes",
    "title": "Week 7",
    "section": "",
    "text": "Bootstrapping Methods\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo 1 | Video 2"
  },
  {
    "objectID": "posts/7.html#further-reading",
    "href": "posts/7.html#further-reading",
    "title": "Week 7",
    "section": "Further Reading",
    "text": "Further Reading\n\nAvailable from Broome Library\nComputational Statistics: Chapter 13\n\n\nGreat Books (Don’t Buy)\nStatistical Computing in R: Chapter 8\n\n\nWebsites"
  },
  {
    "objectID": "posts/3.html",
    "href": "posts/3.html",
    "title": "Week 3",
    "section": "",
    "text": "Random Variables\nRandom Number Generation\nRandom Variable Generations\nCentral Limit Theorem"
  },
  {
    "objectID": "posts/3.html#learning-outcomes",
    "href": "posts/3.html#learning-outcomes",
    "title": "Week 3",
    "section": "",
    "text": "Random Variables\nRandom Number Generation\nRandom Variable Generations\nCentral Limit Theorem"
  },
  {
    "objectID": "posts/3.html#resources",
    "href": "posts/3.html#resources",
    "title": "Week 3",
    "section": "Resources",
    "text": "Resources\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo"
  },
  {
    "objectID": "posts/3.html#further-reading",
    "href": "posts/3.html#further-reading",
    "title": "Week 3",
    "section": "Further Reading",
    "text": "Further Reading\n\nAvailable from Broome Library\nIntroducing Monte Carlo Methods in R: Chapter 2\nExplorations in Monte Carlo Methods: Chapter 2\nComputational Statistics: Chapter 7\n\n\nGreat Books (Don’t Buy)\nSimulation and the Monte Carlo Method: Chapter 2\nStatistical Computing in R: Chapter 3\n\n\nWebsites\nStatLect"
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "A list of recommended books to learn more about Statistics, the majority are freely available from the Broome Library:"
  },
  {
    "objectID": "books.html#basics",
    "href": "books.html#basics",
    "title": "Books",
    "section": "Basics",
    "text": "Basics\n\nIntroduction to Statistics and Data Analysis\n\nHeumann and Shalabh\n\nStatistical Foundations, Reasoning and Inference\n\nKauermann, Küchenhoff, and Heumann"
  },
  {
    "objectID": "books.html#regression",
    "href": "books.html#regression",
    "title": "Books",
    "section": "Regression",
    "text": "Regression\n\nGeneralized Linear Models With Examples in R\n\nDunn and Smyth\nGraduate\n\nLinear and Generalized Linear Mixed Models and Their Applications (2nd Edition)\n\nJiang and Nguyen\nGraudate\n\nRegression Modeling Strategies\n\nHarrell\nUndergraduate\n\nVector Generalized Linear and Additive Models\n\nYee\nGraduate"
  },
  {
    "objectID": "books.html#nonparametric",
    "href": "books.html#nonparametric",
    "title": "Books",
    "section": "Nonparametric",
    "text": "Nonparametric\n\nSemiparametric Regression with R\n\nHarezlak, Ruppert, and Wand\nGraduate"
  },
  {
    "objectID": "books.html#computational",
    "href": "books.html#computational",
    "title": "Books",
    "section": "Computational",
    "text": "Computational\n\nBootstrap Methods with applications in R\n\nDikta and Scheer\nGraduate\n\nModern Optimization with R (2nd Edition)\n\nCortez\nGraduate\n\nComputational Statistics\n\nGentle\n\nMonte Carlo and Quasi-Monte Carlo Sampling\n\nLemieux\n\nStatistics With Julia\n\nNazarathy andKlok\n\nIntroducing Monte Carlo Methods in R\n\nRobert and Casella\n\nPermutation Statistical Methods with R\n\nBerry, Kvamme, Johnston, and Mielke\n\nMonte Carlo Strategies in Scientific Computing\n\nLiu"
  },
  {
    "objectID": "books.html#bayesian",
    "href": "books.html#bayesian",
    "title": "Books",
    "section": "Bayesian",
    "text": "Bayesian\n\nIntroduction to Bayesian Inference, Methods and Computation\n\nHeard\n\nApplied Bayesian Statistics\n\nCowles\n\nBayesian Statistical Modeling with Stan, R, and Python\n\nMatsuura\n\nBayesian Essentials in R\n\nMarin and Robert"
  },
  {
    "objectID": "books.html#theoretical",
    "href": "books.html#theoretical",
    "title": "Books",
    "section": "Theoretical",
    "text": "Theoretical\n\nEssentials of Stochastic Processes (3rd Edition)\n\nDurrett\nGraduate\n\nA Concise Introduction to Measure Theory\n\nShirali\nGraduate\n\nLarge Sample Techniques for Statistics (2nd Edition)\n\nJiang\nGraduate\n\nA Course in Mathematical Statistics and Large Sample Theory\n\nBhattacharya, Lin, and Patrangenaru\nGraduate\n\nMixture and Hidden Markov Models with R\n\nVisser and Speekenbrink\nUndergraduate\n\nModern Mathematical Statistics (3rd Edition)\n\nDevore, Berk, and Carlton\nUndergraduate\n\nProbability Theory (3rd Edition)\n\nKlenke\nGraduate\n\nTesting Statistical Hypotheses (4th Edition)\n\nLehmann and Romano\nGraduate\n\nTheory of Point Estimation\n\nLehmann and Casella\nGraduate\nMay not be available"
  },
  {
    "objectID": "books.html#longitudinal-data-analysis",
    "href": "books.html#longitudinal-data-analysis",
    "title": "Books",
    "section": "Longitudinal Data Analysis",
    "text": "Longitudinal Data Analysis\n\nLongitudinal Categorical Data Analysis\n\nSutradhar"
  },
  {
    "objectID": "books.html#survival-analysis",
    "href": "books.html#survival-analysis",
    "title": "Books",
    "section": "Survival Analysis",
    "text": "Survival Analysis\n\nStatistical Modelling of Survival Data with Random Effects\n\nHa, Jeong, and Lee\n\nSurvival Analysis (3rd Edition)\n\nKleinbaum and Klein\n\nApplied Survival Analysis in R\n\nMoore\n\nBayesian Survival Analysis\n\nIbrahim, Chen, and Sinha\n\nSurvival Analysis Techniques for Censored and Truncated Data (2nd Edition)\n\nKlein and Moeschberger"
  },
  {
    "objectID": "books.html#machine-learning",
    "href": "books.html#machine-learning",
    "title": "Books",
    "section": "Machine Learning",
    "text": "Machine Learning\n\nFundamental of High-Dimensional Statistics\n\nLederer\n\nAn Introduction to Statistical Learning (2nd Edition)\n\nJames, Witten, Hastie and Tibshirani\n\nStatistical Learning from a Regression Perspective (2nd Edition)\n\nBerk\n\nElements of Statistical Learning\n\nHastie, Friedman, and Tibshirani\n\nStatistics for High Dimensional Data\n\nBühlmann and van der Geer\n\nProbability and Statistics for Machine Learning\n\nDas Gupta"
  },
  {
    "objectID": "books.html#time-series",
    "href": "books.html#time-series",
    "title": "Books",
    "section": "Time-Series",
    "text": "Time-Series\n\nIntroduction to Time Series and Forcasting (3rd Edition)\n\nBrockwell and Davis\n\nTime Series Analysis and Its Applications\n\nShumway and Stoffer\n\nTime Series Analysis for the State-Space Model with R/Stan\n\nHagiwara"
  },
  {
    "objectID": "books.html#study-desing-and-causal-inference",
    "href": "books.html#study-desing-and-causal-inference",
    "title": "Books",
    "section": "Study Desing and Causal Inference",
    "text": "Study Desing and Causal Inference\n\nCausal Inference What IF\n\nHernán and Robins\n\nDesign of Observational Studies\n\nRosenbaum\n\n\nBolded Titles, I have read thoroughly."
  },
  {
    "objectID": "hw/hw3.html",
    "href": "hw/hw3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Due 7/12/24 @ 11:59 PM\nYou must comment all your code to receive credit.\nEmail me the *.html file to isaac.qs@csuci.edu.\nUse Monte Carlo Methods to evaluate the following integrals:"
  },
  {
    "objectID": "hw/hw3.html#problem-1",
    "href": "hw/hw3.html#problem-1",
    "title": "Homework 3",
    "section": "Problem 1",
    "text": "Problem 1\n\\[\n\\int^\\infty_0 e^{-\\frac{x^2 - 4x +4}{8}}dx\n\\]"
  },
  {
    "objectID": "hw/hw3.html#problem-2",
    "href": "hw/hw3.html#problem-2",
    "title": "Homework 3",
    "section": "Problem 2",
    "text": "Problem 2\n\\[\n\\int^3_1 x^3dx\n\\]"
  },
  {
    "objectID": "hw/hw3.html#problem-3",
    "href": "hw/hw3.html#problem-3",
    "title": "Homework 3",
    "section": "Problem 3",
    "text": "Problem 3\n\\[\n\\int^\\infty_{-\\infty}\\frac{\\ln(x)}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}dx\n\\]"
  },
  {
    "objectID": "hw/hw1.html",
    "href": "hw/hw1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Due 6/14/24 @ 11:59 PM\nYou must comment all your code to receive credit.\nSubmit the *.html file to canvas.\nFor all problems, you must use at least one for or while loop."
  },
  {
    "objectID": "hw/hw1.html#problem-1",
    "href": "hw/hw1.html#problem-1",
    "title": "Homework 1",
    "section": "Problem 1",
    "text": "Problem 1\nUsing the following code:\n\nx &lt;- matrix(rnorm(1500), nrow = 10)\n\nWrite the code to produce the output if you use the rowMeans() on the R object x."
  },
  {
    "objectID": "hw/hw1.html#problem-2",
    "href": "hw/hw1.html#problem-2",
    "title": "Homework 1",
    "section": "Problem 2",
    "text": "Problem 2\nUsing the following code:\n\ny &lt;- matrix(sample(1:400, 100, replace = T), nrow = 10)\n\nFind the median value for each column of the matrix y."
  },
  {
    "objectID": "hw/hw1.html#problem-3",
    "href": "hw/hw1.html#problem-3",
    "title": "Homework 1",
    "section": "Problem 3",
    "text": "Problem 3\nWrite the code that will generate the first 1000 numbers of the Fibonacci Sequence."
  },
  {
    "objectID": "hw/hw1.html#problem-4",
    "href": "hw/hw1.html#problem-4",
    "title": "Homework 1",
    "section": "Problem 4",
    "text": "Problem 4\nWrite a function for the following equation:\n\\[\nf(x, y, z) =\\left\\{\\begin{array}{cc}\nx^2+\\sqrt y & z  = 0 \\\\\nx^2+2x+3+ \\log(y) & z = 1\n\\end{array}\\right.\n\\]\nInclude any error messages if necessary."
  },
  {
    "objectID": "hw/hw1.html#problem-5",
    "href": "hw/hw1.html#problem-5",
    "title": "Homework 1",
    "section": "Problem 5",
    "text": "Problem 5\nCreate a function that will give you the first x Fibonacci numbers. You must use a for loop."
  },
  {
    "objectID": "hw/hw2.html",
    "href": "hw/hw2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Due 6/28/24 @ 11:59 PM\nYou must comment all your code to receive credit.\nEmail me the *.html file to isaac.qs@csuci.edu."
  },
  {
    "objectID": "hw/hw2.html#problem-1",
    "href": "hw/hw2.html#problem-1",
    "title": "Homework 2",
    "section": "Problem 1",
    "text": "Problem 1\nUse the Discrete Inverse Technique to simulate a \\(NB(0.2, 4)\\)."
  },
  {
    "objectID": "hw/hw2.html#problem-2",
    "href": "hw/hw2.html#problem-2",
    "title": "Homework 2",
    "section": "Problem 2",
    "text": "Problem 2\nUse the Accept-Reject algorithm to generate a \\(Beta(4,6)\\)."
  },
  {
    "objectID": "hw/hw2.html#problem-3",
    "href": "hw/hw2.html#problem-3",
    "title": "Homework 2",
    "section": "Problem 3",
    "text": "Problem 3\nUse the Accept-Reject algorithm to generate a \\(Weibull(2,3)\\)."
  },
  {
    "objectID": "hw/hw2.html#problem-4",
    "href": "hw/hw2.html#problem-4",
    "title": "Homework 2",
    "section": "Problem 4",
    "text": "Problem 4\nUse the Accept-Reject algorithm to generate a \\(Pareto(1,4)\\). The pdf of the Pareto distribution can be obtained from the EnvStats Package"
  },
  {
    "objectID": "hw/hw4.html",
    "href": "hw/hw4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Due 7/26/24 @ 11:59 PM\nYou must comment all your code to receive credit.\nEmail me the *.html file to isaac.qs@csuci.edu.\nUse Monte Carlo Methods to evaluate the following integrals:"
  },
  {
    "objectID": "hw.html",
    "href": "hw.html",
    "title": "Homework",
    "section": "",
    "text": "Quarto Template for HW\n\n\n\n\n\nDownload it here: Github Repo\nOR Type this in the RStudio terminal:\nquarto use template inqs909/m408_hw\nType Y for the trust author. Type a name of a new directory in where to save the file. For example, type hw1.\nOR Save this in an empty source quarto document:\n---\ntitle: \"Title\"\nauthor: \"Name Here\"\ndate: \"`r format(Sys.time(),'%m-%d-%Y')`\"\nformat: \n  html:\n    toc: true\n    toc-depth: 2\n    code-fold: true\n    code-tools: true\n    code-line-numbers: true\n    embed-resources: true\nknitr:\n  opts_chunk:\n    echo: true\n    message: false\n    warning: false\n    error: true\n    tidy: styler\n    R.options:\n      digits: 3\n      max.print: 100\n---\n\n## Problem 1\n\n## Problem 2\n\n## Problem 3\n\n\n\n\nBelow are the different homework assignments for the course. Make sure to upload your assignment as a single file on Canvas.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 3\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2.html",
    "href": "posts/2.html",
    "title": "Week 2",
    "section": "",
    "text": "Nested Loops\nWhile Loops\nVectorized Code\nFunctions\n*apply functions"
  },
  {
    "objectID": "posts/2.html#learning-outcomes",
    "href": "posts/2.html#learning-outcomes",
    "title": "Week 2",
    "section": "",
    "text": "Nested Loops\nWhile Loops\nVectorized Code\nFunctions\n*apply functions"
  },
  {
    "objectID": "posts/2.html#resources",
    "href": "posts/2.html#resources",
    "title": "Week 2",
    "section": "Resources",
    "text": "Resources\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo"
  },
  {
    "objectID": "posts/6.html",
    "href": "posts/6.html",
    "title": "Week 6",
    "section": "",
    "text": "Power Analysis\nPermutation Tests\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo 1 | Video 2"
  },
  {
    "objectID": "posts/6.html#learning-outcomes",
    "href": "posts/6.html#learning-outcomes",
    "title": "Week 6",
    "section": "",
    "text": "Power Analysis\nPermutation Tests\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo 1 | Video 2"
  },
  {
    "objectID": "posts/6.html#further-reading",
    "href": "posts/6.html#further-reading",
    "title": "Week 6",
    "section": "Further Reading",
    "text": "Further Reading\n\nAvailable from Broome Library\nComputational Statistics: Chapter 12\nPermutation Statistical Methods with R\n\n\nGreat Books (Don’t Buy)\nStatistical Computing in R: Chapter 10\n\n\nWebsites"
  },
  {
    "objectID": "posts/5.html",
    "href": "posts/5.html",
    "title": "Week 5",
    "section": "",
    "text": "Hypothesis Testing\nMonte Carlo P-Value\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo 1 | Video 2"
  },
  {
    "objectID": "posts/5.html#learning-outcomes",
    "href": "posts/5.html#learning-outcomes",
    "title": "Week 5",
    "section": "",
    "text": "Hypothesis Testing\nMonte Carlo P-Value\n\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nVideo 1 | Video 2"
  },
  {
    "objectID": "posts/5.html#further-reading",
    "href": "posts/5.html#further-reading",
    "title": "Week 5",
    "section": "Further Reading",
    "text": "Further Reading\n\nAvailable from Broome Library\nComputational Statistics: Chapter 11\n\n\nGreat Books (Don’t Buy)\nStatistical Computing in R: Chapter 7\n\n\nWebsites\nData Science and Statistical Computing"
  },
  {
    "objectID": "posts/week_1.html",
    "href": "posts/week_1.html",
    "title": "Week 1",
    "section": "",
    "text": "Installing R and RStudio\nScripts\nR Calculator\nR Objects\nR Packages"
  },
  {
    "objectID": "posts/week_1.html#learning-outcomes",
    "href": "posts/week_1.html#learning-outcomes",
    "title": "Week 1",
    "section": "",
    "text": "Installing R and RStudio\nScripts\nR Calculator\nR Objects\nR Packages"
  },
  {
    "objectID": "posts/week_1.html#resources",
    "href": "posts/week_1.html#resources",
    "title": "Week 1",
    "section": "Resources",
    "text": "Resources\n\n\n\nLecture\nVideos\n\n\n\n\nSlide\nNot Available"
  },
  {
    "objectID": "lectures/3.html#random-process",
    "href": "lectures/3.html#random-process",
    "title": "Monte Carlo Methods",
    "section": "Random Process",
    "text": "Random Process\nA random process is act of observing an outcome of an event that is unpredictable.\n\nExamples:\n\nFlipping a coin\nRolling a die"
  },
  {
    "objectID": "lectures/3.html#random-variable",
    "href": "lectures/3.html#random-variable",
    "title": "Monte Carlo Methods",
    "section": "Random Variable",
    "text": "Random Variable\nA random variable connects the outcomes observed from a random process to a probability space."
  },
  {
    "objectID": "lectures/3.html#flipping-a-coin",
    "href": "lectures/3.html#flipping-a-coin",
    "title": "Monte Carlo Methods",
    "section": "Flipping a Coin",
    "text": "Flipping a Coin\n\n\n\nOutcome\nHead\nTails\n\n\nProbability\n0.5\n0.5\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(patchwork)\nx &lt;- sample(c(\"H\", \"T\"), 5000, replace = T)\nx |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nylab(\"Probability\") +\ntheme_bw()"
  },
  {
    "objectID": "lectures/3.html#rolling-a-die",
    "href": "lectures/3.html#rolling-a-die",
    "title": "Monte Carlo Methods",
    "section": "Rolling a Die",
    "text": "Rolling a Die\n\n\n\nOutcome\n1\n2\n3\n4\n5\n6\n\n\nProbability\n1/6\n1/6\n1/6\n1/6\n1/6\n1/6\n\n\n\n\n\nCode\n# library(tidyverse)\nx &lt;- sample(1:6, 50000, replace = T)\nx |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nylab(\"Probability\") +\ntheme_bw()"
  },
  {
    "objectID": "lectures/3.html#discrete-random-variables",
    "href": "lectures/3.html#discrete-random-variables",
    "title": "Monte Carlo Methods",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\nA random variable is considered to be discrete if the outcome are only whole numbers (integers)."
  },
  {
    "objectID": "lectures/3.html#pmf",
    "href": "lectures/3.html#pmf",
    "title": "Monte Carlo Methods",
    "section": "PMF",
    "text": "PMF\nThe probability mass function of discrete variable can be represented by a formula, table, or a graph. The Probability of a random variable Y can be expressed as \\(P(Y=y)\\) for all values of \\(y\\)."
  },
  {
    "objectID": "lectures/3.html#rolling-a-die-1",
    "href": "lectures/3.html#rolling-a-die-1",
    "title": "Monte Carlo Methods",
    "section": "Rolling a Die",
    "text": "Rolling a Die\n\n\nCode\nx &lt;- sample(1:6, 50000, replace = T)\nx |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nylab(\"Probability\") +\ntheme_bw()"
  },
  {
    "objectID": "lectures/3.html#cdf",
    "href": "lectures/3.html#cdf",
    "title": "Monte Carlo Methods",
    "section": "CDF",
    "text": "CDF\nThe cumulative distribution function provides the \\(P(Y\\leq y)\\) for a random variable \\(Y\\)."
  },
  {
    "objectID": "lectures/3.html#expected-value",
    "href": "lectures/3.html#expected-value",
    "title": "Monte Carlo Methods",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value is the value we expect when we randomly sample from population that follows a specific distribution. The expected value of Y is\n\\[\nE(Y)=\\sum_y yP(y)\n\\]"
  },
  {
    "objectID": "lectures/3.html#variance",
    "href": "lectures/3.html#variance",
    "title": "Monte Carlo Methods",
    "section": "Variance",
    "text": "Variance\nThe variance is the expected squared difference between the random variable and expected value.\n\\[\nVar(Y)=\\sum_y\\{y-E(Y)\\}^2P(y)\n\\]\n\\[\nVar(Y) = E(X^2) - E(X)^2\n\\]"
  },
  {
    "objectID": "lectures/3.html#known-distributions",
    "href": "lectures/3.html#known-distributions",
    "title": "Monte Carlo Methods",
    "section": "Known Distributions",
    "text": "Known Distributions\n\n\n\n\n\n\n\n\nDistribution\nParameter(s)\nPMF \\(P(Y=y)\\)\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p\\)\n\n\nBinomial\n\\(n\\) and \\(p\\)\n\\((^n_y)p^y(1-p)^{n-p}\\)\n\n\nGeometric\n\\(p\\)\n\\((1-p)^{y-1}p\\)\n\n\nNegative Binomial\n\\(r\\) and \\(p\\)\n\\((^{y-1}_{r-1})p^{r-1}(1-p)^{y-r}\\)\n\n\nHypergeometric\n\\(N\\), \\(n\\), and \\(r\\)\n\\(\\frac{(^r_y)(^{N-r}_{n-y})}{(^N_n)}\\)\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\frac{\\lambda^y}{y!} e^{-\\lambda}\\)"
  },
  {
    "objectID": "lectures/3.html#binomial-distribution",
    "href": "lectures/3.html#binomial-distribution",
    "title": "Monte Carlo Methods",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nAn experiment is said to follow a binomial distribution if\n\nFixed \\(n\\)\nEach trial has 2 outcomes\nThe probability of success is a constant \\(p\\)\nThe trials are independent of each\n\n\n\\(P(X=x)=(^n_x)p^x(1-p)^{n-x}\\)\n\n\n\\(X\\) can be any value between 0 to n\n\n\n\\(X \\sim Bin(n,p)\\)"
  },
  {
    "objectID": "lectures/3.html#bernoulli-distribution-n-1-p-0.1-biased-coin-flip",
    "href": "lectures/3.html#bernoulli-distribution-n-1-p-0.1-biased-coin-flip",
    "title": "Monte Carlo Methods",
    "section": "Bernoulli Distribution (n = 1, p = 0.1; Biased Coin Flip)",
    "text": "Bernoulli Distribution (n = 1, p = 0.1; Biased Coin Flip)\n\n\nCode\np &lt;- 0.1\nx &lt;- rbinom(50000, 1, p)\np1 &lt;- x |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nylab(\"Probability\") +\nggtitle(\"PMF\") +\ntheme_bw()\np2 &lt;- 0:1 |&gt; pbinom(1, p) |&gt; tibble(x = 0:1, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_step() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-n-30-p-0.1",
    "href": "lectures/3.html#distribution-n-30-p-0.1",
    "title": "Monte Carlo Methods",
    "section": "Distribution (n = 30, p = 0.1)",
    "text": "Distribution (n = 30, p = 0.1)\n\n\nCode\np &lt;- 0.1\nx &lt;- rbinom(50000, 30, p)\np1 &lt;- x |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nxlim(c(0,30)) +\nylab(\"Probability\") +\nggtitle(\"PMF\") +\ntheme_bw()\np2 &lt;- 0:30 |&gt; pbinom(30, p) |&gt; tibble(x = 0:30, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_step() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-n-30-p-0.5",
    "href": "lectures/3.html#distribution-n-30-p-0.5",
    "title": "Monte Carlo Methods",
    "section": "Distribution (n = 30, p = 0.5)",
    "text": "Distribution (n = 30, p = 0.5)\n\n\nCode\np &lt;- 0.5\nx &lt;- rbinom(50000, 30, p)\np1 &lt;- x |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nxlim(c(0,30)) +\nylab(\"Probability\") +\nggtitle(\"PMF\") +\ntheme_bw()\np2 &lt;- 0:30 |&gt; pbinom(30, p) |&gt; tibble(x = 0:30, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_step() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-n-30-p-0.85",
    "href": "lectures/3.html#distribution-n-30-p-0.85",
    "title": "Monte Carlo Methods",
    "section": "Distribution (n = 30, p = 0.85)",
    "text": "Distribution (n = 30, p = 0.85)\n\n\nCode\np &lt;- 0.85\nx &lt;- rbinom(50000, 30, p)\np1 &lt;- x |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nxlim(c(0,30)) +\nylab(\"Probability\") +\nggtitle(\"PMF\") +\ntheme_bw()\np2 &lt;- 0:30 |&gt; pbinom(30, p) |&gt; tibble(x = 0:30, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_step() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#expectations",
    "href": "lectures/3.html#expectations",
    "title": "Monte Carlo Methods",
    "section": "Expectations",
    "text": "Expectations\n\\[\nE(X) =  np\n\\]\n\\[\nVar(X) = np(1-p)\n\\]"
  },
  {
    "objectID": "lectures/3.html#poisson-distribution",
    "href": "lectures/3.html#poisson-distribution",
    "title": "Monte Carlo Methods",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nThe poisson distribution describes an experiment that measures that occurrence of an event at specific point and/or time period.\n\n\\(P(X=x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}\\)\n\n\n\\(X\\) can take any value from 0 to \\(\\infty\\)\n\n\n\\(X \\sim Pois(\\lambda)\\)"
  },
  {
    "objectID": "lectures/3.html#distribution-lambda-3.5",
    "href": "lectures/3.html#distribution-lambda-3.5",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\lambda\\) = 3.5)",
    "text": "Distribution (\\(\\lambda\\) = 3.5)\n\n\nCode\np &lt;- 3.5\nx &lt;- rpois(50000, p)\np1 &lt;- x |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nylab(\"Probability\") +\nggtitle(\"PMF\") +\ntheme_bw()\np2 &lt;- 0:max(x) |&gt; ppois(p) |&gt; tibble(x = 0:max(x), y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_step() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-lambda-34.5",
    "href": "lectures/3.html#distribution-lambda-34.5",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\lambda\\) = 34.5)",
    "text": "Distribution (\\(\\lambda\\) = 34.5)\n\n\nCode\np &lt;- 34.5\nx &lt;- rpois(50000, p)\np1 &lt;- x |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) + \nylab(\"Probability\") +\nggtitle(\"PMF\") +\ntheme_bw()\np2 &lt;- 0:max(x) |&gt; ppois(p) |&gt; tibble(x = 0:max(x), y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_step() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#expectations-1",
    "href": "lectures/3.html#expectations-1",
    "title": "Monte Carlo Methods",
    "section": "Expectations",
    "text": "Expectations\n\\[\nE(X) = \\lambda\n\\]\n\\[\nVar(X) = \\lambda\n\\]"
  },
  {
    "objectID": "lectures/3.html#negative-binomial",
    "href": "lectures/3.html#negative-binomial",
    "title": "Monte Carlo Methods",
    "section": "Negative Binomial",
    "text": "Negative Binomial\nThe negative binomial distribution is a discrete probability distribution that models the number of failures required to achieve a specified number of successes in a sequence of independent and identically distributed Bernoulli trials.\n\\[\nP(X = k) = \\binom{k + r - 1}{r - 1} p^r (1 - p)^k\n\\]\n\n\\(X\\) can take the values from 0 to \\(\\infty\\)\n\n\n\\(X\\sim NB(p, r)\\)"
  },
  {
    "objectID": "lectures/3.html#expectations-2",
    "href": "lectures/3.html#expectations-2",
    "title": "Monte Carlo Methods",
    "section": "Expectations",
    "text": "Expectations\n\\[\nE(X) = \\frac{r (1 - p)}{p}\n\\]\n\\[\n\\text{Var}(X) = \\frac{r (1 - p)}{p^2}\n\\]"
  },
  {
    "objectID": "lectures/3.html#distribution-r-11-p-0.1",
    "href": "lectures/3.html#distribution-r-11-p-0.1",
    "title": "Monte Carlo Methods",
    "section": "Distribution (r = 11, p = 0.1)",
    "text": "Distribution (r = 11, p = 0.1)\n\n\nCode\np &lt;- 0.1\nx &lt;- rnbinom(50000,11, p)\np1 &lt;- x |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) +\nylab(\"Probability\") +\nggtitle(\"PMF\") +\ntheme_bw()\np2 &lt;- 0:max(x) |&gt; pnbinom(11, p) |&gt; tibble(x = 0:max(x), y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_step() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-r-11-p-0.45",
    "href": "lectures/3.html#distribution-r-11-p-0.45",
    "title": "Monte Carlo Methods",
    "section": "Distribution (r = 11, p = 0.45)",
    "text": "Distribution (r = 11, p = 0.45)\n\n\nCode\np &lt;- 0.45\nx &lt;- rnbinom(50000, 11, p)\np1 &lt;- x |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) +\nylab(\"Probability\") +\nggtitle(\"PMF\") +\ntheme_bw()\np2 &lt;- 0:max(x) |&gt; pnbinom(11, p) |&gt; tibble(x = 0:max(x), y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_step() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-r-11-p-0.63",
    "href": "lectures/3.html#distribution-r-11-p-0.63",
    "title": "Monte Carlo Methods",
    "section": "Distribution (r = 11, p = 0.63)",
    "text": "Distribution (r = 11, p = 0.63)\n\n\nCode\np &lt;- 0.63\nx &lt;- rnbinom(50000, 11, p)\np1 &lt;- x |&gt; tibble() |&gt; \nggplot(aes(x)) +\ngeom_bar(aes(y = (..count..)/sum(..count..))) +\nylab(\"Probability\") +\nggtitle(\"PMF\") +\ntheme_bw()\np2 &lt;- 0:max(x) |&gt; pnbinom(11, p) |&gt; tibble(x = 0:max(x), y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_step() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#continuous-random-variables",
    "href": "lectures/3.html#continuous-random-variables",
    "title": "Monte Carlo Methods",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nA random variable \\(X\\) is considered continuous if the \\(P(X=x)\\) does not exist."
  },
  {
    "objectID": "lectures/3.html#cdf-1",
    "href": "lectures/3.html#cdf-1",
    "title": "Monte Carlo Methods",
    "section": "CDF",
    "text": "CDF\nThe cumulative distribution function of \\(X\\) provides the \\(P(X\\leq x)\\), denoted by \\(F(x)\\), for the domain of \\(X\\).\nProperties of the CDF of \\(X\\):\n\n\\(F(-\\infty)\\equiv \\lim_{y\\rightarrow -\\infty}F(y)=0\\)\n\\(F(\\infty)\\equiv \\lim_{y\\rightarrow \\infty}F(y)=1\\)\n\\(F(x)\\) is a nondecreaseing function"
  },
  {
    "objectID": "lectures/3.html#pdf",
    "href": "lectures/3.html#pdf",
    "title": "Monte Carlo Methods",
    "section": "PDF",
    "text": "PDF\nThe probability density function of the random variable \\(X\\) is given by\n\\[\nf(x)=\\frac{dF(x)}{d(x)}=F^\\prime(x)\n\\]\nwherever the derivative exists.\nProperties of pdfs:\n\n\\(f(x)\\geq 0\\)\n\\(\\int^\\infty_{-\\infty}f(x)dx=1\\)\n\\(P(a\\leq X\\leq b) = P(a&lt;X&lt;b)=\\int^b_af(x)dx\\)"
  },
  {
    "objectID": "lectures/3.html#expected-value-1",
    "href": "lectures/3.html#expected-value-1",
    "title": "Monte Carlo Methods",
    "section": "Expected Value",
    "text": "Expected Value\nThe expected value for a continuous distribution is defined as\n\\[\nE(X)=\\int x f(x)dx\n\\]\nThe expectation of a function \\(g(X)\\) is defined as\n\\[\nE\\{g(X)\\}=\\int g(x)f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/3.html#variance-1",
    "href": "lectures/3.html#variance-1",
    "title": "Monte Carlo Methods",
    "section": "Variance",
    "text": "Variance\nThe variance of continuous variable is defined as\n\\[\nVar(X) =  E[\\{X-E(X)\\}^2] = \\int \\{X-E(X)\\}^2 f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/3.html#uniform-distribution",
    "href": "lectures/3.html#uniform-distribution",
    "title": "Monte Carlo Methods",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nA random variable is said to follow uniform distribution if the density function is constant between two parameters.\n\n\\[\nf(x) = \\left\\{\\begin{array}{cc}\n\\frac{1}{b-a} & a \\leq x \\leq b\\\\\n0 & \\mathrm{elsewhere}\n\\end{array}\\right.\n\\]\n\n\n\\(X\\) can take any value between \\(a\\) and \\(b\\)\n\n\n\\(X \\sim U(a,b)\\)"
  },
  {
    "objectID": "lectures/3.html#distribution-a-4-b-25",
    "href": "lectures/3.html#distribution-a-4-b-25",
    "title": "Monte Carlo Methods",
    "section": "Distribution (a = 4, b = 25)",
    "text": "Distribution (a = 4, b = 25)\n\n\nCode\na &lt;- 4\nb &lt;- 25\nx &lt;- seq(a, b, length.out = 1000)\np1 &lt;- dunif(x, a, b) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- punif(x, a, b) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-a-0-b-1",
    "href": "lectures/3.html#distribution-a-0-b-1",
    "title": "Monte Carlo Methods",
    "section": "Distribution (a = 0, b = 1)",
    "text": "Distribution (a = 0, b = 1)\n\n\nCode\na &lt;- 0\nb &lt;- 1\nx &lt;- seq(a, b, length.out = 1000)\np1 &lt;- dunif(x, a, b) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- punif(x, a, b) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#expectations-3",
    "href": "lectures/3.html#expectations-3",
    "title": "Monte Carlo Methods",
    "section": "Expectations",
    "text": "Expectations\n\\[\nE(X) = \\frac{a+b}{2}\n\\]\n\\[\nVar(X) = \\frac{1}{12}(b-a)^2\n\\]"
  },
  {
    "objectID": "lectures/3.html#normal-distribution",
    "href": "lectures/3.html#normal-distribution",
    "title": "Monte Carlo Methods",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nA random variable is said to follow a normal distribution if the the frequency of occurrence follow a Gaussian function.\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}\n\\]\n\n\\(X\\) can take any value between \\(-\\infty\\) and \\(\\infty\\)\n\n\n\\(X\\sim N(\\mu, \\sigma^2)\\)"
  },
  {
    "objectID": "lectures/3.html#distribution-mu-34-sigma2-5",
    "href": "lectures/3.html#distribution-mu-34-sigma2-5",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\mu\\) = 34, \\(\\sigma^2\\) = 5)",
    "text": "Distribution (\\(\\mu\\) = 34, \\(\\sigma^2\\) = 5)\n\n\nCode\na &lt;- 25\nb &lt;- 45\nx &lt;- seq(a, b, length.out = 1000)\np1 &lt;- dnorm(x, 34, sqrt(5)) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- pnorm(x, 34, sqrt(5)) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-mu--8-sigma2-10",
    "href": "lectures/3.html#distribution-mu--8-sigma2-10",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\mu\\) = -8, \\(\\sigma^2\\) = 10)",
    "text": "Distribution (\\(\\mu\\) = -8, \\(\\sigma^2\\) = 10)\n\n\nCode\na &lt;- -20\nb &lt;- 4\nx &lt;- seq(a, b, length.out = 1000)\np1 &lt;- dnorm(x, -8, sqrt(10)) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- pnorm(x, -8, sqrt(10)) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#expectations-4",
    "href": "lectures/3.html#expectations-4",
    "title": "Monte Carlo Methods",
    "section": "Expectations",
    "text": "Expectations\n\\[\nE(X) = \\mu\n\\]\n\\[\nVar(X) = \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/3.html#gamma-distribution",
    "href": "lectures/3.html#gamma-distribution",
    "title": "Monte Carlo Methods",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\nA gamma random variable is characterized by the gamma distribution, used to model waiting times or the time until an event occurs a certain number of times.\n\\[\nf(x; \\alpha, \\beta) = \\frac{x^{\\alpha - 1} e^{-x/\\beta}}{\\beta^\\alpha \\Gamma(\\alpha)}\n\\]\n\\[\n\\Gamma(\\alpha) = \\int_0^\\infty t^{\\alpha - 1} e^{-t} \\, dt\n\\]\n\n\\(X\\) can take any value between 0 and \\(\\infty\\)\n\n\n\\(X\\sim Gamma(\\alpha,\\beta)\\)"
  },
  {
    "objectID": "lectures/3.html#expectations-5",
    "href": "lectures/3.html#expectations-5",
    "title": "Monte Carlo Methods",
    "section": "Expectations",
    "text": "Expectations\n\\[\nE(X) = \\alpha \\beta\n\\]\n\\[\n\\text{Var}(X) = \\alpha \\beta^2\n\\]"
  },
  {
    "objectID": "lectures/3.html#distribution-alpha-1.5-beta-2.6",
    "href": "lectures/3.html#distribution-alpha-1.5-beta-2.6",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\alpha\\) = 1.5, \\(\\beta\\) = 2.6)",
    "text": "Distribution (\\(\\alpha\\) = 1.5, \\(\\beta\\) = 2.6)\n\n\nCode\ny &lt;- rgamma(1000, 1.5, 2.6)\na &lt;- 0\nb &lt;- 10\nx &lt;- seq(a, b, length.out = 1000)\np1 &lt;- dgamma(x, 1.5, 2.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- pgamma(x, 1.5, 2.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-alpha-3.5-beta-1.6",
    "href": "lectures/3.html#distribution-alpha-3.5-beta-1.6",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\alpha\\) = 3.5, \\(\\beta\\) = 1.6)",
    "text": "Distribution (\\(\\alpha\\) = 3.5, \\(\\beta\\) = 1.6)\n\n\nCode\ny &lt;- rgamma(1000, 3.5, 1.6)\na &lt;- 0\nb &lt;- 10\nx &lt;- seq(a, b, length.out = 1000)\np1 &lt;- dgamma(x, 3.5, 1.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- pgamma(x, 3.5, 1.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-alpha-5.2-beta-1.6",
    "href": "lectures/3.html#distribution-alpha-5.2-beta-1.6",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\alpha\\) = 5.2, \\(\\beta\\) = 1.6)",
    "text": "Distribution (\\(\\alpha\\) = 5.2, \\(\\beta\\) = 1.6)\n\n\nCode\ny &lt;- rgamma(1000, 5.2, 1.6)\na &lt;- 0\nb &lt;- 10\nx &lt;- seq(a, b, length.out = 1000)\np1 &lt;- dgamma(x, 5.2, 1.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- pgamma(x, 5.2, 1.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#beta-distribution",
    "href": "lectures/3.html#beta-distribution",
    "title": "Monte Carlo Methods",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nThe beta distribution is often used to model random variables that represent proportions or probabilities.\n\\[\nf(x; \\alpha, \\beta) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}\n\\]\n\\[\nB(\\alpha, \\beta) = \\int_0^1 t^{\\alpha - 1} (1 - t)^{\\beta - 1} \\, dt\n\\]\n\n\\(X\\) can take a value between 0 and 1\n\n\n\\(X\\sim Beta(\\alpha,\\beta)\\)"
  },
  {
    "objectID": "lectures/3.html#expectations-6",
    "href": "lectures/3.html#expectations-6",
    "title": "Monte Carlo Methods",
    "section": "Expectations",
    "text": "Expectations\n\\[\nE(X) = \\frac{\\alpha}{\\alpha + \\beta}\n\\]\n\\[\n\\text{Var}(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\n\\]"
  },
  {
    "objectID": "lectures/3.html#distribution-alpha-5.2-beta-1.6-1",
    "href": "lectures/3.html#distribution-alpha-5.2-beta-1.6-1",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\alpha\\) = 5.2, \\(\\beta\\) = 1.6)",
    "text": "Distribution (\\(\\alpha\\) = 5.2, \\(\\beta\\) = 1.6)\n\n\nCode\ny &lt;- rbeta(1000, 5.2, 1.6)\na &lt;- 0\nb &lt;- 1\nx &lt;- seq(a, b, length.out = 1000)\np1 &lt;- dbeta(x, 5.2, 1.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- pbeta(x, 5.2, 1.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-alpha-1.3-beta-1.6",
    "href": "lectures/3.html#distribution-alpha-1.3-beta-1.6",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\alpha\\) = 1.3, \\(\\beta\\) = 1.6)",
    "text": "Distribution (\\(\\alpha\\) = 1.3, \\(\\beta\\) = 1.6)\n\n\nCode\ny &lt;- rbeta(1000, 1.3, 1.6)\na &lt;- 0\nb &lt;- 1\nx &lt;- seq(a, b, length.out = 1000)\np1 &lt;- dbeta(x, 1.3, 1.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- pbeta(x, 1.3, 1.6) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distribution-alpha-.72-beta-.76",
    "href": "lectures/3.html#distribution-alpha-.72-beta-.76",
    "title": "Monte Carlo Methods",
    "section": "Distribution (\\(\\alpha\\) = .72, \\(\\beta\\) = .76)",
    "text": "Distribution (\\(\\alpha\\) = .72, \\(\\beta\\) = .76)\n\n\nCode\na &lt;- .72\nb &lt;- .76\nx &lt;- seq(0, 1, length.out = 1000)\np1 &lt;- dbeta(x, a, b) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x, y)) +\ngeom_line() +\nylab(\"Density\") +\nggtitle(\"PDF\") +\ntheme_bw()\np2 &lt;- pbeta(x, a, b) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))\np1 + p2"
  },
  {
    "objectID": "lectures/3.html#distributions-in-r",
    "href": "lectures/3.html#distributions-in-r",
    "title": "Monte Carlo Methods",
    "section": "Distributions in R",
    "text": "Distributions in R\nSeveral common distributions can be utilized in R with the 4 common functions:\n\n\n\n\n\n\n\nLetter\nFunctionality\n\n\n\n\nd\nreturns the height of the probability density/mass function\n\n\np\nreturns the cumulative density function value\n\n\nq\nreturns the inverse cumulative density function (percentiles)\n\n\nr\nreturns a randomly generated number"
  },
  {
    "objectID": "lectures/3.html#generating-random-numbers",
    "href": "lectures/3.html#generating-random-numbers",
    "title": "Monte Carlo Methods",
    "section": "Generating Random Numbers",
    "text": "Generating Random Numbers\nA number is an outcome from a random experiment.\n\nRandom experiment is an experiment where the outcome is not predicted.\n\n\nThe outcomes have a probability of being observed, whether equal or not."
  },
  {
    "objectID": "lectures/3.html#generating-random-numbers-1",
    "href": "lectures/3.html#generating-random-numbers-1",
    "title": "Monte Carlo Methods",
    "section": "Generating Random Numbers",
    "text": "Generating Random Numbers"
  },
  {
    "objectID": "lectures/3.html#generating-random-numbers-2",
    "href": "lectures/3.html#generating-random-numbers-2",
    "title": "Monte Carlo Methods",
    "section": "Generating Random Numbers",
    "text": "Generating Random Numbers"
  },
  {
    "objectID": "lectures/3.html#generating-random-numbers-3",
    "href": "lectures/3.html#generating-random-numbers-3",
    "title": "Monte Carlo Methods",
    "section": "Generating Random Numbers",
    "text": "Generating Random Numbers"
  },
  {
    "objectID": "lectures/3.html#generating-random-numbers-4",
    "href": "lectures/3.html#generating-random-numbers-4",
    "title": "Monte Carlo Methods",
    "section": "Generating Random Numbers",
    "text": "Generating Random Numbers"
  },
  {
    "objectID": "lectures/3.html#psuedo-random-numbers",
    "href": "lectures/3.html#psuedo-random-numbers",
    "title": "Monte Carlo Methods",
    "section": "Psuedo Random Numbers",
    "text": "Psuedo Random Numbers\nThese methods are considered time-consuming when a large number values are necessary.\n\nWith the advent of computers, random number can be generated with the use deterministic algorithms, where a mechanism is used to make it random, such as time.\n\n\nComputer-generated random numbers are considered psuedo random numbers because an algorithm is used to generate them given an initial single value, known as a seed.\n\n\nSupplying a seed to a random number generator will ensure that the same numbers are produced every time."
  },
  {
    "objectID": "lectures/3.html#mersenne-twister",
    "href": "lectures/3.html#mersenne-twister",
    "title": "Monte Carlo Methods",
    "section": "Mersenne Twister",
    "text": "Mersenne Twister\nThe Mersenne Twister is a widely used pseudorandom number generator (PRNG) known for its high quality and efficiency. It was developed by Makoto Matsumoto and Takuji Nishimura in 1997.\nThe default random number generator in R."
  },
  {
    "objectID": "lectures/3.html#uniform-distribution-r",
    "href": "lectures/3.html#uniform-distribution-r",
    "title": "Monte Carlo Methods",
    "section": "Uniform Distribution R",
    "text": "Uniform Distribution R\n\nDescriptionCode\n\n\nThe runif function in R will generate a value the come from a uniform distribution.\nrunif arguments:\n\nn: number of values to generate\nmin: the smallest possible value to generate\nmax: the largest possible value to generate\n\n\n\n\n\nCode\nrunif(1, 0, 1)\n\n\n#&gt; [1] 0.4641163"
  },
  {
    "objectID": "lectures/3.html#random-variable-generation",
    "href": "lectures/3.html#random-variable-generation",
    "title": "Monte Carlo Methods",
    "section": "Random Variable Generation",
    "text": "Random Variable Generation\nSeveral distribution, common and uncommon, can be generated using a uniform random variables.\n\nMore complex distributions may require the use of common distributions."
  },
  {
    "objectID": "lectures/3.html#inverse-transform-method",
    "href": "lectures/3.html#inverse-transform-method",
    "title": "Monte Carlo Methods",
    "section": "Inverse-Transform Method",
    "text": "Inverse-Transform Method\n\n\nCode\na &lt;- -20\nb &lt;- 4\nx &lt;- seq(a, b, length.out = 1000)\npnorm(x, -8, sqrt(10)) |&gt; tibble(x = x, y = _) |&gt; \nggplot(aes(x,y)) +\ngeom_line() +\ntheme_bw() +\nggtitle(\"CDF\") +\nylab(paste0(\"P(X\",\"\\u2264\",\" x)\"))"
  },
  {
    "objectID": "lectures/3.html#inverse-transformation-algorithm",
    "href": "lectures/3.html#inverse-transformation-algorithm",
    "title": "Monte Carlo Methods",
    "section": "Inverse-Transformation Algorithm",
    "text": "Inverse-Transformation Algorithm\n\nGenerate a random value \\(U\\) that follows a \\(U(0,1)\\)\nUsing the CDF (\\(F(X)\\)) for random variable \\(X\\), compute:\n\n\\[\nX = F^{-1}(U)\n\\]"
  },
  {
    "objectID": "lectures/3.html#exponential-distribution",
    "href": "lectures/3.html#exponential-distribution",
    "title": "Monte Carlo Methods",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nAn exponential random variable is characterized by the exponential distribution, used to model waiting times or the time until an event occurs a certain number of times.\nThe exponential distribution is a gamma random variable with \\(\\alpha = 1\\)."
  },
  {
    "objectID": "lectures/3.html#exponential-distribution-1",
    "href": "lectures/3.html#exponential-distribution-1",
    "title": "Monte Carlo Methods",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\n\\[\nf(x) = \\frac{1}{\\lambda} \\exp\\left\\{-\\frac{x}{\\lambda}\\right\\}\n\\]\n\\[\nF(x) = 1-\\exp\\left\\{-\\frac{x}{\\lambda}\\right\\}\n\\]\n\\[\nF^{-1}(x) = -\\lambda \\log(1-x)\n\\]"
  },
  {
    "objectID": "lectures/3.html#simulating-an-exponential-rv",
    "href": "lectures/3.html#simulating-an-exponential-rv",
    "title": "Monte Carlo Methods",
    "section": "Simulating an Exponential RV",
    "text": "Simulating an Exponential RV\n\\[\nX \\sim Exp(2)\n\\]\n\n\nCode\nxe &lt;- seq(0, 4, length.out = 1000)\nu &lt;- runif(100000)\nu |&gt; tibble(x = _) |&gt; \nggplot(aes(x=u, y = ..density..)) +\ngeom_histogram() +\ngeom_line(data = tibble(x = xe, y = dexp(xe, rate = 1/2)),\n          mapping = aes(x,y)) +\ntheme_bw()"
  },
  {
    "objectID": "lectures/3.html#simulating-an-exponential-rv-1",
    "href": "lectures/3.html#simulating-an-exponential-rv-1",
    "title": "Monte Carlo Methods",
    "section": "Simulating an Exponential RV",
    "text": "Simulating an Exponential RV\n\n\nCode\nu &lt;- runif(100000)\nx &lt;- -2 * log(1-u)"
  },
  {
    "objectID": "lectures/3.html#simulating-an-exponential-rv-2",
    "href": "lectures/3.html#simulating-an-exponential-rv-2",
    "title": "Monte Carlo Methods",
    "section": "Simulating an Exponential RV",
    "text": "Simulating an Exponential RV\n\n\nCode\nx |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) +\ngeom_histogram() +\ngeom_line(data = tibble(x = xe, y = dexp(xe, rate = 1/2)),\n          mapping = aes(x,y)) +\ntheme_bw()"
  },
  {
    "objectID": "lectures/3.html#exponential-rv-in-r",
    "href": "lectures/3.html#exponential-rv-in-r",
    "title": "Monte Carlo Methods",
    "section": "Exponential RV in R",
    "text": "Exponential RV in R\n\nDescriptionCode\n\n\nThe exponential distribution can be simulated in R using the rexp function with the following arguments:\n\nn: number of values to generate\nrate: how fast would events occur\n\n\n\n\n\nCode\nrexp(1, rate = 1)\n\n\n#&gt; [1] 0.2819719"
  },
  {
    "objectID": "lectures/3.html#discrete-rv-inverse-transformations",
    "href": "lectures/3.html#discrete-rv-inverse-transformations",
    "title": "Monte Carlo Methods",
    "section": "Discrete RV Inverse-Transformations",
    "text": "Discrete RV Inverse-Transformations\n\nGenerate a random value \\(U\\) that follows a \\(U(0,1)\\)\nUsing the CDF (\\(F(X)\\)), find the smallest integer value \\(k\\) such that:\n\n\\[\nU \\leq F(k)\n\\] 3. \\(X \\leftarrow k\\)"
  },
  {
    "objectID": "lectures/3.html#poisson-distribution-1",
    "href": "lectures/3.html#poisson-distribution-1",
    "title": "Monte Carlo Methods",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n\nCode\nxe &lt;- 0:20\nu &lt;- runif(100000)\nu |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) +\ngeom_histogram(bins = 20) +\ngeom_step(data = tibble(x = xe, y = dpois(xe, lambda = 6)),\n          mapping = aes(x,y)) +\ntheme_bw()"
  },
  {
    "objectID": "lectures/3.html#poisson-distribution-2",
    "href": "lectures/3.html#poisson-distribution-2",
    "title": "Monte Carlo Methods",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n\nCode\nfinder &lt;- function(u){\n  x &lt;- 0\n  condition &lt;- TRUE\n  while (condition) {\n    uu &lt;- ppois(x, lambda = 6)\n    condition &lt;- uu &lt;= u\n    if(condition){\n      x &lt;- x + 1\n    }\n  }\n  return(x)\n}\nxx &lt;- sapply(u, finder)  \nxx |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) +\ngeom_histogram(bins = 21) +\ngeom_step(data = tibble(x = xe, y = dpois(xe, lambda = 6)),\n          mapping = aes(x,y)) +\ntheme_bw()"
  },
  {
    "objectID": "lectures/3.html#exponential-rv-in-r-1",
    "href": "lectures/3.html#exponential-rv-in-r-1",
    "title": "Monte Carlo Methods",
    "section": "Exponential RV in R",
    "text": "Exponential RV in R\n\nDescriptionCode\n\n\nThe Poisson distribution can be simulated in R using the rpois function with the following arguments:\n\nn: number of values to generate\nlambda: the average expected event\n\n\n\n\n\nCode\nrpois(1, lambda = 1)\n\n\n#&gt; [1] 0"
  },
  {
    "objectID": "lectures/3.html#normal-distribution-1",
    "href": "lectures/3.html#normal-distribution-1",
    "title": "Monte Carlo Methods",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nObtaining the inverse distribution function of a normal distribution requires the use of numeric algorithms.\n\nTherefore it is computationally inefficient to use the inverse-transformation algorithm to generate normal random variables.\n\n\nThe Box-Muller algorithm was developed to generate 2 standard normal (\\(N(0,1)\\)) random variables from uniform random variables."
  },
  {
    "objectID": "lectures/3.html#normal-distribution-2",
    "href": "lectures/3.html#normal-distribution-2",
    "title": "Monte Carlo Methods",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\\[\ny = \\int^x_{-\\infty}\n\\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{-\\frac{z^2}{2}\\right\\}dz\n\\]"
  },
  {
    "objectID": "lectures/3.html#box-muller-algorithm",
    "href": "lectures/3.html#box-muller-algorithm",
    "title": "Monte Carlo Methods",
    "section": "Box-Muller Algorithm",
    "text": "Box-Muller Algorithm\n\nGenerate 2 independent random variables from \\(U(0,1)\\), \\(U_1\\) and \\(U_2\\)\n\\(X_1 = (-2 \\log(U_1))^{1/2}\\cos(2\\pi U_2)\\)\n\\(X_2 = (-2 \\log(U_1))^{1/2}\\sin(2\\pi U_2)\\)\n\nBoth \\(X_1\\) and \\(X_2\\) are independent \\(N(0,1)\\)"
  },
  {
    "objectID": "lectures/3.html#normal-distribution-r",
    "href": "lectures/3.html#normal-distribution-r",
    "title": "Monte Carlo Methods",
    "section": "Normal Distribution R",
    "text": "Normal Distribution R\n\nDescriptionCode\n\n\nThe normal distribution can be simulated in R using the rnorm function with the following arguments:\n\nn: number of values to generate\nmean: the central tendency (peak)\nsd: the variation of the data (width)\n\n\n\n\n\nCode\nrnorm(1, mean = 0, sd = 1)\n\n\n#&gt; [1] -0.4939713"
  },
  {
    "objectID": "lectures/3.html#accept-reject-algorithm",
    "href": "lectures/3.html#accept-reject-algorithm",
    "title": "Monte Carlo Methods",
    "section": "Accept-Reject Algorithm",
    "text": "Accept-Reject Algorithm\nThe Accept-Reject algorithm allows you to generate noncommon random variable by simulating from a common random variable."
  },
  {
    "objectID": "lectures/3.html#algorithm-set-up",
    "href": "lectures/3.html#algorithm-set-up",
    "title": "Monte Carlo Methods",
    "section": "Algorithm Set Up",
    "text": "Algorithm Set Up\nLet \\(X\\) be the random variable, that is difficult to generate, you want to generate with a pdf \\(f(x)\\).\nLet \\(Y\\) be an easily generated random variable with a pdf \\(g(y)\\). That follows the same support as \\(f(x)\\)\nLastly, multiply \\(g(y)\\) with a constant \\(c\\) such that \\(f(y)\\leq cg(y)\\)."
  },
  {
    "objectID": "lectures/3.html#algorithm",
    "href": "lectures/3.html#algorithm",
    "title": "Monte Carlo Methods",
    "section": "Algorithm",
    "text": "Algorithm\n\nGenerate \\(Y\\) with a pdf of \\(g(y)\\)\nGenerate \\(U\\) from \\(U(0, cg(y))\\)\nAccept-Reject\nAccept: \\(U\\leq f(y)\\); \\(Y \\rightarrow X\\)\nReject: \\(U&gt;f(y)\\); repeat the algorithm"
  },
  {
    "objectID": "lectures/3.html#modified-algorithm",
    "href": "lectures/3.html#modified-algorithm",
    "title": "Monte Carlo Methods",
    "section": "Modified Algorithm",
    "text": "Modified Algorithm\n\nGenerate \\(Y\\) with a pdf of \\(g(y)\\)\nGenerate \\(U\\) from \\(U(0,1)\\)\nAccept-Reject\nAccept: \\(U\\leq f(y)/(cg(y))\\); \\(Y \\rightarrow X\\)\nReject: \\(U&gt;f(y)/(cg(y))\\); repeat the algorithm"
  },
  {
    "objectID": "lectures/3.html#gamma-random-variable",
    "href": "lectures/3.html#gamma-random-variable",
    "title": "Monte Carlo Methods",
    "section": "Gamma Random Variable",
    "text": "Gamma Random Variable\n\n\nCode\nxe &lt;- seq(0, 20, length.out = 1000)\nxe |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = dgamma(x, shape = 2.3, scale = 1.2))) + \ngeom_line() +\nylab(\"Density\") +\ntheme_bw()"
  },
  {
    "objectID": "lectures/3.html#gamma-rv",
    "href": "lectures/3.html#gamma-rv",
    "title": "Monte Carlo Methods",
    "section": "Gamma RV",
    "text": "Gamma RV\n\n\nCode\nxe &lt;- seq(0, 20, length.out = 1000)\nx &lt;- rexp(100000)\nx |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) + \ngeom_histogram(aes(color = \"Exponential\")) +\ngeom_line(data = tibble(x = xe, \n                        y = dgamma(x, shape = 2.3, scale = 1.2)), \n          aes(x,y, color = \"Gamma\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())"
  },
  {
    "objectID": "lectures/3.html#gamma-rv-1",
    "href": "lectures/3.html#gamma-rv-1",
    "title": "Monte Carlo Methods",
    "section": "Gamma RV",
    "text": "Gamma RV\n\n\nCode\nxe &lt;- seq(0, 20, length.out = 1000)\nxe |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = dgamma(x, shape = 2.3, scale = 1.2))) + \ngeom_line(aes(color = \"Gamma\")) +\ngeom_line(data = tibble(x = xe, y = dexp(xe, 1/3)), aes(x,y, color = \"Exponential\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())"
  },
  {
    "objectID": "lectures/3.html#accept-reject-gamma-rv",
    "href": "lectures/3.html#accept-reject-gamma-rv",
    "title": "Monte Carlo Methods",
    "section": "Accept-Reject Gamma RV",
    "text": "Accept-Reject Gamma RV\n\n\nCode\nxe &lt;- seq(0, 20, length.out = 1000)\nxe |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = dgamma(x, shape = 2.3, scale = 1.2))) + \ngeom_line(aes(color = \"Gamma\")) +\ngeom_line(data = tibble(x = xe, y = 1.5*dexp(xe, 1/3)), aes(x,y, color = \"Exponential\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())"
  },
  {
    "objectID": "lectures/3.html#accept-reject-gamma-rv-1",
    "href": "lectures/3.html#accept-reject-gamma-rv-1",
    "title": "Monte Carlo Methods",
    "section": "Accept-Reject Gamma RV",
    "text": "Accept-Reject Gamma RV\n\n\nCode\nxe &lt;- seq(0, 20, length.out = 1000)\nxe |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = dgamma(x, shape = 2.3, scale = 1.2))) + \ngeom_line(aes(color = \"Gamma\")) +\ngeom_line(data = tibble(x = xe, y = 3*dexp(xe, 1/3)), aes(x,y, color = \"Exponential\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())"
  },
  {
    "objectID": "lectures/3.html#accept-reject-gamma-rv-2",
    "href": "lectures/3.html#accept-reject-gamma-rv-2",
    "title": "Monte Carlo Methods",
    "section": "Accept-Reject Gamma RV",
    "text": "Accept-Reject Gamma RV\n\n\nCode\nx &lt;- c()\nn &lt;- 0\nwhile(n &lt; 10000){\n  e &lt;- rexp(1, 1/2.3)\n  u &lt;- runif(1)\n  f &lt;- dgamma(e, 2.3, 1/1.2)\n  g &lt;- dexp(e, 1/2.3) * 3\n  if (u &lt; (f/g)){\n    x &lt;- c(x, e)\n    n &lt;- length(x)\n  }\n}"
  },
  {
    "objectID": "lectures/3.html#gamma-rv-2",
    "href": "lectures/3.html#gamma-rv-2",
    "title": "Monte Carlo Methods",
    "section": "Gamma RV",
    "text": "Gamma RV\n\n\nCode\nx |&gt; tibble(x = _) |&gt; \nggplot(aes(x=x, y = ..density..)) + \ngeom_histogram(aes(color = \"Exponential\")) +\ngeom_line(data = tibble(x = xe, \n                        y = dgamma(x, shape = 2.3, scale = 1.2)), \n          aes(x,y, color = \"Gamma\")) +\nylab(\"Density\") +\ntheme_bw() +\ntheme(legend.position = \"bottom\",\n      legend.title = element_blank())"
  },
  {
    "objectID": "lectures/3.html#gamma-distribution-r",
    "href": "lectures/3.html#gamma-distribution-r",
    "title": "Monte Carlo Methods",
    "section": "Gamma Distribution R",
    "text": "Gamma Distribution R\n\nDescriptionCode\n\n\nThe gamma distribution can be simulated in R using the rgamma function with the following arguments:\n\nn: number of values to generate\nshape: describes the shape of distribution (\\(\\alpha\\))\nscale: the spread of the data (\\(\\beta\\))\n\n\n\n\n\nCode\nrgamma(1, shape = 1.2, rate = .5)\n\n\n#&gt; [1] 3.404348"
  },
  {
    "objectID": "lectures/3.html#beta-rv-in-r",
    "href": "lectures/3.html#beta-rv-in-r",
    "title": "Monte Carlo Methods",
    "section": "Beta RV in R",
    "text": "Beta RV in R\n\nDescriptionCode\n\n\nThe beta distribution can be simulated in R using the rbeta function with the following arguments:\n\nn: number of values to generate\nshape1: controls the shape of distribution\nshape2: controls the shape of distribution\n\n\n\n\n\nCode\nrbeta(1, shape1 = 1.2, shape2 = 6.5)\n\n\n#&gt; [1] 0.1582012"
  },
  {
    "objectID": "lectures/3.html#bernoulli-rv-in-r",
    "href": "lectures/3.html#bernoulli-rv-in-r",
    "title": "Monte Carlo Methods",
    "section": "Bernoulli RV in R",
    "text": "Bernoulli RV in R\n\nDescriptionCode\n\n\nThe bernoulli distribution can be simulated in R using the rbinom function with the following arguments:\n\nn: number of values to generate\nsize = 1: will give a bernoulli distribution\nprob: probability of observing 1 (success)\n\n\n\n\n\nCode\nrbinom(1, prob = .2, size = 1)\n\n\n#&gt; [1] 0"
  },
  {
    "objectID": "lectures/3.html#binomial-rv-in-r",
    "href": "lectures/3.html#binomial-rv-in-r",
    "title": "Monte Carlo Methods",
    "section": "Binomial RV in R",
    "text": "Binomial RV in R\n\nDescriptionCode\n\n\nThe binomial distribution can be simulated in R using the rbinom function with the following arguments:\n\nn: number of values to generate\nsize: how many bernoulli trials to conduct\nprob: probability of observing 1 (success)\n\n\n\n\n\nCode\nrbinom(1, prob = .5, size = 25)\n\n\n#&gt; [1] 13"
  },
  {
    "objectID": "lectures/3.html#negative-binomial-rv-in-r",
    "href": "lectures/3.html#negative-binomial-rv-in-r",
    "title": "Monte Carlo Methods",
    "section": "Negative Binomial RV in R",
    "text": "Negative Binomial RV in R\n\nDescriptionCode\n\n\nThe negative binomial distribution can be simulated in R using the rnbinom function with the following arguments:\n\nn: number of values to generate\nsize: number of successful trials\nprob: probability of observing 1 (success)\n\n\n\n\n\nCode\nrnbinom(1, prob = .6, size = 5)\n\n\n#&gt; [1] 0"
  },
  {
    "objectID": "lectures/3.html#n01",
    "href": "lectures/3.html#n01",
    "title": "Monte Carlo Methods",
    "section": "\\(N(0,1)\\)",
    "text": "\\(N(0,1)\\)\n\\[\nX \\sim N(\\mu, \\sigma^2)\n\\]\n\\[\nZ = \\frac{X-\\mu}{\\sigma} \\sim N(0,1)\n\\]"
  },
  {
    "objectID": "lectures/3.html#nmu-sigma2",
    "href": "lectures/3.html#nmu-sigma2",
    "title": "Monte Carlo Methods",
    "section": "\\(N(\\mu, \\sigma^2)\\)",
    "text": "\\(N(\\mu, \\sigma^2)\\)\n\\[\nZ \\sim N(0,1)\n\\]\n\\[\nX = Z\\sigma + \\mu \\sim N(\\mu, \\sigma^2)\n\\]"
  },
  {
    "objectID": "lectures/3.html#chi21",
    "href": "lectures/3.html#chi21",
    "title": "Monte Carlo Methods",
    "section": "\\(\\chi^2(1)\\)",
    "text": "\\(\\chi^2(1)\\)\n\\[\nZ \\sim N(0,1)\n\\]\n\\[\nZ^2 \\sim \\chi^2(1)\n\\]"
  },
  {
    "objectID": "lectures/3.html#fmn",
    "href": "lectures/3.html#fmn",
    "title": "Monte Carlo Methods",
    "section": "\\(F(m,n)\\)",
    "text": "\\(F(m,n)\\)\n\\[\nU \\sim \\chi^2(m)\n\\]\n\\[\nV \\sim \\chi^2(n)\n\\]\n\\[\nF = \\frac{U/m}{V/n} \\sim F(m,n)\n\\]"
  },
  {
    "objectID": "lectures/3.html#tn",
    "href": "lectures/3.html#tn",
    "title": "Monte Carlo Methods",
    "section": "\\(t(n)\\)",
    "text": "\\(t(n)\\)\n\\[\nZ \\sim N(0,1)\n\\]\n\\[\nU \\sim \\chi^2(m)\n\\]\n\\[\nT = \\frac{Z}{\\sqrt{U/m}} \\sim t(n)\n\\]"
  },
  {
    "objectID": "lectures/3.html#betaalpha-beta",
    "href": "lectures/3.html#betaalpha-beta",
    "title": "Monte Carlo Methods",
    "section": "\\(Beta(\\alpha, \\beta)\\)",
    "text": "\\(Beta(\\alpha, \\beta)\\)\n\\[\nU \\sim Gamma(\\alpha,\\lambda)\n\\]\n\\[\nV \\sim Gamma(\\beta,\\lambda)\n\\]\n\\[\nX = \\frac{U}{U+V} \\sim Beta(\\alpha,\\beta)\n\\]"
  },
  {
    "objectID": "lectures/3.html#central-limit-theorem-1",
    "href": "lectures/3.html#central-limit-theorem-1",
    "title": "Monte Carlo Methods",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf random variables \\(X_1, X_2, \\cdots, X_n\\) are independent come from the same distribution (\\(iid\\)), \\(E(X_i) = \\mu &lt;\\infty\\) (finite), \\(Var(X_i) = \\sigma^2&lt;\\infty\\) (finite), then\n\\[\n\\bar X \\sim N(\\mu, \\sigma^2/n)\n\\]"
  },
  {
    "objectID": "lectures/3.html#normal-distribution-3",
    "href": "lectures/3.html#normal-distribution-3",
    "title": "Monte Carlo Methods",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nFor \\(n\\) random variables:\n\\[\nX_i \\sim N(4, 32)\n\\] By CLT\n\\[\n\\bar X \\sim N(4, 32 / n)\n\\]"
  },
  {
    "objectID": "lectures/3.html#normal-distribution-4",
    "href": "lectures/3.html#normal-distribution-4",
    "title": "Monte Carlo Methods",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\nCode\nnorm &lt;- function(x){\n  rnorm(10000, 4, sqrt(32))\n}\nX &lt;- sapply(1:1000, norm)\nXbar &lt;- apply(X, 2, mean)\nxx &lt;- seq(min(Xbar)-.1, max(Xbar)+.1, length.out = 100)\nXbar |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x=x, y = ..density..)) +\n    geom_histogram() +\n    geom_line(data = tibble(x = xx, y = dnorm(xx, 4, sqrt(32/10000))),\n              aes(x, y))"
  },
  {
    "objectID": "lectures/3.html#normal-distribution-5",
    "href": "lectures/3.html#normal-distribution-5",
    "title": "Monte Carlo Methods",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\nCode\nnorm &lt;- function(x){\n  rnorm(10000, 4, sqrt(32))\n}\nX &lt;- sapply(1:1000, norm)\nXbar &lt;- apply(X, 2, mean)\nxx &lt;- seq(min(Xbar)-.1, max(Xbar)+.1, length.out = 100)\nXbar |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x=x, y = ..density..)) +\n    geom_histogram() +\n    geom_line(data = tibble(x = xx, y = dnorm(xx, 4, sqrt(32/10000))),\n              aes(x, y)) +\n    theme_bw()"
  },
  {
    "objectID": "lectures/3.html#poisson-distribution-3",
    "href": "lectures/3.html#poisson-distribution-3",
    "title": "Monte Carlo Methods",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n\nCode\nnorm &lt;- function(x){\n  rpois(10000, 8.5)\n}\nX &lt;- sapply(1:1000, norm)\nXbar &lt;- apply(X, 2, mean)\nxx &lt;- seq(min(Xbar)-.1, max(Xbar)+.1, length.out = 100)\nXbar |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x=x, y = ..density..)) +\n    geom_histogram() +\n    geom_line(data = tibble(x = xx, y = dnorm(xx, 8.5, sqrt(8.5/10000))),\n              aes(x, y))"
  },
  {
    "objectID": "lectures/3.html#gamma-distribution-1",
    "href": "lectures/3.html#gamma-distribution-1",
    "title": "Monte Carlo Methods",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\n\n\nCode\nnorm &lt;- function(x){\n  rgamma(10000, shape = 5, scale = 2)\n}\nX &lt;- sapply(1:1000, norm)\nXbar &lt;- apply(X, 2, mean)\nxx &lt;- seq(min(Xbar)-.1, max(Xbar)+.1, length.out = 100)\nXbar |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x=x, y = ..density..)) +\n    geom_histogram() +\n    geom_line(data = tibble(x = xx, y = dnorm(xx, 10, sqrt(20/10000))),\n              aes(x, y))"
  },
  {
    "objectID": "lectures/3.html#cauchy-distribution",
    "href": "lectures/3.html#cauchy-distribution",
    "title": "Monte Carlo Methods",
    "section": "Cauchy Distribution",
    "text": "Cauchy Distribution\n\n\nCode\nnorm &lt;- function(x){\n  rcauchy(10000, location = -1)\n}\nX &lt;- sapply(1:1000, norm)\nXbar &lt;- apply(X, 2, mean)\nXbar |&gt; tibble(x = _) |&gt; \n  ggplot(aes(x=x)) +\n    geom_histogram(bins = 100)"
  },
  {
    "objectID": "lectures/7.html#r-packages",
    "href": "lectures/7.html#r-packages",
    "title": "Monte Carlo Methods",
    "section": "R Packages",
    "text": "R Packages\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\ntheme_set(theme_bw())\ntheme_update(axis.title = element_text(size = 24))\n\n\ntuesdata &lt;- tidytuesdayR::tt_load(2020, week = 28)\n\n#&gt; \n#&gt;  Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee_ratings &lt;- tuesdata$coffee_ratings\ncoffee_aroma &lt;- coffee_ratings |&gt; filter(aroma &gt; 5.5)\n\nshuffle &lt;- function(x){\n  n &lt;- length(x)\n  return(sample(x, n))\n}\n\nresample &lt;- function(df){\n  if (!is.data.frame(df)){\n    stop(\"The df object must be a data frame.\")\n  }\n  dplyr::slice_sample(df, n = nrow(df), replace = T )\n}\n\n\npenguins &lt;- penguins |&gt; drop_na()"
  },
  {
    "objectID": "lectures/7.html#empirical-distribution-function",
    "href": "lectures/7.html#empirical-distribution-function",
    "title": "Monte Carlo Methods",
    "section": "Empirical Distribution Function",
    "text": "Empirical Distribution Function\nThe empirical distribution function is designed to estimate a random variable’s distribution function. For an observed sample \\(\\{x_i\\}^n_{i=1}\\), the empirical distribution function is\n\\[\nF_n(x) \\left\\{\\begin{array}{cc}\n0, & x &lt; x_{(1)} \\\\\n\\frac{i}{n},& x_{(i)} \\leq x &lt;x_{(i+1)},\\ i = 1,\\ldots,n-1\\\\\n1,& x_{(n)}\\leq x\n\\end{array}\n\\right.\n\\]\nwhere \\(x_{(1)}, \\ldots, x_{(n)}\\) are the ordered sample."
  },
  {
    "objectID": "lectures/7.html#sampling-an-unknown-f",
    "href": "lectures/7.html#sampling-an-unknown-f",
    "title": "Monte Carlo Methods",
    "section": "Sampling an unknown \\(F\\)",
    "text": "Sampling an unknown \\(F\\)\nThe idea behind bootstrapping is that the data comes from a distribution \\(F\\) with unknown parameters.\nUsing the sample, we can get parameters that explain a parameteric distribution or the emperical distribution for a nonparameteric approach."
  },
  {
    "objectID": "lectures/7.html#the-bootstrap-method",
    "href": "lectures/7.html#the-bootstrap-method",
    "title": "Monte Carlo Methods",
    "section": "The Bootstrap Method",
    "text": "The Bootstrap Method\nThe Bootstrap Method utilizes the sample to describe the target distribution function to construct a sampling mechanism of the target distribution.\nThis method will allow us to construct a new sample that targets the distribution.\nWe can then construct the sampling distribution of a statistic based on the data."
  },
  {
    "objectID": "lectures/7.html#standard-error",
    "href": "lectures/7.html#standard-error",
    "title": "Monte Carlo Methods",
    "section": "Standard Error",
    "text": "Standard Error\nThe bootstrap-based standard error of a test statistic is shown to provide an unbiased estimate of the true standard error."
  },
  {
    "objectID": "lectures/7.html#limitation-to-boostrap-methods",
    "href": "lectures/7.html#limitation-to-boostrap-methods",
    "title": "Monte Carlo Methods",
    "section": "Limitation to Boostrap Methods",
    "text": "Limitation to Boostrap Methods\nThe assumption is that the data provides a good estimate of the distribution function.\nIf the data set is small, it may not contain enough information to accurately describe the distribution."
  },
  {
    "objectID": "lectures/7.html#limitation-example",
    "href": "lectures/7.html#limitation-example",
    "title": "Monte Carlo Methods",
    "section": "Limitation Example",
    "text": "Limitation Example"
  },
  {
    "objectID": "lectures/7.html#parameteric-bootstrap-1",
    "href": "lectures/7.html#parameteric-bootstrap-1",
    "title": "Monte Carlo Methods",
    "section": "Parameteric Bootstrap",
    "text": "Parameteric Bootstrap\nParametric bootstrap methods are statistical techniques used to estimate the sampling distribution of an estimator or test statistic by resampling with a model-based approach. This method assumes that the data follow a known probability distribution, and utilizes the estimated statistics as the parameters for the distribution function to construct the sampling distribution."
  },
  {
    "objectID": "lectures/7.html#parameteric-bootstrap-algorithm",
    "href": "lectures/7.html#parameteric-bootstrap-algorithm",
    "title": "Monte Carlo Methods",
    "section": "Parameteric Bootstrap Algorithm",
    "text": "Parameteric Bootstrap Algorithm\n\nEstimate the Parameters: Fit a parametric model to the observed data and estimate the parameters of the model.\nGenerate Bootstrap Samples: Using the estimated parameters, generate a large number of new data sets (bootstrap samples) from the fitted model. These samples are simulated data sets that mimic the original data but are generated from the parametric model.\nCompute the Statistic of Interest: For each bootstrap sample, calculate the statistic of interest (e.g., the mean, variance, regression coefficients, etc.).\nConstruct the Sampling Distribution: Use the calculated statistics from all the bootstrap samples to construct an empirical sampling distribution.\nEstimate Confidence Intervals: Use the empirical sampling distribution to estimate confidence intervals."
  },
  {
    "objectID": "lectures/7.html#example",
    "href": "lectures/7.html#example",
    "title": "Monte Carlo Methods",
    "section": "Example",
    "text": "Example\nUse a parameteric bootstrap model to determine the standard errors of the mean body mass of each penguin species.\n\n\nCode\npenguins |&gt; group_by(species) |&gt; \n  summarise(mean = mean(body_mass_g),\n            se = sd(body_mass_g) / sqrt(n()))\n\n\n#&gt; # A tibble: 3 × 3\n#&gt;   species    mean    se\n#&gt;   &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Adelie    3706.  38.0\n#&gt; 2 Chinstrap 3733.  46.6\n#&gt; 3 Gentoo    5092.  46.0\n\n\nAnswer:\n\n\nCode\nmeans &lt;- penguins$body_mass_g |&gt; tapply(penguins$species, mean)\nnns &lt;- penguins$body_mass_g |&gt; tapply(penguins$species, length)\nsds &lt;- penguins$body_mass_g |&gt; tapply(penguins$species, sd)\nAmeans &lt;- numeric(10000)\nCmeans &lt;- numeric(10000)\nGmeans &lt;- numeric(10000)\nfor (i in 1:10000){\n  Ameans[i] &lt;- rnorm(nns[1], mean = means[1], sd = sds[1]) |&gt; mean()\n  Cmeans[i] &lt;- rnorm(nns[2], mean = means[2], sd = sds[2]) |&gt; mean()\n  Gmeans[i] &lt;- rnorm(nns[3], mean = means[3], sd = sds[3]) |&gt; mean()\n}"
  },
  {
    "objectID": "lectures/7.html#nonparameteric-bootsrap",
    "href": "lectures/7.html#nonparameteric-bootsrap",
    "title": "Monte Carlo Methods",
    "section": "Nonparameteric Bootsrap",
    "text": "Nonparameteric Bootsrap\nThe nonparameteric approach assumes that distribution function of the data does not follow a common distribution function. Therefore, the data itself will be contain all the information needed to construct the sampling distribution.\nThis requires sampling with replacement."
  },
  {
    "objectID": "lectures/7.html#nonparameteric-bootstrap-algorithm",
    "href": "lectures/7.html#nonparameteric-bootstrap-algorithm",
    "title": "Monte Carlo Methods",
    "section": "Nonparameteric Bootstrap Algorithm",
    "text": "Nonparameteric Bootstrap Algorithm\n\nDraw a sample \\(X*\\) of size \\(n\\) with replacement from the original data \\(X\\).\n\n\\(n\\) is the size of the data\n\nCompute the bootstrap replicate statistic \\(T* = g(X*)\\), where \\(g(\\cdot)\\) is the function that computes the statistic of interest.\nRepeat steps 1-2 \\(B\\) times to obtain \\(B\\) bootstrap replicates \\({T*_1, T*_2, ..., T*_B}\\).\nThe computed statistics from \\(B\\) samples are the empirical bootstrap distribution of the statistic, \\(g(X)\\).\nCalculate the bootstrap standard error of the statistic, \\(se_b(g(X))\\), as the standard deviation of the bootstrap replicates.\nCalculate the bootstrap confidence interval for the statistic, \\(CI(g(X))\\), with the \\(\\alpha\\) and \\((1-\\alpha)%\\) percentiles of the bootstrap replicates, where \\(\\alpha\\) is the desired level of significance."
  },
  {
    "objectID": "lectures/7.html#example-1",
    "href": "lectures/7.html#example-1",
    "title": "Monte Carlo Methods",
    "section": "Example",
    "text": "Example\nFitting the following model:\n\n\nCode\nlibrary(palmerpenguins)\nlibrary(tidyverse)\npenguins &lt;- penguins |&gt; drop_na()\npenguins |&gt; lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm,\n               data = _)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n#&gt;     bill_depth_mm, data = penguins)\n#&gt; \n#&gt; Coefficients:\n#&gt;       (Intercept)  flipper_length_mm     bill_length_mm      bill_depth_mm  \n#&gt;         -6445.476             50.762              3.293             17.836\n\n\nObtain the Bootstrap-based Standard Errors for the regression coefficients. Use \\(B=1000\\) bootstrap samples."
  },
  {
    "objectID": "lectures/7.html#markov-chain",
    "href": "lectures/7.html#markov-chain",
    "title": "Monte Carlo Methods",
    "section": "Markov Chain",
    "text": "Markov Chain\n\n\nA Markov chain is a collection states of a certain phenomenom\n\n\\(X^{(0)},X^{(1)},X^{(2)},X^{(3)},X^{(4)},X^{(5)},X^{(6)},X^{(7)}, \\cdots, X^{(k)}\\)\n\nThe changing of the state is only dependent on the current state, not the previous states\n\n\\(P\\left\\{X^{(k+1)}\\boldsymbol{\\Big|}X^{(k)},X^{(k-1)},X^{(k-2)},\\ldots,X^{(1)},X^{(0)}\\right\\}=P\\left\\{X^{(k+1)}\\boldsymbol{\\Big |}X^{(k)}\\right\\}\\)"
  },
  {
    "objectID": "lectures/7.html#cat-markov-chains",
    "href": "lectures/7.html#cat-markov-chains",
    "title": "Monte Carlo Methods",
    "section": "Cat Markov Chains",
    "text": "Cat Markov Chains"
  },
  {
    "objectID": "lectures/7.html#markov-kernel",
    "href": "lectures/7.html#markov-kernel",
    "title": "Monte Carlo Methods",
    "section": "Markov Kernel",
    "text": "Markov Kernel\n\n\n\n\nA Markov kernel provides the probability of going to another state, given the current state\nAlso known a transition matrix"
  },
  {
    "objectID": "lectures/7.html#stationary-limiting-distribution",
    "href": "lectures/7.html#stationary-limiting-distribution",
    "title": "Monte Carlo Methods",
    "section": "Stationary (limiting) distribution",
    "text": "Stationary (limiting) distribution\n\n\nConditions\n\n\nIrreducibility: The kernel allows for free movement of all the state space\nRecurrent: The chain will return to any nonnegligible set an infinite number of times\nAperiodic: The chain can return to any state immediately\n\n\n\nResulting\n\n\n\\(X^{(t)}\\rightarrow X\\)\n\nRegardless of \\(X^{(0)}\\)\n\n\\(X \\sim f\\)\n\n\\(f\\): is a distribution function\n\n\\(\\frac{1}{T}\\sum_{t=1}^{T} h\\{X^{(t)}\\} \\rightarrow E_f\\{h(X)\\}\\)\n\n\\(h\\): any integrable function\nby Law of Large Numbers"
  },
  {
    "objectID": "lectures/7.html#markov-chains-monte-carlo",
    "href": "lectures/7.html#markov-chains-monte-carlo",
    "title": "Monte Carlo Methods",
    "section": "Markov Chains Monte Carlo",
    "text": "Markov Chains Monte Carlo\n\n\nMCMC Methods are used to a distribution function that is not easily obtained.\nA Markov chain is contructed by simulating Monte Carlo Samples and accepted based on a certain criteria\nBased on the MCMC Central Limit Theorem, the Markov chain will construct a limiting distribution that is desired."
  },
  {
    "objectID": "lectures/7.html#hamiltonian-monte-carlo",
    "href": "lectures/7.html#hamiltonian-monte-carlo",
    "title": "Monte Carlo Methods",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\n\nHamiltonian Monte Carlo is a relatively new MCMC technique used to construct the target distribution\nIt utilizes Hamiltonian dynamics to simulate the next random variable\nThe random variable is the accepted based the MH probability\nUsing Hamiltonian dyanmics improves the mixing properties of the chain and draws are more targeted to the desired distribution"
  },
  {
    "objectID": "lectures/7.html#bayesian-analysis-1",
    "href": "lectures/7.html#bayesian-analysis-1",
    "title": "Monte Carlo Methods",
    "section": "Bayesian Analysis",
    "text": "Bayesian Analysis"
  },
  {
    "objectID": "lectures/7.html#bayesian-analysis-in-r",
    "href": "lectures/7.html#bayesian-analysis-in-r",
    "title": "Monte Carlo Methods",
    "section": "Bayesian Analysis in R",
    "text": "Bayesian Analysis in R\n\n\nCode\nlibrary(brms)\ny &lt;- rnorm(10000, mean = 3, sd = 1)\nbrm(y~1, data = tibble(y = y),\n    family = gaussian())"
  },
  {
    "objectID": "lectures/7.html#bayesian-analysis-in-r-1",
    "href": "lectures/7.html#bayesian-analysis-in-r-1",
    "title": "Monte Carlo Methods",
    "section": "Bayesian Analysis in R",
    "text": "Bayesian Analysis in R\n\n\nCode\nlibrary(brms)\ny &lt;- rpois(10000, 3)\nbrm(y~1, data = tibble(y = y),\n    family = poisson())"
  },
  {
    "objectID": "lectures/15b.html#supervised-machine-learning",
    "href": "lectures/15b.html#supervised-machine-learning",
    "title": "Unsupervised\nMachine Learning",
    "section": "Supervised Machine Learning",
    "text": "Supervised Machine Learning\n\\[\n\\boldsymbol X = (X_1, X_2, \\cdots, X_n)\\mrTr\n\\] where\n\\[\nX_i = (x_{i1}, x_{i2}, \\cdots, x_{in})\n\\] and\n\\[\n\\boldsymbol Y = (Y_1, Y_2, \\cdots, Y_n)\\mrTr\n\\]"
  },
  {
    "objectID": "lectures/15b.html#unsupervised-machine-learning-1",
    "href": "lectures/15b.html#unsupervised-machine-learning-1",
    "title": "Unsupervised\nMachine Learning",
    "section": "Unsupervised Machine Learning",
    "text": "Unsupervised Machine Learning\nGiven:\n\\[\n\\boldsymbol X = (X_1, X_2, \\cdots, X_n)\\mrTr\n\\] where\n\\[\nX_i = (x_{i1}, x_{i2}, \\cdots, x_{in})\n\\]\nGroup the data to \\(K\\) categories."
  },
  {
    "objectID": "lectures/15b.html#unsupervised-machine-learning-2",
    "href": "lectures/15b.html#unsupervised-machine-learning-2",
    "title": "Unsupervised\nMachine Learning",
    "section": "Unsupervised Machine Learning",
    "text": "Unsupervised Machine Learning\nWe can naturally group data with the following techniques:\n\nPrincipal Component Analysis\nK-Means Clustering\nHierarchical Clustering\nMixture Models"
  },
  {
    "objectID": "lectures/15b.html#principal-componenet-analysis",
    "href": "lectures/15b.html#principal-componenet-analysis",
    "title": "Unsupervised\nMachine Learning",
    "section": "Principal Componenet Analysis",
    "text": "Principal Componenet Analysis\nPrincipal Component Analysis (PCA) is a dimensionality reduction technique used to simplify complex datasets while retaining as much of the original variability as possible. It accomplishes this by transforming the original variables into a new set of orthogonal variables called principal components. PCA is widely used in data analysis, visualization, and machine learning for tasks such as feature extraction, data compression, and noise reduction."
  },
  {
    "objectID": "lectures/15b.html#k-means-clustering",
    "href": "lectures/15b.html#k-means-clustering",
    "title": "Unsupervised\nMachine Learning",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nK-Means clustering is one of the most popular unsupervised machine learning algorithms used for partitioning a dataset into a predetermined number of clusters. It aims to group similar data points together and discover underlying patterns or structures within the data. K-Means is simple, efficient, and widely applicable in various domains, including data analysis, image processing, and customer segmentation."
  },
  {
    "objectID": "lectures/15b.html#hierarchical-clustering",
    "href": "lectures/15b.html#hierarchical-clustering",
    "title": "Unsupervised\nMachine Learning",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering is a method used to cluster data into a hierarchy of clusters. Unlike K-Means, which requires specifying the number of clusters upfront, hierarchical clustering builds a tree-like structure (dendrogram) that reflects the relationships between data points at different levels of granularity. Hierarchical clustering can be divided into two main types: agglomerative and divisive."
  },
  {
    "objectID": "lectures/15b.html#mixture-models",
    "href": "lectures/15b.html#mixture-models",
    "title": "Unsupervised\nMachine Learning",
    "section": "Mixture Models",
    "text": "Mixture Models\nMixture models for clustering, often referred to as Gaussian Mixture Models (GMMs), are probabilistic models used to describe the distribution of data as a mixture of multiple Gaussian distributions. Unlike K-Means or hierarchical clustering, which assign data points to discrete clusters, GMMs represent each cluster as a probability distribution over the entire feature space. This allows for more flexible modeling of complex data distributions and enables soft assignment of data points to clusters based on their probabilities."
  },
  {
    "objectID": "lectures/15b.html#topic-modelling",
    "href": "lectures/15b.html#topic-modelling",
    "title": "Unsupervised\nMachine Learning",
    "section": "Topic Modelling",
    "text": "Topic Modelling\nTopic modeling is a statistical technique used to identify latent topics or themes within a collection of text documents. It aims to uncover the underlying structure of the text data by automatically clustering documents into topics based on the distribution of words across documents. Topic modeling is widely used in natural language processing (NLP) and text mining for tasks such as document clustering, information retrieval, and content analysis."
  },
  {
    "objectID": "lectures/15b.html#latent-dirichlet-allocation",
    "href": "lectures/15b.html#latent-dirichlet-allocation",
    "title": "Unsupervised\nMachine Learning",
    "section": "Latent Dirichlet Allocation",
    "text": "Latent Dirichlet Allocation\nLatent Dirichlet Allocation (LDA) is a probabilistic model used for topic modeling in natural language processing (NLP). LDA assumes that each document in the corpus is generated by a probabilistic process involving a mixture of topics. It posits that documents exhibit multiple topics, and each word within a document is associated with one of these topics."
  },
  {
    "objectID": "lectures/15b.html#structural-topic-model",
    "href": "lectures/15b.html#structural-topic-model",
    "title": "Unsupervised\nMachine Learning",
    "section": "Structural Topic Model",
    "text": "Structural Topic Model\nThe Structural Topic Model (STM) is an extension of the Latent Dirichlet Allocation (LDA) model that incorporates document metadata and covariates to capture the structural aspects of text data. Unlike LDA, which assumes that topics are generated independently of document metadata, STM allows for the incorporation of metadata or covariates associated with each document. Covariates could include document-level characteristics such as authorship, publication year, geographic location, or any other relevant metadata."
  },
  {
    "objectID": "lectures/15b.html#text-mining-resource",
    "href": "lectures/15b.html#text-mining-resource",
    "title": "Unsupervised\nMachine Learning",
    "section": "Text Mining Resource",
    "text": "Text Mining Resource"
  },
  {
    "objectID": "lectures/15b.html#r-packages",
    "href": "lectures/15b.html#r-packages",
    "title": "Unsupervised\nMachine Learning",
    "section": "R Packages",
    "text": "R Packages\n\nlibrary(tidyverse)\nlibrary(taylor)\nlibrary(tidytext)\nlibrary(stm)"
  },
  {
    "objectID": "lectures/15b.html#data-cleaning",
    "href": "lectures/15b.html#data-cleaning",
    "title": "Unsupervised\nMachine Learning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\ntidy_taylor &lt;-\n  taylor_album_songs |&gt;\n  unnest(lyrics) |&gt; \n  unnest_tokens(word, lyric)\n\n\ntidy_taylor |&gt; \n  anti_join(get_stopwords()) |&gt; \n  count(track_name, word, sort = TRUE) |&gt; \n  head(4)\n\n#&gt; # A tibble: 4 × 3\n#&gt;   track_name                          word      n\n#&gt;   &lt;chr&gt;                               &lt;chr&gt; &lt;int&gt;\n#&gt; 1 Red (Taylor's Version)              red     107\n#&gt; 2 I Did Something Bad                 di       81\n#&gt; 3 I Wish You Would (Taylor's Version) wish     81\n#&gt; 4 Shake It Off (Taylor's Version)     shake    70\n\nlyrics_sparse &lt;-\n  tidy_taylor |&gt; \n  count(track_name, word) |&gt; \n  filter(n &gt; 3) |&gt; \n  cast_sparse(track_name, word, n)"
  },
  {
    "objectID": "lectures/15b.html#topic-modelling-1",
    "href": "lectures/15b.html#topic-modelling-1",
    "title": "Unsupervised\nMachine Learning",
    "section": "Topic Modelling",
    "text": "Topic Modelling\n\nset.seed(123)\ntopic_model &lt;- stm(lyrics_sparse, K = 8, verbose = FALSE)"
  },
  {
    "objectID": "lectures/15b.html#summary",
    "href": "lectures/15b.html#summary",
    "title": "Unsupervised\nMachine Learning",
    "section": "Summary",
    "text": "Summary\n\nsummary(topic_model)\n\n#&gt; A topic model with 8 topics, 209 documents and a 909 word dictionary.\n\n\n#&gt; Topic 1 Top Words:\n#&gt;       Highest Prob: i, was, you, the, it, like, and \n#&gt;       FREX: red, isn't, snow, beach, him, was, too \n#&gt;       Lift: between, hair, prayer, rare, sacred, stairs, wind \n#&gt;       Score: red, snow, beach, him, was, isn't, there \n#&gt; Topic 2 Top Words:\n#&gt;       Highest Prob: i, you, the, and, me, wanna, what \n#&gt;       FREX: shake, wanna, wish, would, mm, bye, game \n#&gt;       Lift: team, stephen, hide, fancy, tear, game, bye \n#&gt;       Score: shake, wanna, wish, mm, off, fake, hung \n#&gt; Topic 3 Top Words:\n#&gt;       Highest Prob: you, i, and, the, me, to, my \n#&gt;       FREX: fly, left, losing, jump, go, someday, belong \n#&gt;       Lift: shoulda, okay, ours, superstar, slope, lately, start \n#&gt;       Score: la, times, fly, mean, bet, losing, smile \n#&gt; Topic 4 Top Words:\n#&gt;       Highest Prob: the, i, we, in, you, and, of \n#&gt;       FREX: woods, clear, huh, mine, car, getaway, walk \n#&gt;       Lift: ready, shimmer, walk, checkin, mailbox, ridin, huh \n#&gt;       Score: clear, woods, yet, daylight, out, walk, street \n#&gt; Topic 5 Top Words:\n#&gt;       Highest Prob: oh, you, and, the, this, i, is \n#&gt;       FREX: trouble, oh, rains, this, grow, asking, last \n#&gt;       Lift: promises, sing, these, lovin, rest, usin, flew \n#&gt;       Score: oh, last, trouble, asking, grow, rains, being \n#&gt; Topic 6 Top Words:\n#&gt;       Highest Prob: you, the, ooh, i, and, ah, to \n#&gt;       FREX: ha, starlight, ah, ooh, twenty, thing, whoa \n#&gt;       Lift: bought, count, keeping, everyone's, humming, kitchen, push \n#&gt;       Score: ooh, ha, ah, dorothea, starlight, twenty, you'll \n#&gt; Topic 7 Top Words:\n#&gt;       Highest Prob: you, it, a, i, and, the, we \n#&gt;       FREX: di, karma, blood, beautiful, wonderland, call, we've \n#&gt;       Lift: deep, worship, sad, turns, felt, why's, boyfriend \n#&gt;       Score: di, blood, karma, call, we've, hey, da \n#&gt; Topic 8 Top Words:\n#&gt;       Highest Prob: you, i, to, the, me, been, and \n#&gt;       FREX: york, welcome, mr, been, stay, i've, would've \n#&gt;       Lift: guiding, caught, both, quite, beat, bright, closure \n#&gt;       Score: york, welcome, stay, mr, would've, new, soundtrack"
  },
  {
    "objectID": "lectures/15b.html#plot",
    "href": "lectures/15b.html#plot",
    "title": "Unsupervised\nMachine Learning",
    "section": "Plot",
    "text": "Plot\n\n\nCode\nlyrics_gamma &lt;- tidy(\n  topic_model, \n  matrix = \"gamma\",\n  document_names = rownames(lyrics_sparse)\n) \n\nlyrics_gamma |&gt; \n  left_join(\n    taylor_album_songs |&gt; \n      select(album_name, document = track_name) |&gt; \n      mutate(album_name = fct_inorder(album_name))\n  ) |&gt; \n  mutate(topic = factor(topic)) |&gt; \n  ggplot(aes(gamma, topic, fill = topic)) +\n  geom_boxplot(alpha = 0.7, show.legend = FALSE) +\n  facet_wrap(vars(album_name)) +\n  labs(x = expression(gamma))"
  },
  {
    "objectID": "lectures/15b.html#significant-effects",
    "href": "lectures/15b.html#significant-effects",
    "title": "Unsupervised\nMachine Learning",
    "section": "Significant Effects",
    "text": "Significant Effects\n\n\nCode\nset.seed(909)\n\neffects &lt;-\n  estimateEffect(\n    1:8 ~ album_name,\n    topic_model,\n    taylor_album_songs |&gt; distinct(track_name, album_name) |&gt; arrange(track_name)\n  )\n\n\ntidy(effects) |&gt;  \n  filter(term != \"(Intercept)\", p.value &lt; 0.1) |&gt; \n  select(topic, term, p.value)\n\n\n#&gt; # A tibble: 8 × 3\n#&gt;   topic term                                   p.value\n#&gt;   &lt;int&gt; &lt;chr&gt;                                    &lt;dbl&gt;\n#&gt; 1     1 album_nameMidnights                    0.0313 \n#&gt; 2     2 album_nameMidnights                    0.0781 \n#&gt; 3     3 album_nameFearless (Taylor's Version)  0.0205 \n#&gt; 4     3 album_namefolklore                     0.00472\n#&gt; 5     3 album_nameSpeak Now (Taylor's Version) 0.0242 \n#&gt; 6     3 album_nameTaylor Swift                 0.0289 \n#&gt; 7     7 album_nameFearless (Taylor's Version)  0.0475 \n#&gt; 8     7 album_nameSpeak Now (Taylor's Version) 0.0441"
  },
  {
    "objectID": "lectures/15b.html#topic-3",
    "href": "lectures/15b.html#topic-3",
    "title": "Unsupervised\nMachine Learning",
    "section": "Topic 3",
    "text": "Topic 3\n\ntidy(topic_model, matrix = \"lift\") |&gt; \n  filter(topic == 3)\n\n#&gt; # A tibble: 909 × 2\n#&gt;    topic term     \n#&gt;    &lt;int&gt; &lt;chr&gt;    \n#&gt;  1     3 shoulda  \n#&gt;  2     3 okay     \n#&gt;  3     3 ours     \n#&gt;  4     3 superstar\n#&gt;  5     3 slope    \n#&gt;  6     3 lately   \n#&gt;  7     3 start    \n#&gt;  8     3 under    \n#&gt;  9     3 peace    \n#&gt; 10     3 lover    \n#&gt; # ℹ 899 more rows"
  },
  {
    "objectID": "lectures/15b.html#the-missing-statistics-sememster",
    "href": "lectures/15b.html#the-missing-statistics-sememster",
    "title": "Unsupervised\nMachine Learning",
    "section": "The Missing Statistics Sememster",
    "text": "The Missing Statistics Sememster\nHere is a list of resources to expand on topics not covered in your education.\nAdapted from https://missing.csail.mit.edu/"
  },
  {
    "objectID": "lectures/15b.html#introduction-to-statistics",
    "href": "lectures/15b.html#introduction-to-statistics",
    "title": "Unsupervised\nMachine Learning",
    "section": "Introduction to Statistics",
    "text": "Introduction to Statistics\n\nTraditional Statistics\nModern Basic Statistics\nStatistical Modeling\nStatistical Thinking\nStats for People Who Hate Stats"
  },
  {
    "objectID": "lectures/15b.html#statistical-computing",
    "href": "lectures/15b.html#statistical-computing",
    "title": "Unsupervised\nMachine Learning",
    "section": "Statistical Computing",
    "text": "Statistical Computing\n\nComputational Statistics (2009, Springer; Download from CSUCI Library)\nBasic Elements of Computational Statistics (2017, Springer; Download from CSUCI)\nOptimization (2013, Springer; Download from CSUCI)"
  },
  {
    "objectID": "lectures/15b.html#regression",
    "href": "lectures/15b.html#regression",
    "title": "Unsupervised\nMachine Learning",
    "section": "Regression",
    "text": "Regression\n\nBeyond Linear Regression\nRegression Modelling Strategies (Download from CSUCI Library)\nLinear Models\nGeneralized Linear Models With Examples in R (Download from CSUCI Library)\nLinear and Generalized Linear Mixed Models and Their Applications (2nd Edition) (Download from CSUCI Library)\nVector Generalized Linear and Additive Models; Yee (Download from CSUCI Library)"
  },
  {
    "objectID": "lectures/15b.html#other-statistics-resources",
    "href": "lectures/15b.html#other-statistics-resources",
    "title": "Unsupervised\nMachine Learning",
    "section": "Other Statistics Resources",
    "text": "Other Statistics Resources\n\nStatLect\nCausal Inference\nLinear Algebra"
  },
  {
    "objectID": "lectures/15b.html#r-programming",
    "href": "lectures/15b.html#r-programming",
    "title": "Unsupervised\nMachine Learning",
    "section": "R Programming",
    "text": "R Programming\n\nBasic R\nBasic R\nAdvanced R\nEfficient R\nR Bootcamp\nDeep R\nRcpp\nRcpp 4 Everyone\nRcpp Armadillo"
  },
  {
    "objectID": "lectures/15b.html#python-programming",
    "href": "lectures/15b.html#python-programming",
    "title": "Unsupervised\nMachine Learning",
    "section": "Python Programming",
    "text": "Python Programming\n\nBasic Python\nAnaconda\nLearn Python\nPython Data Science\nReticulate"
  },
  {
    "objectID": "lectures/15b.html#sql",
    "href": "lectures/15b.html#sql",
    "title": "Unsupervised\nMachine Learning",
    "section": "SQL",
    "text": "SQL\n\nSQL for Data Science\nKhan Academy\nSQLBolt\nSQLZoo"
  },
  {
    "objectID": "lectures/15b.html#shell-terminal",
    "href": "lectures/15b.html#shell-terminal",
    "title": "Unsupervised\nMachine Learning",
    "section": "Shell-Terminal",
    "text": "Shell-Terminal\n\nMissing Semester\nShell\nExplain Shell\nVim Adventures\nNeovim\ntmux\nHPCC Manuals"
  },
  {
    "objectID": "lectures/15b.html#git",
    "href": "lectures/15b.html#git",
    "title": "Unsupervised\nMachine Learning",
    "section": "Git",
    "text": "Git\n\nMissing Semester\nHappy Git\n\nPro Git\n\nOh S***, Git!?!\n\nGit in Simple Words"
  },
  {
    "objectID": "lectures/15b.html#markdown",
    "href": "lectures/15b.html#markdown",
    "title": "Unsupervised\nMachine Learning",
    "section": "Markdown",
    "text": "Markdown\n\nR Mardown\nQuarto\nLaTeX\nLaTeX for Beginners\nTypst"
  },
  {
    "objectID": "lectures/15b.html#dashboards",
    "href": "lectures/15b.html#dashboards",
    "title": "Unsupervised\nMachine Learning",
    "section": "Dashboards",
    "text": "Dashboards\n\nShiny\nQuarto Dashboards\nTableau\nPower BI"
  },
  {
    "objectID": "lectures/15b.html#other-programming",
    "href": "lectures/15b.html#other-programming",
    "title": "Unsupervised\nMachine Learning",
    "section": "Other Programming",
    "text": "Other Programming\n\nCpp\nCpp Armadillo\nJulia Data Science\nArcGIS\nSAS\nStata\nSPSS\nVBA\nJMP"
  },
  {
    "objectID": "lectures/4.html#r-packages",
    "href": "lectures/4.html#r-packages",
    "title": "Monte Carlo Methods",
    "section": "R Packages",
    "text": "R Packages\n\n\nCode\nlibrary(tidyverse)"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-integration-1",
    "href": "lectures/4.html#monte-carlo-integration-1",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Integration",
    "text": "Monte Carlo Integration\nMonte Carlo Integration is a numerical technique to compute a numerical of an integral.\nIt relies on simulating from a know distribution to obtain the expected value of a desired function."
  },
  {
    "objectID": "lectures/4.html#integration",
    "href": "lectures/4.html#integration",
    "title": "Monte Carlo Methods",
    "section": "Integration",
    "text": "Integration\nIntegration is commonly used to find the area under a curve."
  },
  {
    "objectID": "lectures/4.html#expectation",
    "href": "lectures/4.html#expectation",
    "title": "Monte Carlo Methods",
    "section": "Expectation",
    "text": "Expectation\nLet \\(X\\) be a continuous random variable:\n\\[\nE(X) = \\int_{X}xf(x)dx\n\\]\n\\[\nE\\{g(X)\\} = \\int_Xg(x)f(x)dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#strong-law-of-large-numbers",
    "href": "lectures/4.html#strong-law-of-large-numbers",
    "title": "Monte Carlo Methods",
    "section": "Strong Law of Large Numbers",
    "text": "Strong Law of Large Numbers\nAs \\(n\\rightarrow \\infty\\) (ie simulate a large number of random variables):\n\\[\n\\bar X_n \\rightarrow E_f(X)\n\\]\nwhere\n\\[\n\\bar X_n \\rightarrow = \\frac{1}{n}\\sum^n_{i=1}X_i\n\\]"
  },
  {
    "objectID": "lectures/4.html#strong-law-of-large-numbers-1",
    "href": "lectures/4.html#strong-law-of-large-numbers-1",
    "title": "Monte Carlo Methods",
    "section": "Strong Law of Large Numbers",
    "text": "Strong Law of Large Numbers\n\\[\n\\bar X_n^{(g)} \\rightarrow E_f\\{g(X)\\}\n\\]\nwhere\n\\[\n\\bar X_n^{(g)} \\rightarrow = \\frac{1}{n}\\sum^n_{i=1}g(X_i)\n\\]"
  },
  {
    "objectID": "lectures/4.html#the-expected-value-of-a-normal-distribution",
    "href": "lectures/4.html#the-expected-value-of-a-normal-distribution",
    "title": "Monte Carlo Methods",
    "section": "The Expected Value of a Normal Distribution",
    "text": "The Expected Value of a Normal Distribution\n\\[\nE(X) = \\int^{\\infty}_{-\\infty}\\frac{x}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{(x-\\mu)^2}{\\sigma^2}\\right\\} dx = \\mu\n\\]"
  },
  {
    "objectID": "lectures/4.html#variance-of-a-normal-distribution",
    "href": "lectures/4.html#variance-of-a-normal-distribution",
    "title": "Monte Carlo Methods",
    "section": "Variance of a Normal Distribution",
    "text": "Variance of a Normal Distribution\n\\[\nVar(X) = E[\\{X-E(X)\\}^2] \\\\= \\int^{\\infty}_{-\\infty}\\frac{\\{x-E(X)\\}^2}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{(x-\\mu)^2}{\\sigma^2}\\right\\} dx = \\sigma^2\n\\]"
  },
  {
    "objectID": "lectures/4.html#using-monte-carlo-integration-to-obtain-expectations",
    "href": "lectures/4.html#using-monte-carlo-integration-to-obtain-expectations",
    "title": "Monte Carlo Methods",
    "section": "Using Monte Carlo Integration to obtain expectations",
    "text": "Using Monte Carlo Integration to obtain expectations\n\nSimulate from a target distribution \\(f\\)\nCalculate the mean for the expected value"
  },
  {
    "objectID": "lectures/4.html#using-monte-carlo-integration",
    "href": "lectures/4.html#using-monte-carlo-integration",
    "title": "Monte Carlo Methods",
    "section": "Using Monte Carlo Integration",
    "text": "Using Monte Carlo Integration\n\\[\nX \\sim N(\\mu, \\sigma^2)\n\\]\n\n\nCode\nx &lt;- rnorm(100000, mean = -2, sd = 3)\nmean(x)\n\n\n#&gt; [1] -2.001199\n\n\nCode\nvar(x)\n\n\n#&gt; [1] 9.030126"
  },
  {
    "objectID": "lectures/4.html#gamma-distrbution",
    "href": "lectures/4.html#gamma-distrbution",
    "title": "Monte Carlo Methods",
    "section": "Gamma Distrbution",
    "text": "Gamma Distrbution\n\\[\nX \\sim Gamma(3,4)\n\\]"
  },
  {
    "objectID": "lectures/4.html#beta-distribution",
    "href": "lectures/4.html#beta-distribution",
    "title": "Monte Carlo Methods",
    "section": "Beta Distribution",
    "text": "Beta Distribution\n\\[\nX \\sim Beta(2,3)\n\\]"
  },
  {
    "objectID": "lectures/4.html#chi2p",
    "href": "lectures/4.html#chi2p",
    "title": "Monte Carlo Methods",
    "section": "\\(\\chi^2(p)\\)",
    "text": "\\(\\chi^2(p)\\)\n\\[\nX \\sim \\chi^2(39)\n\\]"
  },
  {
    "objectID": "lectures/4.html#finding-the-probability",
    "href": "lectures/4.html#finding-the-probability",
    "title": "Monte Carlo Methods",
    "section": "Finding the Probability",
    "text": "Finding the Probability\nIntegration is commonly used to determine the probability of observing a certain range of values for a continuous random variable.\n\\[\nP(a &lt; X &lt; b)\n\\]"
  },
  {
    "objectID": "lectures/4.html#graphical-setting",
    "href": "lectures/4.html#graphical-setting",
    "title": "Monte Carlo Methods",
    "section": "Graphical Setting",
    "text": "Graphical Setting\n\n\nCode\nx &lt;- seq(-4, 4, length.out = 1000)\ndt_two&lt;-function(x){\n            y &lt;- dnorm(x)\n            y[x&lt; -1 | x&gt;2] &lt;-NA\n            return(y)\n        }\nx |&gt; (\\(.) tibble(x = ., y = dnorm(.)))() |&gt; \n  ggplot(aes(x, y)) +\n    geom_line() +\n    stat_function(fun = dt_two, geom = \"area\", fill = \"green\") + \n    theme_bw()"
  },
  {
    "objectID": "lectures/4.html#finding-the-propbabilities-of-a-random-variable",
    "href": "lectures/4.html#finding-the-propbabilities-of-a-random-variable",
    "title": "Monte Carlo Methods",
    "section": "Finding the Propbabilities of a Random Variable",
    "text": "Finding the Propbabilities of a Random Variable\nFor a given random variable \\(X\\), finding the probability is the same as\n\\[\nE\\{I(a&lt;X&lt;b)\\} = \\int_X I(a&lt;X&lt;b) f(x) dx\n\\]\nwhere \\(I(a&lt;X&lt;b)\\) is the indicator function."
  },
  {
    "objectID": "lectures/4.html#indicator-function",
    "href": "lectures/4.html#indicator-function",
    "title": "Monte Carlo Methods",
    "section": "Indicator Function",
    "text": "Indicator Function\n\\[\nI(a&lt;X&lt;b) = \\left\\{\\begin{array}{cc}\n1 & a&lt;X&lt;b\\\\\n0 & \\mathrm{otherwise}\n\\end{array}\n\\right.\n\\]"
  },
  {
    "objectID": "lectures/4.html#finding-the-probability-1",
    "href": "lectures/4.html#finding-the-probability-1",
    "title": "Monte Carlo Methods",
    "section": "Finding the Probability",
    "text": "Finding the Probability\n\\[\n\\begin{align}\nE\\{I(a&lt;X&lt;b)\\} &  = \\int_X I(a&lt;X&lt;b) f(x) dx\\\\\n& = \\int_a^b f(x) dx\\\\\n& = P(a &lt; X &lt; b)\n\\end{align}\n\\]"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-probability",
    "href": "lectures/4.html#monte-carlo-probability",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Probability",
    "text": "Monte Carlo Probability\n\nSimulate from a target distribution \\(f\\)\nCalculate the mean for \\(I(a&lt;X&lt;b)\\)"
  },
  {
    "objectID": "lectures/4.html#normal-rv-example",
    "href": "lectures/4.html#normal-rv-example",
    "title": "Monte Carlo Methods",
    "section": "Normal RV Example",
    "text": "Normal RV Example\nLet \\(X\\sim N(4, 2)\\), find \\(P(3 &lt; X &lt; 6)\\)\n\n\nCode\npnorm(6, 4, sqrt(2)) -  pnorm(3, 4, sqrt(2))\n\n\n#&gt; [1] 0.6816003"
  },
  {
    "objectID": "lectures/4.html#using-monte-carlo-methods",
    "href": "lectures/4.html#using-monte-carlo-methods",
    "title": "Monte Carlo Methods",
    "section": "Using Monte Carlo Methods",
    "text": "Using Monte Carlo Methods\n\n\nCode\nx &lt;- rnorm(1000000, 4, sqrt(2))\nmean((x &gt; 3 & x &lt; 6))\n\n\n#&gt; [1] 0.68179"
  },
  {
    "objectID": "lectures/4.html#logistic-rv-example",
    "href": "lectures/4.html#logistic-rv-example",
    "title": "Monte Carlo Methods",
    "section": "Logistic RV Example",
    "text": "Logistic RV Example\nLet \\(X\\sim Logistic(3, 5)\\), find \\(P(-1 &lt; X &lt; 5)\\)"
  },
  {
    "objectID": "lectures/4.html#weibull-rv-example",
    "href": "lectures/4.html#weibull-rv-example",
    "title": "Monte Carlo Methods",
    "section": "Weibull RV Example",
    "text": "Weibull RV Example\nLet \\(X\\sim Weibull(1, 1)\\), find \\(P(2 &lt; X &lt; 5.5)\\)"
  },
  {
    "objectID": "lectures/4.html#f-rv-example",
    "href": "lectures/4.html#f-rv-example",
    "title": "Monte Carlo Methods",
    "section": "F RV Example",
    "text": "F RV Example\nLet \\(X\\sim F(2, 45)\\), find \\(P(1 &lt; X &lt; 3)\\)"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-integration-2",
    "href": "lectures/4.html#monte-carlo-integration-2",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Integration",
    "text": "Monte Carlo Integration\nMonte Carlo Integration can be used to evaluate finite-bounded integrals of the following form:\n\\[\n\\int^b_a g(x) dx\n\\] such that \\(-\\infty &lt;a,b&lt;\\infty\\)."
  },
  {
    "objectID": "lectures/4.html#monte-carlo-example-integration",
    "href": "lectures/4.html#monte-carlo-example-integration",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Example Integration",
    "text": "Monte Carlo Example Integration\n\\[\n\\int^1_{0} \\{\\cos(50x) - sin(20x)\\}^2dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-example-integration-1",
    "href": "lectures/4.html#monte-carlo-example-integration-1",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Example Integration",
    "text": "Monte Carlo Example Integration\nLet \\(X \\sim U(0,1)\\) with \\(f(x) = 1\\), then\n\\[\nE[\\{\\cos(50x) - sin(20x)\\}^2] =\\int^1_{0} \\{\\cos(50x) - sin(20x)\\}^2dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#using-numerical-integration",
    "href": "lectures/4.html#using-numerical-integration",
    "title": "Monte Carlo Methods",
    "section": "Using Numerical Integration",
    "text": "Using Numerical Integration\n\n\nCode\nff &lt;- function(x){\n  (cos(50*x)-sin(50*x))^2\n}\nintegrate(ff,0,1)\n\n\n#&gt; 0.9986232 with absolute error &lt; 9.5e-11"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-example-integration-2",
    "href": "lectures/4.html#monte-carlo-example-integration-2",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Example Integration",
    "text": "Monte Carlo Example Integration\n\n\nCode\nx &lt;- runif(10000000, 0, 1)\nmean((cos(50*x)-sin(50*x))^2)\n\n\n#&gt; [1] 0.9993568"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-example-integration-3",
    "href": "lectures/4.html#monte-carlo-example-integration-3",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Example Integration",
    "text": "Monte Carlo Example Integration\n\\[\n\\int^{15}_{10} \\{\\cos(50x) - sin(20x)\\}^2dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-integration-3",
    "href": "lectures/4.html#monte-carlo-integration-3",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Integration",
    "text": "Monte Carlo Integration\nLet \\(X \\sim U(10,15)\\) with \\(f(x) = 1\\), then\n\\[\nE[\\{\\cos(50x) - sin(20x)\\}^2] = \\\\\n\\int^{15}_{10} \\frac{1}{5} \\{\\cos(50x) - sin(20x)\\}^2dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-example-integration-4",
    "href": "lectures/4.html#monte-carlo-example-integration-4",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Example Integration",
    "text": "Monte Carlo Example Integration\n\\[\n\\int^{15}_{10} \\{\\cos(50x) - sin(20x)\\}^2dx = \\\\\n5 * E[\\{\\cos(50x) - sin(20x)\\}^2]\n\\]"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-example-integration-5",
    "href": "lectures/4.html#monte-carlo-example-integration-5",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Example Integration",
    "text": "Monte Carlo Example Integration\n\n\nCode\nff &lt;- function(x){\n  (cos(50*x)-sin(50*x))^2\n}\nintegrate(ff,10,15)\n\n\n#&gt; 4.993274 with absolute error &lt; 1.1e-06"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-example-integration-6",
    "href": "lectures/4.html#monte-carlo-example-integration-6",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Example Integration",
    "text": "Monte Carlo Example Integration\n\n\nCode\nx &lt;- runif(10000000, 10, 15)\nmean(ff(x)) * 5\n\n\n#&gt; [1] 4.994512"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-integration-algorithm",
    "href": "lectures/4.html#monte-carlo-integration-algorithm",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Integration Algorithm",
    "text": "Monte Carlo Integration Algorithm\nGiven: \\[\n\\int_a^b g(x) dx\n\\]\n\nSimulate \\(n\\) value from \\(X \\sim U(a,b)\\)\nTake the average, \\(\\frac{1}{n}\\sum^{n}_{i=1}g(x_i)\\)\nMultiply the average by \\(b-a\\): \\(\\frac{b-a}{n}\\sum^{n}_{i=1}g(x_i)\\)"
  },
  {
    "objectID": "lectures/4.html#mc-examples",
    "href": "lectures/4.html#mc-examples",
    "title": "Monte Carlo Methods",
    "section": "MC Examples",
    "text": "MC Examples\n\\[\n\\int_0^{2}  e^{-x^2/2} dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#importance-sampling-1",
    "href": "lectures/4.html#importance-sampling-1",
    "title": "Monte Carlo Methods",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nImportance sampling is an extension of Monte Carlo integration where it addresses the limitations of large variance of the expected value and the bounds required in integrals.\nThis is done by simulating from a random variable that has an infinite support system."
  },
  {
    "objectID": "lectures/4.html#importance-sampling-2",
    "href": "lectures/4.html#importance-sampling-2",
    "title": "Monte Carlo Methods",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nLet’s say we are interested in finding the numerical value of the following integral:\n\\[\n\\int_{-\\infty}^\\infty g(x) dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#importance-sampling-3",
    "href": "lectures/4.html#importance-sampling-3",
    "title": "Monte Carlo Methods",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nIf we view the integral as an expectation of an easily simulated random variable, we can compute the numerical value.\nLet \\(X\\) be a random variable \\(f\\), then\n\\[\n\\int_{-\\infty}^\\infty g(x) dx = \\int_{-\\infty}^\\infty \\frac{g(x)}{f(x)} f(x) dx = E\\left\\{\\frac{g(x)}{f(x)}\\right\\}\n\\]"
  },
  {
    "objectID": "lectures/4.html#importance-sampling-4",
    "href": "lectures/4.html#importance-sampling-4",
    "title": "Monte Carlo Methods",
    "section": "Importance Sampling",
    "text": "Importance Sampling\nSince the integral is the expectation of \\(X\\), it can be obtained by taking the mean of the simulated values applied to \\(g(x)/f(x)\\)."
  },
  {
    "objectID": "lectures/4.html#example",
    "href": "lectures/4.html#example",
    "title": "Monte Carlo Methods",
    "section": "Example",
    "text": "Example\n\\[\n\\int_{-\\infty}^{\\infty}  e^{-x^2/2} dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#example-1",
    "href": "lectures/4.html#example-1",
    "title": "Monte Carlo Methods",
    "section": "Example",
    "text": "Example\n\n\nCode\nx &lt;- rt(1000000, df = 1)\nf2 &lt;- function(x){\n  exp(-x^2/2) / dt(x, 1)\n}\nmean(f2(x))\n\n\n#&gt; [1] 2.505336\n\n\nCode\nsqrt(2*pi)\n\n\n#&gt; [1] 2.506628"
  },
  {
    "objectID": "lectures/4.html#choosing-fx",
    "href": "lectures/4.html#choosing-fx",
    "title": "Monte Carlo Methods",
    "section": "Choosing \\(f(x)\\)",
    "text": "Choosing \\(f(x)\\)\nChoose a value \\(f(x)\\) that follows a shape close enough to \\(g(x)\\) that has the same bounds as the integral."
  },
  {
    "objectID": "lectures/4.html#example-2",
    "href": "lectures/4.html#example-2",
    "title": "Monte Carlo Methods",
    "section": "Example",
    "text": "Example\n\\[\n\\int_{-\\infty}^{\\infty}  e^{-(x-4)^2/10} dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#example-3",
    "href": "lectures/4.html#example-3",
    "title": "Monte Carlo Methods",
    "section": "Example",
    "text": "Example\n\\[\n\\int_{0}^{\\infty} x^3 e^{x/2} dx\n\\]"
  },
  {
    "objectID": "lectures/4.html#optimization",
    "href": "lectures/4.html#optimization",
    "title": "Monte Carlo Methods",
    "section": "Optimization",
    "text": "Optimization\n\\[\nh(x) = -3x^4+4x^3\n\\]\n\n\nCode\nff &lt;- function(x){\n  -3*x^4+4*x^3\n}\nx &lt;- seq(-1, 2, length.out = 1000) \ny &lt;- ff(x)\ntibble(x, y) |&gt; \n  ggplot(aes(x, y)) +\n  geom_line() + theme_bw()"
  },
  {
    "objectID": "lectures/4.html#optimization-1",
    "href": "lectures/4.html#optimization-1",
    "title": "Monte Carlo Methods",
    "section": "Optimization",
    "text": "Optimization\nOptimization is the method to find the values of a variable that will maximize a function of interest (\\(h\\))."
  },
  {
    "objectID": "lectures/4.html#numerical-optimization-techniques",
    "href": "lectures/4.html#numerical-optimization-techniques",
    "title": "Monte Carlo Methods",
    "section": "Numerical Optimization Techniques",
    "text": "Numerical Optimization Techniques\n\n\nNewton-Raphson Method\nGradient Descent\nQuasi Newton-Raphson Method"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-optimization-1",
    "href": "lectures/4.html#monte-carlo-optimization-1",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Optimization",
    "text": "Monte Carlo Optimization\nMonte Carlo Optimization technique a brute force method that will simulate a high number of random values, evaluate them with \\(h(x)\\), and identify which value provides a the maximum value."
  },
  {
    "objectID": "lectures/4.html#optimization-2",
    "href": "lectures/4.html#optimization-2",
    "title": "Monte Carlo Methods",
    "section": "Optimization",
    "text": "Optimization\n\n\nCode\nff &lt;- function(x){\n  y &lt;- -3*x^4+4*x^3\n  return(y)\n}\nx &lt;- seq(-1, 2, length.out = 1000) \ny &lt;- ff(x)\ntibble(x, y) |&gt; \n  ggplot(aes(x, y)) +\n  geom_line() + theme_bw()"
  },
  {
    "objectID": "lectures/4.html#numerical-optimization",
    "href": "lectures/4.html#numerical-optimization",
    "title": "Monte Carlo Methods",
    "section": "Numerical Optimization",
    "text": "Numerical Optimization\n\n\nCode\nff &lt;- function(x){\n  y &lt;- -3*x^4+4*x^3\n  return(-y)\n}\n\noptimize(ff, c(-5,5))\n\n\n#&gt; $minimum\n#&gt; [1] 1\n#&gt; \n#&gt; $objective\n#&gt; [1] -1"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-methods",
    "href": "lectures/4.html#monte-carlo-methods",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods\n\n\nCode\nx &lt;- runif(1000000, -5,5)\ny &lt;- ff(x)\nwhich.min(y)\n\n\n#&gt; [1] 361836\n\n\nCode\nx[which.min(y)]\n\n\n#&gt; [1] 0.9999983"
  },
  {
    "objectID": "lectures/4.html#finding-the-maximum-the-gaussian-function.",
    "href": "lectures/4.html#finding-the-maximum-the-gaussian-function.",
    "title": "Monte Carlo Methods",
    "section": "Finding the maximum the Gaussian function.",
    "text": "Finding the maximum the Gaussian function.\n\\[\nf(x) =  e^{-(x-4)^2/10}\n\\]"
  },
  {
    "objectID": "lectures/4.html#minimizing-sse",
    "href": "lectures/4.html#minimizing-sse",
    "title": "Monte Carlo Methods",
    "section": "Minimizing SSE",
    "text": "Minimizing SSE\nGiven a data set \\((X_i, Y_i)\\) for \\(i = 1, \\ldots, n\\):\n\n\nCode\nx &lt;- rnorm(1000)\ny &lt;- 8 -4 * x + rnorm(1000, sd = 0.5)"
  },
  {
    "objectID": "lectures/4.html#minimizing-sse-1",
    "href": "lectures/4.html#minimizing-sse-1",
    "title": "Monte Carlo Methods",
    "section": "Minimizing SSE",
    "text": "Minimizing SSE\nFind the values \\(\\beta_0\\) and \\(\\beta_1\\) that minimizes the following formula:\n\\[\nSSE =  \\sum^n_{i=1}\\{Y_i - (\\beta_0 + \\beta_1 X_i)\\}^2\n\\]"
  },
  {
    "objectID": "lectures/4.html#minimizing-sse-2",
    "href": "lectures/4.html#minimizing-sse-2",
    "title": "Monte Carlo Methods",
    "section": "Minimizing SSE",
    "text": "Minimizing SSE\n\n\nCode\nsse &lt;- function(beta, x, y){\n  sum((y - (beta[1] + beta[2] * x))^2)\n}\nbeta0 &lt;- seq(-20, 20, length.out = 50)\nbeta1 &lt;- seq(-20, 20, length.out = 50)\nzz &lt;- matrix(nrow = 50, ncol = 50)\nfor (i in seq_along(beta0)){\n  for(ii in seq_along(beta1)){\n    zz[i, ii] &lt;- sse(c(beta0[i], beta1[ii]), x, y) \n  }\n}\npersp(beta0, beta1, zz)"
  },
  {
    "objectID": "lectures/4.html#minimizing-sse-3",
    "href": "lectures/4.html#minimizing-sse-3",
    "title": "Monte Carlo Methods",
    "section": "Minimizing SSE",
    "text": "Minimizing SSE\n\n\nCode\nsse &lt;- function(beta, x, y){\n  sum((y - (beta[1] + beta[2] * x))^2)\n}\n\nuu &lt;- sapply(1:1000000, \\(x) runif(2, -10, 10))\nval &lt;- apply(uu, 2, sse, x = x, y = y)\nwhich.min(val)\n\n\n#&gt; [1] 629279\n\n\nCode\nuu[,which.min(val)]\n\n\n#&gt; [1]  8.016216 -4.003703"
  },
  {
    "objectID": "lectures/4.html#toy-collectors-problem-1",
    "href": "lectures/4.html#toy-collectors-problem-1",
    "title": "Monte Carlo Methods",
    "section": "Toy Collector’s Problem",
    "text": "Toy Collector’s Problem\nThere is a cereal company that is planning to give away 15 toys in their cereal boxes where the the probability of getting a certain toy from a box is 1/15. Assuming that you only get one toy from each cereal box, what is the number of cereal boxes you will need to buy to obtain all 15 toys?"
  },
  {
    "objectID": "lectures/4.html#geometric-distribution",
    "href": "lectures/4.html#geometric-distribution",
    "title": "Monte Carlo Methods",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\nThe geometric distribution is used to determine the number of failures needed to get the first success.\nA success is getting a new toy."
  },
  {
    "objectID": "lectures/4.html#collecting-toys",
    "href": "lectures/4.html#collecting-toys",
    "title": "Monte Carlo Methods",
    "section": "Collecting Toys",
    "text": "Collecting Toys\nThe probability of getting a new toy given we have i toys goes as follows: \\[P(New Toy|i)= \\frac{15-i}{15}.\\]\nIf we have 0 toys \\((i=0)\\) the probability of getting a new toy after opening 1 box is 1 \\[P(New Toy|0)=\\frac{15-0}{15}=1.\\]"
  },
  {
    "objectID": "lectures/4.html#getting-8th-toy",
    "href": "lectures/4.html#getting-8th-toy",
    "title": "Monte Carlo Methods",
    "section": "Getting 8th Toy",
    "text": "Getting 8th Toy\nNow if we have 7 toys, the probability of getting a new toy goes as follows: \\[P(New Toy|7)=\\frac{15-7}{15}=\\frac{8}{15}=0.533.\\]\nTo get a new toy, we would have to open at least \\(\\frac{1}{0.533}=1.875\\) boxes."
  },
  {
    "objectID": "lectures/4.html#getting-15th-toy",
    "href": "lectures/4.html#getting-15th-toy",
    "title": "Monte Carlo Methods",
    "section": "Getting 15th Toy",
    "text": "Getting 15th Toy\nNow if we had 14 toys already, we would need to open at least 15 boxes to get that.\n\\[P(New Toy|14)=\\frac{15-14}{15}=\\frac{1}{15}=0.067\\] So the number of boxes to open to get the last toy is \\(\\frac{1}{0.067}=\\frac{15}{1}=15\\)"
  },
  {
    "objectID": "lectures/4.html#average-number-of-boxes",
    "href": "lectures/4.html#average-number-of-boxes",
    "title": "Monte Carlo Methods",
    "section": "Average number of boxes",
    "text": "Average number of boxes\n\\[\\frac{15}{15}+\\frac{15}{14}+\\frac{15}{13}+...+\\frac{15}{2}+\\frac{15}{1} \\approx 49.77.\\]"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-methods-1",
    "href": "lectures/4.html#monte-carlo-methods-1",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods\nMonte Carlo can be used to estimate the expected number of boxes you will need to buy to obtain all 15 toys.\nThink about having \\(n\\) shoppers going out an buying as many boxes needed to buy to obtain all 15 toys. Then you compute the mean and standard error for your sample."
  },
  {
    "objectID": "lectures/4.html#monte-carlo-methods-2",
    "href": "lectures/4.html#monte-carlo-methods-2",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods\n\n\nCode\nmc_shopper&lt;-function(n, x, p){\n  k &lt;- vector(mode = \"numeric\", length = n)\n  for (i in 1:n){\n    j &lt;- 1\n    a &lt;- sample(x, 1, prob = p)\n    while(length(unique(a)) &lt; 15){\n      b &lt;- sample(x, 1, prob = p)\n      a &lt;- c(a, b)\n      j &lt;- j + 1\n    }\n    k[i] &lt;- j\n  }\n  m &lt;- mean(k)\n  se &lt;- sd(k)/sqrt(n)\n  ci &lt;- m + c(-1, 1) * 1.96 * se\n  return(list(mean = m, se = se, ci_95 = ci))\n}"
  },
  {
    "objectID": "lectures/4.html#monte-carlo-methods-3",
    "href": "lectures/4.html#monte-carlo-methods-3",
    "title": "Monte Carlo Methods",
    "section": "Monte Carlo Methods",
    "text": "Monte Carlo Methods\n\n\nCode\nx &lt;- 1:15\np &lt;- rep(1/15, times = 15)\nmc_shopper(10000, x, p)\n\n\n#&gt; $mean\n#&gt; [1] 49.9947\n#&gt; \n#&gt; $se\n#&gt; [1] 0.173234\n#&gt; \n#&gt; $ci_95\n#&gt; [1] 49.65516 50.33424"
  },
  {
    "objectID": "lectures/4.html#toy-collector-problem-2",
    "href": "lectures/4.html#toy-collector-problem-2",
    "title": "Monte Carlo Methods",
    "section": "Toy Collector Problem 2",
    "text": "Toy Collector Problem 2\nThere is a cereal company that is planning to give away 15 toys in their cereal boxes. The probability for each toy is no longer equal, the probability for each toy goes as follows:\n\n\n\nFigure\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nProbability\n.2\n.1\n.1\n.1\n.1\n.1\n.05\n.05\n\n\n\n\n\n\nFigure\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\nProbability\n.05\n.05\n.03\n.02\n.02\n.02\n.01\n\n\n\nAssuming that you only get one toy from each cereal box, what is the number of cereal boxes you will need to get to obtain all 15 toys?"
  },
  {
    "objectID": "lectures/8.html#r-packages",
    "href": "lectures/8.html#r-packages",
    "title": "Simulation Studies",
    "section": "R Packages",
    "text": "R Packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(Hmisc)\nlibrary(rms)\nlibrary(gam)\ntheme_set(theme_bw())"
  },
  {
    "objectID": "lectures/8.html#exponential-family-of-distributions",
    "href": "lectures/8.html#exponential-family-of-distributions",
    "title": "Simulation Studies",
    "section": "Exponential Family of Distributions",
    "text": "Exponential Family of Distributions\nAn exponential family of distributions are random variables that allow their probability density function to have the following form:\n\\[\nf(y; \\theta,\\phi) = a(y,\\phi)\\exp\\left\\{\\frac{y\\theta-\\kappa(\\theta)}{\\phi}\\right\\}\n\\]\n\n\\(\\theta\\): is the canonical parameter (also a function of other parameters)\n\\(\\kappa(\\theta)\\): is a known cumulant function\n\\(\\phi&gt;0\\): dispersion parameter function\n\\(a(y,\\phi)\\): normalizing constant"
  },
  {
    "objectID": "lectures/8.html#canonical-parameter",
    "href": "lectures/8.html#canonical-parameter",
    "title": "Simulation Studies",
    "section": "Canonical Parameter",
    "text": "Canonical Parameter\nThe canonical parameter represents the relationship between the random variable and the \\(E(Y)=\\mu\\)"
  },
  {
    "objectID": "lectures/8.html#normal-distribution",
    "href": "lectures/8.html#normal-distribution",
    "title": "Simulation Studies",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\\[\nf(y;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\n\\]"
  },
  {
    "objectID": "lectures/8.html#binomial-distribution",
    "href": "lectures/8.html#binomial-distribution",
    "title": "Simulation Studies",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\\[\nf(y;n,p)=\\big(^n_y\\big) p^y(1-p)^{n-y}\n\\]"
  },
  {
    "objectID": "lectures/8.html#poisson-distribution",
    "href": "lectures/8.html#poisson-distribution",
    "title": "Simulation Studies",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\\[\nf(y;\\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!}\n\\]"
  },
  {
    "objectID": "lectures/8.html#negative-binomial-distribution",
    "href": "lectures/8.html#negative-binomial-distribution",
    "title": "Simulation Studies",
    "section": "Negative Binomial Distribution",
    "text": "Negative Binomial Distribution\n\\[\nf(y;\\mu, \\theta) = \\left(\\begin{array}{c}\ny+\\theta+1\\\\\ny\n\\end{array}\\right)\n\\left(\\frac{\\mu}{\\mu+\\theta}\\right)^y\n\\left(\\frac{\\theta}{\\mu+\\theta}\\right)^\\theta\n\\]\n\n\\(\\mu\\): average count\n\\(\\theta\\): dispersion of data\n\\(E(Y)=\\mu\\)\n\\(Var(Y) = \\mu + \\mu^2/\\theta\\)"
  },
  {
    "objectID": "lectures/8.html#gamma-distribution",
    "href": "lectures/8.html#gamma-distribution",
    "title": "Simulation Studies",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution"
  },
  {
    "objectID": "lectures/8.html#common-distributions-and-canonical-parameters",
    "href": "lectures/8.html#common-distributions-and-canonical-parameters",
    "title": "Simulation Studies",
    "section": "Common Distributions and Canonical Parameters",
    "text": "Common Distributions and Canonical Parameters\n\n\n\nRandom Variable\nCanonical Parameter\n\n\n\n\nNormal\n\\(\\mu\\)\n\n\nBinomial\n\\(\\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\)\n\n\nNegative Binomial\n\\(\\log\\left(\\frac{\\mu}{\\mu+k}\\right)\\)\n\n\nPoisson\n\\(\\log(\\mu)\\)\n\n\nGamma\n\\(-\\frac{1}{\\mu}\\)\n\n\nInverse Gaussian\n\\(-\\frac{1}{2\\mu^2}\\)"
  },
  {
    "objectID": "lectures/8.html#generalized-linear-models-1",
    "href": "lectures/8.html#generalized-linear-models-1",
    "title": "Simulation Studies",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA generalized linear model (GLM) is used to model the association between an outcome variable (of any data type) and a set of predictor values. We estimate a set of regression coefficients \\(\\boldsymbol \\beta\\) to explain how each predictor is related to the expected value of the outcome."
  },
  {
    "objectID": "lectures/8.html#generalized-linear-models-2",
    "href": "lectures/8.html#generalized-linear-models-2",
    "title": "Simulation Studies",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\nA GLM is composed of a systematic and random component."
  },
  {
    "objectID": "lectures/8.html#random-component",
    "href": "lectures/8.html#random-component",
    "title": "Simulation Studies",
    "section": "Random Component",
    "text": "Random Component\nThe random component is the random variable that defines the randomness and variation of the outcome variable."
  },
  {
    "objectID": "lectures/8.html#systematic-component",
    "href": "lectures/8.html#systematic-component",
    "title": "Simulation Studies",
    "section": "Systematic Component",
    "text": "Systematic Component\nThe systematic component is the linear model that models the association between a set of predictors and the expected value of Y:\n\\[\ng(\\mu)=\\eta=\\boldsymbol X_i^\\mathrm T \\boldsymbol \\beta\n\\]\n\n\\(\\boldsymbol\\beta\\): regression coefficients\n\\(\\boldsymbol X_i=(1, X_{i1}, \\ldots, X_{ip})^\\mathrm T\\): design vector\n\\(\\eta\\): linear model\n\\(\\mu=E(Y)\\)\n\\(g(\\cdot)\\): link function"
  },
  {
    "objectID": "lectures/8.html#simulating-bernoulli",
    "href": "lectures/8.html#simulating-bernoulli",
    "title": "Simulation Studies",
    "section": "Simulating Bernoulli",
    "text": "Simulating Bernoulli\n\n\nCode\nx &lt;- rnorm(1000)\neta &lt;- boot::inv.logit(-0.85 + 1.3 * x)\ny &lt;- rbinom(1000, size = 1, prob = eta)"
  },
  {
    "objectID": "lectures/8.html#simulating-poisson-rv",
    "href": "lectures/8.html#simulating-poisson-rv",
    "title": "Simulation Studies",
    "section": "Simulating Poisson RV",
    "text": "Simulating Poisson RV\n\n\nCode\nx &lt;- rnorm(1000)\neta &lt;- exp(-0.85 + 1.3 * x)\ny &lt;- rpois(1000, lambda = eta)"
  },
  {
    "objectID": "lectures/8.html#simulating-negative-binomial-rv",
    "href": "lectures/8.html#simulating-negative-binomial-rv",
    "title": "Simulation Studies",
    "section": "Simulating Negative Binomial RV",
    "text": "Simulating Negative Binomial RV\n\n\nCode\nx &lt;- rnorm(1000)\neta &lt;- exp(-0.85 + 1.3 * x)\ny &lt;- rnbinom(1000, mu = eta, size = 0.5)"
  },
  {
    "objectID": "lectures/8.html#simulating-a-gamma-rv",
    "href": "lectures/8.html#simulating-a-gamma-rv",
    "title": "Simulation Studies",
    "section": "Simulating a Gamma RV",
    "text": "Simulating a Gamma RV\n\n\nCode\nx &lt;- rnorm(1000)\ny_true &lt;- exp(0.75 + 1.3 * x)\ny &lt;- rgamma(1000, rate = 10 / y_true, shape = 10)"
  },
  {
    "objectID": "lectures/8.html#bayesian-regression-model-in-r",
    "href": "lectures/8.html#bayesian-regression-model-in-r",
    "title": "Simulation Studies",
    "section": "Bayesian Regression Model in R",
    "text": "Bayesian Regression Model in R\n\n\nCode\nlibrary(brms)\n1brm(formula,\n2    data,\n3    family)\n\n\n\n1\n\nSupply a formula for R\n\n2\n\nSupply the data frame\n\n3\n\nWhich family and link function is used to model data"
  },
  {
    "objectID": "lectures/8.html#logistic-binomial-regression",
    "href": "lectures/8.html#logistic-binomial-regression",
    "title": "Simulation Studies",
    "section": "Logistic (Binomial) Regression",
    "text": "Logistic (Binomial) Regression\nLogistic Regression is used when your outcome is binary:\n\n\nCode\nbrm(y~x, \n    data, \n    family = bernoulli())"
  },
  {
    "objectID": "lectures/8.html#poisson-regression",
    "href": "lectures/8.html#poisson-regression",
    "title": "Simulation Studies",
    "section": "Poisson Regression",
    "text": "Poisson Regression\nPoisson Regression is used when the outcome is count data:\n\n\nCode\nbrm(y~x, \n    data, \n    family = poisson())"
  },
  {
    "objectID": "lectures/8.html#gamma-regression",
    "href": "lectures/8.html#gamma-regression",
    "title": "Simulation Studies",
    "section": "Gamma Regression",
    "text": "Gamma Regression\nGamma Regression is used when modeling the association between predictors and positive continuous values:\n\n\nCode\nbrm(y~x, \n    data, \n    family = Gamma())"
  },
  {
    "objectID": "lectures/8.html#negative-binomial-regression",
    "href": "lectures/8.html#negative-binomial-regression",
    "title": "Simulation Studies",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\nNegative Binomial Regression is used four with overdispersed count data, where the variance is larger than expected.\n\n\nCode\nbrm(y~x, \n    data,\n    family = negbinomial())"
  },
  {
    "objectID": "lectures/8.html#nonparameteric-models-1",
    "href": "lectures/8.html#nonparameteric-models-1",
    "title": "Simulation Studies",
    "section": "Nonparameteric Models",
    "text": "Nonparameteric Models"
  },
  {
    "objectID": "lectures/8.html#nonlinear-models",
    "href": "lectures/8.html#nonlinear-models",
    "title": "Simulation Studies",
    "section": "Nonlinear Models",
    "text": "Nonlinear Models\n\n\nCode\nx &lt;- rnorm(1000, 2)\ny &lt;- 3 + sinpi(x/2) + rnorm(1000, sd = 0.3)\ndf &lt;- tibble(x, y)\nxlm &lt;- df |&gt; lm(y ~ rcs(x, parms = 10), data = _)\ndf |&gt; ggplot(aes(x, y)) +\n  geom_point() +\n  geom_line(aes(x, predict(xlm)), col = \"red\")"
  },
  {
    "objectID": "lectures/8.html#nonlinear-models-1",
    "href": "lectures/8.html#nonlinear-models-1",
    "title": "Simulation Studies",
    "section": "Nonlinear Models",
    "text": "Nonlinear Models\n\n\nCode\nx &lt;- rnorm(1000, -4)\ny &lt;- 3 +  cospi(x/2) + rnorm(1000, sd = 0.3)\ndf &lt;- tibble(x, y)\nxlm &lt;- df |&gt; lm(y ~ rcs(x, parms = 10), data = _)\ndf |&gt; ggplot(aes(x, y)) +\n  geom_point() +\n  geom_line(aes(x, predict(xlm)), col = \"red\")"
  },
  {
    "objectID": "lectures/8.html#generalized-additive-models",
    "href": "lectures/8.html#generalized-additive-models",
    "title": "Simulation Studies",
    "section": "Generalized Additive Models",
    "text": "Generalized Additive Models\n\n\nCode\nx1 &lt;- rnorm(1000, 2)\nx2 &lt;- rnorm(1000, -4)\ny &lt;- sinpi(x1/2) + cospi(x2/2) + rnorm(1000, sd = 0.5)\ndf &lt;- tibble(x1, x2, y)\n\nxgam &lt;- gam(y ~ rcs(x1,10) + rcs(x2, 10))\nxgam$coefficients\n\ndf1 &lt;- tibble(x = x1,\n              y = sinpi(x1/2),\n              pred = rcs(x1,10) %*% xgam$coefficients[2:10]) \ndf2 &lt;- tibble(x = x2,\n              y = cospi(x2/2),\n              pred = rcs(x2,10) %*% xgam$coefficients[11:19]) \ndf1 |&gt; ggplot(aes(x, y)) +\n  geom_point() +\n  geom_line(aes(x, pred), col = \"red\") \n\ndf2 |&gt; ggplot(aes(x, y)) +\n  geom_point() +\n  geom_line(aes(x, pred), col = \"red\") \n\nplot(xgam)"
  }
]